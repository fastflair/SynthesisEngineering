{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2d8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from petals import AutoDistributedModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "# You can also use \"meta-llama/Llama-2-70b-hf\", \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "# repos with LLaMA-65B, \"bigscience/bloom\", or \"bigscience/bloomz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa5bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch your own swarm: https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d74de45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0932f1f27fcd44fa8bcfbe44561e1064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612791dd1dab47679182dbcda03d5a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7540cb40e347e8850d05a49499a9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c10fbfd42d4f339085556b3ee00b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aug 12 17:33:04.830 [\u001b[1m\u001b[34mINFO\u001b[0m] Make sure you follow the LLaMA's terms of use: https://bit.ly/llama2-license for LLaMA 2, https://bit.ly/llama-license for LLaMA 1\n",
      "Aug 12 17:33:04.830 [\u001b[1m\u001b[34mINFO\u001b[0m] Using DHT prefix: Llama-2-70b-chat-hf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e15a0f22f0f4518af820b01701f4bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd3713210ff43849428349c9a698d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoDistributedModelForCausalLM.from_pretrained(model_name)\n",
    "# Embeddings & prompts are on your device, transformer blocks are distributed across the Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db9ca84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# to use internal swarm\\nINITIAL_PEERS = [\\n    \"/ip4/10.1.2.3/tcp/31234/p2p/QmcXhze98AcgGQDDYna23s4Jho96n8wkwLJv78vxtFNq44\",\\n    \"/ip4/10.1.2.4/tcp/31245/p2p/12D3KooWNPaCDFTKMKBkQazoznq2dkdD3jWkXnYCTJH8PFpggNM6\",\\n]\\n\\nmodel = AutoDistributedModelForCausalLM.from_pretrained(\"bigscience/bloom\", initial_peers=INITIAL_PEERS)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# to use internal swarm\n",
    "INITIAL_PEERS = [\n",
    "    \"/ip4/10.1.2.3/tcp/31234/p2p/QmcXhze98AcgGQDDYna23s4Jho96n8wkwLJv78vxtFNq44\",\n",
    "    \"/ip4/10.1.2.4/tcp/31245/p2p/12D3KooWNPaCDFTKMKBkQazoznq2dkdD3jWkXnYCTJH8PFpggNM6\",\n",
    "]\n",
    "\n",
    "model = AutoDistributedModelForCausalLM.from_pretrained(\"bigscience/bloom\", initial_peers=INITIAL_PEERS)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2026147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aug 12 17:33:14.621 [\u001b[1m\u001b[34mINFO\u001b[0m] Route found: 0:20 via …RPFSct => 20:46 via …kDJVjh => 46:66 via …MWaAxr => 66:80 via …rgNAo9\n",
      "Aug 12 17:33:39.509 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mpetals.client.lm_head.chunked_forward:72\u001b[0m] Running the client with dtype bfloat16 on CPU may be slow, since your CPU doesn't support AVX512. Consider loading the model with torch_dtype='float32'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Riddle: I am something people love or hate. I change people's appearances and thoughts. If a person takes care of them self I will go up even higher. To some people, I will fool them. To others, I am a mystery. Some people might want to try and hide me but I will show. No matter how hard people try I will Never go down. What am I?  Answer: Your age. As people\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Riddle: I am something people love or hate. I change people's appearances and thoughts. If a person takes care of them self I will go up even higher. To some people, I will fool them. To others, I am a mystery. Some people might want to try and hide me but I will show. No matter how hard people try I will Never go down. What am I?  Answer:\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "outputs = model.generate(inputs, max_new_tokens=5)\n",
    "print(tokenizer.decode(outputs[0]))  # A cat sat on a mat..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6135d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
