{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f87d72-5305-4778-ba0c-31549333c8db",
   "metadata": {},
   "source": [
    "# Music Video Synthesis\n",
    "* Extract lyrics from song with timestamps\n",
    "* Compose scenes, include timestamps\n",
    "* Construct video text prompt for each scene\n",
    "* Build videos for each scene\n",
    "* Stitch together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b074-e32d-45f2-977f-c923878625e6",
   "metadata": {},
   "source": [
    "# We will use openai whipser for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b45c210-fd2b-4381-9fd3-c1eb18feefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet --upgrade pip\n",
    "#!pip3 install torch==2.4.1 torchvision torchaudio optimum-quanto torchao xformers --index-url https://download.pytorch.org/whl/cu124\n",
    "#!pip install --quiet --upgrade openai-whisper openai\n",
    "# Ubuntu or Debian\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!pip install setuptools-rust\n",
    "#!pip install -U diffusers imageio imageio_ffmpeg opencv-python moviepy transformers huggingface-hub optimum pillow safetensors\n",
    "#!pip install git+https://github.com/xhinker/sd_embed.git@main\n",
    "#!pip install accelerate flash_attention numba -U\n",
    "#!pip install flash_attn --no-build-isolation\n",
    "#!pip install -r requirements.txt -U\n",
    "#!pip install numpy==1.26.4\n",
    "#!pip install git+https://github.com/zRzRzRzRzRzRzR/diffusers.git@cogvideox1.1-5b -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8537e766-eab2-4757-b6f9-fbac4da44930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 22:42:56.569149: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-22 22:42:56.691764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732336976.736868     987 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732336976.749967     987 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 22:42:56.864854: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import diffusers\n",
    "import gc\n",
    "import imageio\n",
    "import imageio_ffmpeg\n",
    "import json\n",
    "import math\n",
    "import moviepy.editor as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import whisper\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from diffusers.utils import export_to_video, load_video, load_image\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from model4 import T5EncoderModel as m_T5EncoderModel, FluxTransformer2DModel\n",
    "from numba import cuda\n",
    "from openai import OpenAI\n",
    "from optimum.quanto import freeze, qfloat8, quantize, requantize\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from safetensors.torch import load_file as load_safetensors, save_file as save_safetensors\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_flux1\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5TokenizerFast, T5EncoderModel as t_T5EncoderModel, T5Tokenizer\n",
    "from xora.models.autoencoders.causal_video_autoencoder import CausalVideoAutoencoder\n",
    "from xora.models.transformers.transformer3d import Transformer3DModel\n",
    "from xora.models.transformers.symmetric_patchifier import SymmetricPatchifier\n",
    "from xora.schedulers.rf import RectifiedFlowScheduler\n",
    "from xora.pipelines.pipeline_xora_video import XoraVideoPipeline\n",
    "from xora.utils.conditioning_method import ConditioningMethod\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Define the paths where quantized weights will be saved\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "retry_limit = 3\n",
    "quantization = int8_weight_only\n",
    "\n",
    "WIDTH=768\n",
    "HEIGHT=512\n",
    "FRAME_RATE = 25\n",
    "NUM_FRAMES = 151\n",
    "NUM_INFERENCE_STEPS=50\n",
    "GUIDANCE_SCALE=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f54ba32-2208-45c8-8ed2-5fcb1e79aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"openai_api_key\": \"\",\n",
    "    \"openai_model\": \"gpt-4o-mini\",\n",
    "    \"openai_model_large\": \"gpt-4o\",\n",
    "    \"hf_token\": \"\",\n",
    "    \"base_working_dir\": \"./images\",\n",
    "    \"base_video_dir\": \"./output\",\n",
    "    \"audio_files\": [\n",
    "        \"//mnt/d/audio/Coustang.mp3\",\n",
    "        \"//mnt/d/audio/Coustang.mp3\",\n",
    "        \"//mnt/d/audio/Coustang.mp3\",\n",
    "    ],\n",
    "    \"device\": device,\n",
    "    \"dtype\": dtype,\n",
    "    \"retry_limit\": retry_limit,\n",
    "    \"MAX_SEED\": MAX_SEED,\n",
    "}\n",
    "\n",
    "# Ensure base directories exist\n",
    "os.makedirs(CONFIG[\"base_working_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"base_video_dir\"], exist_ok=True)\n",
    "\n",
    "# Set model download directory within Hugging Face Spaces\n",
    "model_path = '/mnt/d/data/models/LTXVideo'\n",
    "if not os.path.exists(model_path):\n",
    "    snapshot_download(\n",
    "        \"Lightricks/LTX-Video\", local_dir=model_path, repo_type=\"model\", token=hf_token\n",
    "    )\n",
    "\n",
    "# Global variables to load components\n",
    "vae_dir = Path(model_path) / \"vae\"\n",
    "unet_dir = Path(model_path) / \"unet\"\n",
    "scheduler_dir = Path(model_path) / \"scheduler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34281c1-e231-4472-9d9b-096fa23b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vae(vae_dir):\n",
    "    vae_ckpt_path = f'{vae_dir}/vae_diffusion_pytorch_model.safetensors'\n",
    "    vae_config_path = f'{vae_dir}/config.json'\n",
    "    with open(vae_config_path, \"r\") as f:\n",
    "        vae_config = json.load(f)\n",
    "    vae = CausalVideoAutoencoder.from_config(vae_config)\n",
    "    vae_state_dict = load_safetensors(vae_ckpt_path)\n",
    "    vae.load_state_dict(vae_state_dict)\n",
    "    if torch.cuda.is_available():\n",
    "        vae = vae.cuda()\n",
    "    return vae.to(torch.bfloat16)\n",
    "\n",
    "\n",
    "def load_unet(unet_dir):\n",
    "    unet_ckpt_path = f'{unet_dir}/unet_diffusion_pytorch_model.safetensors'\n",
    "    unet_config_path = f'{unet_dir}/config.json'\n",
    "    transformer_config = Transformer3DModel.load_config(unet_config_path)\n",
    "    transformer = Transformer3DModel.from_config(transformer_config)\n",
    "    unet_state_dict = load_safetensors(unet_ckpt_path)\n",
    "    transformer.load_state_dict(unet_state_dict, strict=True)\n",
    "    if torch.cuda.is_available():\n",
    "        transformer = transformer.cuda()\n",
    "    return transformer\n",
    "\n",
    "\n",
    "def load_scheduler(scheduler_dir):\n",
    "    scheduler_config_path = f'{scheduler_dir}/scheduler_config.json'\n",
    "    scheduler_config = RectifiedFlowScheduler.load_config(scheduler_config_path)\n",
    "    return RectifiedFlowScheduler.from_config(scheduler_config)\n",
    "\n",
    "\n",
    "def center_crop_and_resize(frame, target_height, target_width):\n",
    "    h, w, _ = frame.shape\n",
    "    aspect_ratio_target = target_width / target_height\n",
    "    aspect_ratio_frame = w / h\n",
    "    if aspect_ratio_frame > aspect_ratio_target:\n",
    "        new_width = int(h * aspect_ratio_target)\n",
    "        x_start = (w - new_width) // 2\n",
    "        frame_cropped = frame[:, x_start : x_start + new_width]\n",
    "    else:\n",
    "        new_height = int(w / aspect_ratio_target)\n",
    "        y_start = (h - new_height) // 2\n",
    "        frame_cropped = frame[y_start : y_start + new_height, :]\n",
    "    frame_resized = cv2.resize(frame_cropped, (target_width, target_height))\n",
    "    return frame_resized\n",
    "\n",
    "\n",
    "def load_video_to_tensor_with_resize(video_path, target_height, target_width):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        if target_height is not None:\n",
    "            frame_resized = center_crop_and_resize(\n",
    "                frame_rgb, target_height, target_width\n",
    "            )\n",
    "        else:\n",
    "            frame_resized = frame_rgb\n",
    "        frames.append(frame_resized)\n",
    "    cap.release()\n",
    "    video_np = (np.array(frames) / 127.5) - 1.0\n",
    "    video_tensor = torch.tensor(video_np).permute(3, 0, 1, 2).float()\n",
    "    return video_tensor\n",
    "\n",
    "\n",
    "def load_image_to_tensor_with_resize(image_path, target_height=512, target_width=768):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_np = np.array(image)\n",
    "    frame_resized = center_crop_and_resize(image_np, target_height, target_width)\n",
    "    frame_tensor = torch.tensor(frame_resized).permute(2, 0, 1).float()\n",
    "    frame_tensor = (frame_tensor / 127.5) - 1.0\n",
    "    # Create 5D tensor: (batch_size=1, channels=3, num_frames=1, height, width)\n",
    "    return frame_tensor.unsqueeze(0).unsqueeze(2)\n",
    "    \n",
    "def reset_memory(device):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    \n",
    "def get_openai_prompt_response(\n",
    "    prompt: str,\n",
    "    config: dict,\n",
    "    max_tokens: int = 6000,\n",
    "    temperature: float = 0.33,\n",
    "    openai_model: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response with retry logic.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Act as a helpful assistant, you are an expert editor.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=openai_model or config[\"openai_model\"],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < config[\"retry_limit\"]:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            return message_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == config[\"retry_limit\"]:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{config['retry_limit']})\")\n",
    "                time.sleep(1)  # Optional: wait before retrying\n",
    "\n",
    "\n",
    "def load_flux_pipe():\n",
    "    text_encoder_2: T5EncoderModel = m_T5EncoderModel.from_pretrained(\n",
    "        \"HighCWu/FLUX.1-dev-4bit\",\n",
    "        subfolder=\"text_encoder_2\",\n",
    "        torch_dtype=dtype,\n",
    "    )\n",
    "    \n",
    "    transformer: FluxTransformer2DModel = FluxTransformer2DModel.from_pretrained(\n",
    "        \"HighCWu/FLUX.1-dev-4bit\",\n",
    "        subfolder=\"transformer\",\n",
    "        torch_dtype=dtype,\n",
    "    )\n",
    "    \n",
    "    pipe: FluxPipeline = FluxPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-dev\",\n",
    "        text_encoder_2=text_encoder_2,\n",
    "        transformer=transformer,\n",
    "        torch_dtype=dtype,\n",
    "    )\n",
    "    #pipe.enable_model_cpu_offload() # with cpu offload, it cost 8.5GB vram\n",
    "    pipe.remove_all_hooks()\n",
    "    pipe = pipe.to('cuda') # without cpu offload, it cost 11GB vram\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def gen_flux_image(pipe, prompt, config: dict, height=1024, width=1024, guidance_scale=3.5, num_inference_steps=32, max_sequence_length=512, seed=-1):\n",
    "    \"\"\"\n",
    "    Generates an image based on the provided prompt using the Flux pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds, pooled_prompt_embeds = get_weighted_text_embeddings_flux1(\n",
    "            pipe        = pipe,\n",
    "            prompt    = prompt\n",
    "        )\n",
    "        \n",
    "        image = pipe(\n",
    "            prompt_embeds               = prompt_embeds,\n",
    "            pooled_prompt_embeds      = pooled_prompt_embeds,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            guidance_scale=guidance_scale,\n",
    "            output_type=\"pil\",\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "        ).images[0]\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "def load_video_pipeline():\n",
    "    \"\"\"\n",
    "    Loads and configures the video generation pipeline.\n",
    "    \"\"\"\n",
    "    # Load models\n",
    "    vae = load_vae(vae_dir)\n",
    "    quantize_(vae, quantization())\n",
    "    \n",
    "    unet = load_unet(unet_dir)\n",
    "    scheduler = load_scheduler(scheduler_dir)\n",
    "    patchifier = SymmetricPatchifier(patch_size=1)\n",
    "    text_encoder = t_T5EncoderModel.from_pretrained(\n",
    "        \"PixArt-alpha/PixArt-XL-2-1024-MS\", subfolder=\"text_encoder\"\n",
    "    )\n",
    "    quantize_(text_encoder, quantization())\n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(\n",
    "        \"PixArt-alpha/PixArt-XL-2-1024-MS\", subfolder=\"tokenizer\"\n",
    "    )\n",
    "    \n",
    "    pipeline = XoraVideoPipeline(\n",
    "        transformer=unet,\n",
    "        patchifier=patchifier,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        scheduler=scheduler,\n",
    "        vae=vae,\n",
    "    ).to(device)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def generate_video(pipeline, prompt, image_input, config: dict, seed_value: int = -1, video_filename: str = \"\", num_frames: int = 151):\n",
    "    \"\"\"\n",
    "    Generates and saves a video from the provided image and prompt.\n",
    "    \"\"\"\n",
    "    negative_prompt = \"low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly\"\n",
    "    \n",
    "    if seed_value == -1:\n",
    "        seed_value = random.randint(0, 255)\n",
    "\n",
    "    media_items = (\n",
    "        load_image_to_tensor_with_resize(image_input, HEIGHT, WIDTH).to(device).detach()\n",
    "    )\n",
    "\n",
    "    sample = {\n",
    "        \"prompt\": prompt,\n",
    "        \"prompt_attention_mask\": None,\n",
    "        \"negative_prompt\": negative_prompt,\n",
    "        \"negative_prompt_attention_mask\": None,\n",
    "        \"media_items\": media_items,\n",
    "    }\n",
    "\n",
    "    generator = torch.Generator(device=\"cpu\").manual_seed(seed_value)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            images = pipeline(\n",
    "                num_inference_steps=NUM_INFERENCE_STEPS,\n",
    "                num_images_per_prompt=1,\n",
    "                guidance_scale=GUIDANCE_SCALE,\n",
    "                generator=generator,\n",
    "                output_type=\"pt\",\n",
    "                height=HEIGHT,\n",
    "                width=WIDTH,\n",
    "                num_frames=num_frames,\n",
    "                frame_rate=FRAME_RATE,\n",
    "                **sample,\n",
    "                is_video=True,\n",
    "                vae_per_channel_normalize=True,\n",
    "                conditioning_method=ConditioningMethod.FIRST_FRAME,\n",
    "                mixed_precision=True\n",
    "            ).images\n",
    "\n",
    "        video_np = images.squeeze(0).permute(1, 2, 3, 0).cpu().float().numpy()\n",
    "        video_np = (video_np * 255).astype(np.uint8)\n",
    "        height, width = video_np.shape[1:3]\n",
    "        out = cv2.VideoWriter(\n",
    "            video_filename, cv2.VideoWriter_fourcc(*\"mp4v\"), FRAME_RATE, (width, height)\n",
    "        )\n",
    "        for frame in video_np[..., ::-1]:\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating the video. Please try again. Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    return video_filename\n",
    "\n",
    "\n",
    "def save_video(frames, fps: int, filename: str):\n",
    "    \"\"\"\n",
    "    Saves a list of frames as a video file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        writer = imageio.get_writer(temp_video_path, fps=fps)\n",
    "        for frame in frames:\n",
    "            writer.append_data(np.array(frame))\n",
    "        writer.close()\n",
    "\n",
    "    os.rename(temp_video_path, filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def convert_to_gif(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a video file to a GIF.\n",
    "    \"\"\"\n",
    "    clip = mp.VideoFileClip(video_path)\n",
    "    clip = clip.set_fps(8)\n",
    "    clip = clip.resize(height=240)\n",
    "    gif_path = video_path.replace(\".mp4\", \".gif\")\n",
    "    clip.write_gif(gif_path, fps=8)\n",
    "    return gif_path\n",
    "\n",
    "\n",
    "def resize_if_unfit(input_video: str) -> str:\n",
    "    \"\"\"\n",
    "    Resizes the video to the target dimensions if it does not match.\n",
    "    \"\"\"\n",
    "    width, height = get_video_dimensions(input_video)\n",
    "\n",
    "    if width == 720 and height == 480:\n",
    "        return input_video\n",
    "    else:\n",
    "        return center_crop_resize(input_video)\n",
    "\n",
    "\n",
    "def get_video_dimensions(input_video_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Retrieves the dimensions of the video.\n",
    "    \"\"\"\n",
    "    reader = imageio_ffmpeg.read_frames(input_video_path)\n",
    "    metadata = next(reader)\n",
    "    return metadata[\"size\"]\n",
    "\n",
    "\n",
    "def center_crop_resize(input_video_path: str, target_width: int = 720, target_height: int = 480) -> str:\n",
    "    \"\"\"\n",
    "    Resizes and center-crops the video to the target dimensions.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    width_factor = target_width / orig_width\n",
    "    height_factor = target_height / orig_height\n",
    "    resize_factor = max(width_factor, height_factor)\n",
    "\n",
    "    inter_width = int(orig_width * resize_factor)\n",
    "    inter_height = int(orig_height * resize_factor)\n",
    "\n",
    "    target_fps = 8\n",
    "    ideal_skip = max(0, math.ceil(orig_fps / target_fps) - 1)\n",
    "    skip = min(5, ideal_skip)  # Cap at 5\n",
    "\n",
    "    while (total_frames / (skip + 1)) < 49 and skip > 0:\n",
    "        skip -= 1\n",
    "\n",
    "    processed_frames = []\n",
    "    frame_count = 0\n",
    "    total_read = 0\n",
    "\n",
    "    while frame_count < 49 and total_read < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if total_read % (skip + 1) == 0:\n",
    "            resized = cv2.resize(frame, (inter_width, inter_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            start_x = (inter_width - target_width) // 2\n",
    "            start_y = (inter_height - target_height) // 2\n",
    "            cropped = resized[start_y:start_y + target_height, start_x:start_x + target_width]\n",
    "\n",
    "            processed_frames.append(cropped)\n",
    "            frame_count += 1\n",
    "\n",
    "        total_read += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(temp_video_path, fourcc, target_fps, (target_width, target_height))\n",
    "\n",
    "        for frame in processed_frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    return temp_video_path\n",
    "\n",
    "\n",
    "def extract_last_frame(video_filename: str, output_image_filename: str):\n",
    "    \"\"\"\n",
    "    Extracts the last frame from a video file and saves it as an image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = imageio.get_reader(video_filename, 'ffmpeg')\n",
    "        last_frame = None\n",
    "        for frame in reader:\n",
    "            last_frame = frame\n",
    "        reader.close()\n",
    "\n",
    "        if last_frame is not None:\n",
    "            imageio.imwrite(output_image_filename, last_frame)\n",
    "            print(f\"Last frame saved successfully as '{output_image_filename}'.\")\n",
    "        else:\n",
    "            print(\"The video contains no frames.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{video_filename}' was not found.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except RuntimeError as re:\n",
    "        print(f\"RuntimeError: {re}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def create_scenes(text: str, video_summary: str, config: dict):\n",
    "    \"\"\"\n",
    "    Creates scenes based on the extracted lyrics using OpenAI's API.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Create a json list of diverse, unique scenes (groupings of text), scene_description (200 words or less), and action_sequence (100 words or less) from the following text.  Scenes should be groups of lyrics with new scenes when the lyric context changes.  Text: {text}   \n",
    "The json list should have the start value for the first item in the scene and the text that is combined for all items in the same scene.  \n",
    "The scene_description should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes.  Use theme descriptions, such as graphic novel, photograph, oil painting, etc.\n",
    "The action_sequence should describe the action in the scene.  Scenes should be unique, creative, imaginative, and awe-inspiring to create an amazing video.  Create beautiful and mesmerizing scene descriptions that are creative, unique, artistic, and imaginative. Each scene must be unique, imaginative, and visually captivating, blending creativity with artistic flair. Use powerful, descriptive language to craft scenes that are awe-inspiring and leave the audience in wonder. These scenes should evoke a sense of beauty, grandeur, mystery, or anything emotional, drawing from both realistic and fantastical elements. Ensure the descriptions are immersive, emotionally resonant, and filled with unexpected twists that engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.\n",
    "Here are some examples of action_sequences:\n",
    "    A woman walks away from a white Jeep parked on a city street at night, then ascends a staircase and knocks on a door. The woman, wearing a dark jacket and jeans, walks away from the Jeep parked on the left side of the street, her back to the camera; she walks at a steady pace, her arms swinging slightly by her sides; the street is dimly lit, with streetlights casting pools of light on the wet pavement; a man in a dark jacket and jeans walks past the Jeep in the opposite direction; the camera follows the woman from behind as she walks up a set of stairs towards a building with a green door; she reaches the top of the stairs and turns left, continuing to walk towards the building; she reaches the door and knocks on it with her right hand; the camera remains stationary, focused on the doorway; the scene is captured in real-life footage.\n",
    "    A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair's face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage.\n",
    "    A man in a suit enters a room and speaks to two women sitting on a couch. The man, wearing a dark suit with a gold tie, enters the room from the left and walks towards the center of the frame. He has short gray hair, light skin, and a serious expression. He places his right hand on the back of a chair as he approaches the couch. Two women are seated on a light-colored couch in the background. The woman on the left wears a light blue sweater and has short blonde hair. The woman on the right wears a white sweater and has short blonde hair. The camera remains stationary, focusing on the man as he enters the room. The room is brightly lit, with warm tones reflecting off the walls and furniture. The scene appears to be from a film or television show.\n",
    "    The camera pans across a cityscape of tall buildings with a circular building in the center. The camera moves from left to right, showing the tops of the buildings and the circular building in the center. The buildings are various shades of gray and white, and the circular building has a green roof. The camera angle is high, looking down at the city. The lighting is bright, with the sun shining from the upper left, casting shadows from the buildings. The scene is computer-generated imagery.\n",
    "    A young man with short blond hair and light skin, wearing a red and gold patterned jacket, looks down and to his right, then back up as he speaks to a woman with long red hair and light skin, wearing a red dress with gold embroidery; she stands slightly behind him to his left, her expression serious; they are in a dimly lit room with stone walls and several other people in the background, some holding torches; the camera remains stationary on the man's face, then briefly pans to the woman before returning to the man; the scene is captured in real-life footage.\n",
    "    A person is driving a car on a two-lane road, holding the steering wheel with both hands. The person's hands are light-skinned and they are wearing a black long-sleeved shirt. The steering wheel has a Toyota logo in the center and black leather around it. The car's dashboard is visible, showing a speedometer, tachometer, and navigation screen. The road ahead is straight and there are trees and fields visible on either side. The camera is positioned inside the car, providing a view from the driver's perspective. The lighting is natural and overcast, with a slightly cool tone. The scene is captured in real-life footage.\n",
    "    A bald man with a goatee, wearing a green and yellow robe, stands on a beach with two women, one with long blonde hair in braids wearing a blue dress and the other with long dark hair wearing a brown dress. The man turns his head slightly to the left, looking off-screen, while the blonde woman looks down with a serious expression. The camera remains stationary, capturing the three figures from the chest up. The background features a bright blue sky, a sandy beach, and a rocky outcropping. The scene is brightly lit, likely by sunlight, casting soft shadows on the characters' faces. The scene appears to be from a movie or TV show.\n",
    "    A bumblebee is perched on a cluster of pink flowers, sipping nectar. The bee is mostly black with yellow stripes on its abdomen and has translucent wings. The flowers are a light pink color and have five petals each. They are growing on a green stem with several leaves. The background is blurry, but it appears to be a sunny day.\n",
    "    A pair of hands shapes a piece of clay on a pottery wheel, gradually forming a cone shape. The hands, belonging to a person out of frame, are covered in clay and gently press a ball of clay onto the center of a spinning pottery wheel. The hands move in a circular motion, gradually forming a cone shape at the top of the clay. The camera is positioned directly above the pottery wheel, providing a bird's-eye view of the clay being shaped. The lighting is bright and even, illuminating the clay and the hands working on it. The scene is captured in real-life footage.\n",
    "    A young woman with long, wavy red hair is sitting in a white bathtub, leaning over the edge and reaching for the faucet to turn on the water.\n",
    "    She has a serene expression on her face and her eyes are closed. Her skin is pale and her lips are slightly parted. She is wearing a white towel wrapped around her body. The bathtub is large and round, with a white porcelain finish. It is set in a bathroom with white walls and a large window. The window is covered by a white curtain, which is partially drawn back to let in some natural light.\n",
    "    An elephant walks towards the camera, stops, and lowers its head to eat some grass. The elephant is gray and has large white tusks. The elephant is walking on a grassy field. There are some trees and bushes in the background. The camera is stationary.\n",
    "    A man in a dimly lit room raises a glass to his lips and drinks. He wears a dark green tunic with a brown belt and has shoulder-length dark brown hair and a beard. He raises the glass in his right hand and drinks from it, then lowers it slightly while continuing to look straight ahead. The camera remains stationary, focused on the man from the waist up. The room is lit by a single candle on a stand to the left, casting warm light on the man's face and the surrounding area. The scene appears to be from a movie or TV show.\n",
    "    The squirrel has gray and brown fur and a bushy tail. Its eyes are black and shiny. The squirrel is sitting on a gray brick patio with small pebbles scattered around. There is a white plastic container next to the squirrel.\n",
    "    A young man with blond hair wearing a yellow jacket stands in a forest and looks around. He has light skin and his hair is styled with a middle part. He looks to the left and then to the right, his gaze lingering in each direction. The camera angle is low, looking up at the man, and remains stationary throughout the video. The background is slightly out of focus, with green trees and the sun shining brightly behind the man. The lighting is natural and warm, with the sun creating a lens flare that moves across the man's face. The scene is captured in real-life footage.\n",
    "    A woman with blonde hair styled up, wearing a black dress with sequins and pearl earrings, looks down with a sad expression on her face. The camera remains stationary, focused on the woman's face. The lighting is dim, casting soft shadows on her face. The scene appears to be from a movie or TV show.\n",
    "    A young woman with dark, curly hair and pale skin sits on a chair, her expression serious as she looks down and to the right; she wears a dark, intricately patterned dress with a high collar and long, dark gloves that extend past her elbows; a man with dark, curly hair and a pale face stands behind her to the left, his expression unreadable; he wears a dark vest over a white shirt and dark pants; the camera remains stationary, focused on the woman's face; the scene is dimly lit, with light streaming in from a large window behind the characters; the scene appears to be from a movie.\n",
    "    A herd of elephants walks from right to left across a dry, grassy plain. The elephants, with their gray skin and large ears, move at a steady pace, their trunks occasionally reaching down to graze on the sparse vegetation; one elephant in the foreground, slightly larger than the others, leads the charge, its head held high and ears spread wide; the camera remains stationary, capturing the elephants from a side angle as they move across the frame; the sun casts a bright light, creating sharp shadows on the ground and highlighting the texture of the elephants' skin; the scene is captured in real-life footage.\n",
    "    A man walks towards an oil derrick in a snowy field, stops to observe the flames, and then bends down to examine the ground. Wearing a beige jacket, dark pants, and a dark hat, the man walks from left to right across the snowy field towards a tall metal oil derrick with flames erupting from its top; he stops a few feet from the derrick and looks up at the flames; he then bends down, keeping his back to the camera, and examines the ground; the camera remains stationary throughout the scene, capturing a wide shot of the man, the derrick, and the snowy field; the sky is overcast, and the lighting is dim and natural; the scene is captured in real-life footage.\n",
    "    A man with short blond hair, wearing a white V-neck shirt and a dark gray jacket, stands in a parking lot, talking to another man whose back is to the camera; he has light skin and a neutral expression; the other man wears a light gray shirt; a dark-colored car is parked behind them; the camera remains stationary on the two men; the scene is brightly lit, with sunlight illuminating the parking lot; the scene appears to be real-life footage.\n",
    "    Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers' faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.\n",
    "    The woman has long black hair styled in two braids, adorned with white beads, and her eyes are wide with a hint of surprise. Her dress is a vibrant blue with intricate gold embroidery, and she wears a matching headband with a similar design. The background is a simple white curtain, which creates a sense of mystery and intrigue.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.66)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "def revise_scenes(scenes, config: dict):\n",
    "    \"\"\"\n",
    "    Revise scenes based on the extracted scenes.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Revise the JSON scenes to update the scene_description and action_sequence to engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  JSON scenes: {scenes}   \n",
    "The scene_description (200 words or less) should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes. Use theme descriptions, such as graphic novel, photograph, oil painting, etc.\n",
    "The action_sequence (30 words or less) should describe the action in the scene.  The goal is to create input to create a stunning, cinematic video experience.   Action can't have sudden movements or fast zooms, camera movement.\n",
    "Only update the scene_description and action_sequence.  Do not delete any items as having scenes with the given start times are important.  We do not want to have the same scene_description and action_sequence for the items with repeatitive input text.  Please change these to be creative and consistent with dynamic video sequences.\n",
    "Here are some examples of action_sequences:\n",
    "    A woman walks away from a white Jeep parked on a city street at night, then ascends a staircase and knocks on a door. The woman, wearing a dark jacket and jeans, walks away from the Jeep parked on the left side of the street, her back to the camera; she walks at a steady pace, her arms swinging slightly by her sides; the street is dimly lit, with streetlights casting pools of light on the wet pavement; a man in a dark jacket and jeans walks past the Jeep in the opposite direction; the camera follows the woman from behind as she walks up a set of stairs towards a building with a green door; she reaches the top of the stairs and turns left, continuing to walk towards the building; she reaches the door and knocks on it with her right hand; the camera remains stationary, focused on the doorway; the scene is captured in real-life footage.\n",
    "    A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair's face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage.\n",
    "    A man in a suit enters a room and speaks to two women sitting on a couch. The man, wearing a dark suit with a gold tie, enters the room from the left and walks towards the center of the frame. He has short gray hair, light skin, and a serious expression. He places his right hand on the back of a chair as he approaches the couch. Two women are seated on a light-colored couch in the background. The woman on the left wears a light blue sweater and has short blonde hair. The woman on the right wears a white sweater and has short blonde hair. The camera remains stationary, focusing on the man as he enters the room. The room is brightly lit, with warm tones reflecting off the walls and furniture. The scene appears to be from a film or television show.\n",
    "    The camera pans across a cityscape of tall buildings with a circular building in the center. The camera moves from left to right, showing the tops of the buildings and the circular building in the center. The buildings are various shades of gray and white, and the circular building has a green roof. The camera angle is high, looking down at the city. The lighting is bright, with the sun shining from the upper left, casting shadows from the buildings. The scene is computer-generated imagery.\n",
    "    A young man with short blond hair and light skin, wearing a red and gold patterned jacket, looks down and to his right, then back up as he speaks to a woman with long red hair and light skin, wearing a red dress with gold embroidery; she stands slightly behind him to his left, her expression serious; they are in a dimly lit room with stone walls and several other people in the background, some holding torches; the camera remains stationary on the man's face, then briefly pans to the woman before returning to the man; the scene is captured in real-life footage.\n",
    "    A person is driving a car on a two-lane road, holding the steering wheel with both hands. The person's hands are light-skinned and they are wearing a black long-sleeved shirt. The steering wheel has a Toyota logo in the center and black leather around it. The car's dashboard is visible, showing a speedometer, tachometer, and navigation screen. The road ahead is straight and there are trees and fields visible on either side. The camera is positioned inside the car, providing a view from the driver's perspective. The lighting is natural and overcast, with a slightly cool tone. The scene is captured in real-life footage.\n",
    "    A bald man with a goatee, wearing a green and yellow robe, stands on a beach with two women, one with long blonde hair in braids wearing a blue dress and the other with long dark hair wearing a brown dress. The man turns his head slightly to the left, looking off-screen, while the blonde woman looks down with a serious expression. The camera remains stationary, capturing the three figures from the chest up. The background features a bright blue sky, a sandy beach, and a rocky outcropping. The scene is brightly lit, likely by sunlight, casting soft shadows on the characters' faces. The scene appears to be from a movie or TV show.\n",
    "    A bumblebee is perched on a cluster of pink flowers, sipping nectar. The bee is mostly black with yellow stripes on its abdomen and has translucent wings. The flowers are a light pink color and have five petals each. They are growing on a green stem with several leaves. The background is blurry, but it appears to be a sunny day.\n",
    "    A pair of hands shapes a piece of clay on a pottery wheel, gradually forming a cone shape. The hands, belonging to a person out of frame, are covered in clay and gently press a ball of clay onto the center of a spinning pottery wheel. The hands move in a circular motion, gradually forming a cone shape at the top of the clay. The camera is positioned directly above the pottery wheel, providing a bird's-eye view of the clay being shaped. The lighting is bright and even, illuminating the clay and the hands working on it. The scene is captured in real-life footage.\n",
    "    A young woman with long, wavy red hair is sitting in a white bathtub, leaning over the edge and reaching for the faucet to turn on the water.\n",
    "    She has a serene expression on her face and her eyes are closed. Her skin is pale and her lips are slightly parted. She is wearing a white towel wrapped around her body. The bathtub is large and round, with a white porcelain finish. It is set in a bathroom with white walls and a large window. The window is covered by a white curtain, which is partially drawn back to let in some natural light.\n",
    "    An elephant walks towards the camera, stops, and lowers its head to eat some grass. The elephant is gray and has large white tusks. The elephant is walking on a grassy field. There are some trees and bushes in the background. The camera is stationary.\n",
    "    A man in a dimly lit room raises a glass to his lips and drinks. He wears a dark green tunic with a brown belt and has shoulder-length dark brown hair and a beard. He raises the glass in his right hand and drinks from it, then lowers it slightly while continuing to look straight ahead. The camera remains stationary, focused on the man from the waist up. The room is lit by a single candle on a stand to the left, casting warm light on the man's face and the surrounding area. The scene appears to be from a movie or TV show.\n",
    "    The squirrel has gray and brown fur and a bushy tail. Its eyes are black and shiny. The squirrel is sitting on a gray brick patio with small pebbles scattered around. There is a white plastic container next to the squirrel.\n",
    "    A young man with blond hair wearing a yellow jacket stands in a forest and looks around. He has light skin and his hair is styled with a middle part. He looks to the left and then to the right, his gaze lingering in each direction. The camera angle is low, looking up at the man, and remains stationary throughout the video. The background is slightly out of focus, with green trees and the sun shining brightly behind the man. The lighting is natural and warm, with the sun creating a lens flare that moves across the man's face. The scene is captured in real-life footage.\n",
    "    A woman with blonde hair styled up, wearing a black dress with sequins and pearl earrings, looks down with a sad expression on her face. The camera remains stationary, focused on the woman's face. The lighting is dim, casting soft shadows on her face. The scene appears to be from a movie or TV show.\n",
    "    A young woman with dark, curly hair and pale skin sits on a chair, her expression serious as she looks down and to the right; she wears a dark, intricately patterned dress with a high collar and long, dark gloves that extend past her elbows; a man with dark, curly hair and a pale face stands behind her to the left, his expression unreadable; he wears a dark vest over a white shirt and dark pants; the camera remains stationary, focused on the woman's face; the scene is dimly lit, with light streaming in from a large window behind the characters; the scene appears to be from a movie.\n",
    "    A herd of elephants walks from right to left across a dry, grassy plain. The elephants, with their gray skin and large ears, move at a steady pace, their trunks occasionally reaching down to graze on the sparse vegetation; one elephant in the foreground, slightly larger than the others, leads the charge, its head held high and ears spread wide; the camera remains stationary, capturing the elephants from a side angle as they move across the frame; the sun casts a bright light, creating sharp shadows on the ground and highlighting the texture of the elephants' skin; the scene is captured in real-life footage.\n",
    "    A man walks towards an oil derrick in a snowy field, stops to observe the flames, and then bends down to examine the ground. Wearing a beige jacket, dark pants, and a dark hat, the man walks from left to right across the snowy field towards a tall metal oil derrick with flames erupting from its top; he stops a few feet from the derrick and looks up at the flames; he then bends down, keeping his back to the camera, and examines the ground; the camera remains stationary throughout the scene, capturing a wide shot of the man, the derrick, and the snowy field; the sky is overcast, and the lighting is dim and natural; the scene is captured in real-life footage.\n",
    "    A man with short blond hair, wearing a white V-neck shirt and a dark gray jacket, stands in a parking lot, talking to another man whose back is to the camera; he has light skin and a neutral expression; the other man wears a light gray shirt; a dark-colored car is parked behind them; the camera remains stationary on the two men; the scene is brightly lit, with sunlight illuminating the parking lot; the scene appears to be real-life footage.\n",
    "    Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers' faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.\n",
    "    The woman has long black hair styled in two braids, adorned with white beads, and her eyes are wide with a hint of surprise. Her dress is a vibrant blue with intricate gold embroidery, and she wears a matching headband with a similar design. The background is a simple white curtain, which creates a sense of mystery and intrigue.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.33)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def process_audio_scenes(audio_file: str, config: dict):\n",
    "    # set maximum duration for an image basis, should be in intervals of video generation length\n",
    "    video_gen_length = 6\n",
    "    max_duration_seconds  = video_gen_length * 3\n",
    "    \"\"\"\n",
    "    Processes a single audio file through the entire workflow.\n",
    "    \"\"\"\n",
    "    # Create unique identifier based on audio file name\n",
    "    audio_basename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = f\"{audio_basename}_{timestamp}\"\n",
    "\n",
    "    # Create unique directories for images and videos\n",
    "    print(f\"Create unique directories for images and videos\")\n",
    "    audio_images_dir = os.path.join(config[\"base_working_dir\"], unique_id)\n",
    "    audio_videos_dir = os.path.join(config[\"base_video_dir\"], unique_id)\n",
    "    os.makedirs(audio_images_dir, exist_ok=True)\n",
    "    os.makedirs(audio_videos_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Transcribe audio using Whisper\n",
    "    print(f\"Transcribe audio using Whisper\")\n",
    "    model = whisper.load_model(\"turbo\")\n",
    "    result = model.transcribe(audio_file)\n",
    "\n",
    "    # Cleanup Whisper model memory\n",
    "    del model\n",
    "    reset_memory(device)\n",
    "\n",
    "    segments = result['segments']\n",
    "\n",
    "    # Extract list of start times and texts\n",
    "    segment_texts_and_start_times = [(segment['text'].strip(), segment['start']) for segment in segments]\n",
    "\n",
    "    # Combine texts\n",
    "    text = \"\"\n",
    "    for segment_text, start in segment_texts_and_start_times:\n",
    "        text += f\"Start: {start}, Text: {segment_text}\\n\"\n",
    "\n",
    "    last_end_value = segments[-1]['end']\n",
    "\n",
    "    # Path to scenes.json file\n",
    "    scenes_file_path = os.path.join(audio_images_dir, \"scenes.json\")\n",
    "\n",
    "    # Check if scenes.json exists\n",
    "    if os.path.exists(scenes_file_path):\n",
    "        print(f\"Scenes file already exists at {scenes_file_path}. Skipping scene generation.\")\n",
    "        with open(scenes_file_path, \"r\") as scenes_file:\n",
    "            scenes = json.load(scenes_file)\n",
    "        return scenes, audio_images_dir, audio_videos_dir, last_end_value\n",
    "\n",
    "    # Step 2: Generate video summary using OpenAI\n",
    "    print(f\"Generate video summary using OpenAI\")\n",
    "    video_summary_prompt = f'Create a short summary that describes a music video based on these lyrics: {text}'\n",
    "    video_summary = get_openai_prompt_response(video_summary_prompt, config, openai_model=config[\"openai_model\"])\n",
    "\n",
    "    # Step 3: Create scenes based on lyrics\n",
    "    print(f\"Create scenes based on lyrics\")\n",
    "    try:\n",
    "        scenes = create_scenes(text, video_summary, config)\n",
    "    except:\n",
    "        try:\n",
    "            scenes = create_scenes(text, video_summary, config)\n",
    "        except:\n",
    "            try:\n",
    "                scenes = create_scenes(text, video_summary, config)\n",
    "            except: \n",
    "                return \"\", audio_images_dir, audio_videos_dir, last_end_value\n",
    "            \n",
    "    # we don't want scenes longer than 18 seconds\n",
    "    new_scenes = []\n",
    "    for i in range(len(scenes)):\n",
    "        scene = scenes[i]\n",
    "        if i == 0:\n",
    "            start_time = 0\n",
    "        else:\n",
    "            start_time = scene['start']\n",
    "        # Determine the end time\n",
    "        if i < len(scenes) - 1:\n",
    "            end_time = scenes[i + 1]['start']\n",
    "        else:\n",
    "            end_time = last_end_value\n",
    "        duration = end_time - start_time\n",
    "        # Split the scene if duration exceeds 18 seconds\n",
    "        while duration > 18:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "            start_time += max_duration_seconds\n",
    "            duration = end_time - start_time\n",
    "        # Append the remaining part of the scene\n",
    "        if duration > 0:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "    # Replace the original scenes with the new list\n",
    "    scenes = new_scenes\n",
    "    # improve the scenes with a revision\n",
    "    try:\n",
    "        scenes_revised = revise_scenes(scenes, config)\n",
    "        scenes = scenes_revised\n",
    "        print(f'revised scenes')\n",
    "    except:\n",
    "        try:\n",
    "            scenes_revised = revise_scenes(scenes, config)\n",
    "            scenes = scenes_revised\n",
    "            print(f'revised scenes')\n",
    "        except:\n",
    "            print('cannot revise scenes')\n",
    "            \n",
    "    \n",
    "    # Save the scenes to scenes.json\n",
    "    with open(scenes_file_path, \"w\") as scenes_file:\n",
    "        json.dump(scenes, scenes_file)\n",
    "        \n",
    "    return scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def process_audio_images(config: dict, scenes, audio_images_dir):\n",
    "    # Step 4: Load Flux pipeline and generate images\n",
    "    print(f\"Load Flux pipeline and generate images\")\n",
    "    flux_pipe = load_flux_pipe()\n",
    "    height = HEIGHT\n",
    "    width = WIDTH\n",
    "    guidance_scale = 3.9\n",
    "    num_inference_steps = 48\n",
    "    max_sequence_length = 512\n",
    "    seed = -1\n",
    "\n",
    "    # Generate images for each scene\n",
    "    image_num = 1\n",
    "    for scene in scenes:\n",
    "        image_prompt = scene['scene_description']\n",
    "        image = gen_flux_image(flux_pipe, image_prompt, config, height, width, guidance_scale, num_inference_steps, max_sequence_length, seed)\n",
    "        filename = f\"image_{str(image_num).zfill(2)}.jpg\"\n",
    "        image_path = os.path.join(audio_images_dir, filename)\n",
    "        image.save(image_path, dpi=(300, 300))\n",
    "        image_num += 1\n",
    "\n",
    "    # Move the pipeline back to CPU and delete it\n",
    "    flux_pipe.to('cpu')\n",
    "    del flux_pipe\n",
    "    reset_memory(device)\n",
    "    return\n",
    "\n",
    "def process_audio_video(config: dict, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp):\n",
    "    # Step 6: Load Video Pipeline\n",
    "    print(f\"Load Video Pipeline\")\n",
    "    video_pipe = load_video_pipeline()\n",
    "\n",
    "    # Temporary image path\n",
    "    temp_image = os.path.join(audio_images_dir, \"temp_image.jpg\")\n",
    "    video_num = 1\n",
    "\n",
    "    # Step 7: Generate video sequences\n",
    "    for i, scene in enumerate(scenes):\n",
    "        prompt = scene[\"action_sequence\"]\n",
    "\n",
    "        # Use the initial image for each scene\n",
    "        image_input = os.path.join(audio_images_dir, f\"image_{str(i+1).zfill(2)}.jpg\")\n",
    "\n",
    "        # Calculate duration to keep the video in 6-second increments\n",
    "        if i + 1 < len(scenes):\n",
    "            next_start_time = scenes[i + 1][\"start\"]\n",
    "        else:\n",
    "            next_start_time = last_end_value  # Use the final ending time for the last scene\n",
    "\n",
    "        if i == 0:\n",
    "            duration = next_start_time\n",
    "        else:\n",
    "            duration = next_start_time - scene[\"start\"]\n",
    "        num_video_segments = int((duration + 3) // 6)\n",
    "\n",
    "        print(f\"Scene {i+1} has {num_video_segments} segments\")\n",
    "        for j in range(num_video_segments):\n",
    "            video_name = f\"video_{str(video_num).zfill(2)}_{str(i+1)}_{str(j+1).zfill(2)}_{timestamp}.mp4\"\n",
    "            video_output_path = os.path.join(audio_videos_dir, video_name)\n",
    "            generate_video(video_pipe, prompt, image_input, config, seed_value=-1, video_filename=video_output_path)\n",
    "            time.sleep(1)  # Pause for 1 second\n",
    "\n",
    "            # After generating the video, extract the last frame to use as input for the next segment\n",
    "            extract_last_frame(video_output_path, temp_image)\n",
    "\n",
    "            # Use the last frame as input for the next video segment in the same scene\n",
    "            image_input = temp_image\n",
    "\n",
    "            video_num += 1  # Increment video number for the next segment\n",
    "\n",
    "    # Move the pipeline back to CPU before deleting\n",
    "    video_pipe.to('cpu')\n",
    "    del video_pipe\n",
    "    reset_memory(device)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def process_all_audios(audio_file, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a list of audio files through the workflow.\n",
    "    \"\"\"\n",
    "    print(f\"Processing audio file: {audio_file}\")\n",
    "    scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_audio_scenes(audio_file, config)\n",
    "    print(f'{len(scenes)} scenes:\\n{json.dumps(scenes, indent=4)}')\n",
    "    print(f'last_end_value: {last_end_value} timestamp: {timestamp}')\n",
    "    # Create starting images for scenes\n",
    "    process_audio_images(config, scenes, audio_images_dir)\n",
    "    return config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def create_video():\n",
    "    config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_all_audios(audio_file, CONFIG)\n",
    "    process_audio_video(config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp)\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efd2eb-8304-4027-a8c3-eaafe04ade14",
   "metadata": {},
   "source": [
    "### Run new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0cc96-e609-4cd2-9c67-dcd63562988a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file: //mnt/d/audio/Coustang.mp3\n",
      "Create unique directories for images and videos\n",
      "Transcribe audio using Whisper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/hulk/miniconda3/lib/python3.10/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate video summary using OpenAI\n",
      "Create scenes based on lyrics\n",
      "revised scenes\n",
      "15 scenes:\n",
      "[\n",
      "    {\n",
      "        \"start\": 0,\n",
      "        \"text\": \"Yo, check it, in the land of rods and rubber\\nThere's a beast, no other undercover\\nStarburst bloom, no gloom, called the Kustang brother\\nHalf Mustang, half Cougar, no room for another\\nThundercat rims spinning wild, so free\\nFuzzy white on the seats, pure luxury you see\",\n",
      "        \"scene_description\": \"In a bustling cityscape, the Kustang gleams under a radiant sun, its Thundercat rims sparkling like jewels. The plush white interior contrasts with the deep blue exterior, evoking a sense of opulence. Vibrant street art decorates nearby walls, while the sounds of laughter and music fill the air. The lighting is bright and inviting, highlighting the car's sleek lines and curves, creating an atmosphere brimming with excitement and adventure.\",\n",
      "        \"action_sequence\": \"The Kustang glides effortlessly down the street, its engine a soothing purr. The camera follows, capturing the mesmerizing spin of the rims as the driver bobs his head to the beat, embodying the essence of freedom.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 18,\n",
      "        \"text\": \"Yo, check it, in the land of rods and rubber\\nThere's a beast, no other undercover\\nStarburst bloom, no gloom, called the Kustang brother\\nHalf Mustang, half Cougar, no room for another\\nThundercat rims spinning wild, so free\\nFuzzy white on the seats, pure luxury you see\",\n",
      "        \"scene_description\": \"In a vibrant urban setting, the Kustang glistens under the afternoon sun, its Thundercat rims catching the light in a dazzling display. The car's fuzzy white interior contrasts sharply with the deep blue exterior, evoking a sense of luxury and freedom. The surrounding streets are alive with the energy of city life, with graffiti art splashed across nearby walls. The mood is electrifying, filled with the promise of adventure, and the lighting is bright and cheerful, accentuating the car's curves.\",\n",
      "        \"action_sequence\": \"The Kustang rolls smoothly down the street, its engine purring like a content cat. The camera follows the car as it turns a corner, showcasing its impressive rims spinning. The driver, feeling the rhythm of the music, nods along, embodying the spirit of the ride.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 33.0,\n",
      "        \"text\": \"Kustang, Kustang, rolling through the hood\\nThe bass is banging, but it's misunderstood\\nHe's a legend in his mind, so fine, so good\\nBut the Kustang, oh the Kustang, never catching the goods\\nNow our hero, he tries, with his dice in the mirror\\nHopes flying high, but the girls, they just sneer\",\n",
      "        \"scene_description\": \"The scene transitions to a shadowy street, where the Kustang glides past, bass reverberating through the air like a heartbeat. The driver, clad in a leather jacket and sunglasses, glances at his reflection, confidence radiating from him. Flickering streetlights cast a moody glow, illuminating the vibrant colors of the car against the darkened backdrop. The atmosphere is thick with a mix of bravado and longing, as the sounds of the city pulse around him.\",\n",
      "        \"action_sequence\": \"The driver lowers the window, letting the bass thump into the night. He catches fleeting glances from girls, their expressions revealing indifference. The camera captures his hopeful gaze before cutting to the dismissive reactions of the girls, leaving him in the dim light of his aspirations.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 51.0,\n",
      "        \"text\": \"Kustang, Kustang, rolling through the hood\\nThe bass is banging, but it's misunderstood\\nHe's a legend in his mind, so fine, so good\\nBut the Kustang, oh the Kustang, never catching the goods\\nNow our hero, he tries, with his dice in the mirror\\nHopes flying high, but the girls, they just sneer\",\n",
      "        \"scene_description\": \"The scene shifts to a dimly lit street where the Kustang glides past, bass thumping, creating a heavy rhythm that resonates in the air. The driver, wearing a confident smile and flashy sunglasses, glances at his reflection in the mirror, his hopes high. Surrounding him are the shadows of the night, with flickering streetlights casting an urban glow. The mood is a mix of bravado and disappointment, as the vibrant colors of the vehicle contrast with the darker tones of the street.\",\n",
      "        \"action_sequence\": \"The driver rolls down the window, letting the music spill out into the night. He catches glimpses of girls as they glance his way, but their expressions reveal disinterest. The camera captures his hopeful demeanor, then cuts to the girls shaking their heads as they walk away, leaving him in the dim light of his dreams.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 69.0,\n",
      "        \"text\": \"Kustang, Kustang, rolling through the hood\\nThe bass is banging, but it's misunderstood\\nHe's a legend in his mind, so fine, so good\\nBut the Kustang, oh the Kustang, never catching the goods\\nNow our hero, he tries, with his dice in the mirror\\nHopes flying high, but the girls, they just sneer\",\n",
      "        \"scene_description\": \"The scene shifts to a dimly lit street where the Kustang glides past, bass thumping, creating a heavy rhythm that resonates in the air. The driver, wearing a confident smile and flashy sunglasses, glances at his reflection in the mirror, his hopes high. Surrounding him are the shadows of the night, with flickering streetlights casting an urban glow. The mood is a mix of bravado and disappointment, as the vibrant colors of the vehicle contrast with the darker tones of the street.\",\n",
      "        \"action_sequence\": \"The driver rolls down the window, letting the music spill out into the night. He catches glimpses of girls as they glance his way, but their expressions reveal disinterest. The camera captures his hopeful demeanor, then cuts to the girls shaking their heads as they walk away, leaving him in the dim light of his dreams.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 78.0,\n",
      "        \"text\": \"With dreams of romance, but alas, known come true\\nHe's the night's brightest star, if only he knew\\nBut the Kustang, oh Kustang, a lair and a skill\\nIn his mind, he's a hero, a Casanova in steel\\nBut reality's harsh, makes a hard deal\\nWith every failed attempt, it's a comedic reel\",\n",
      "        \"scene_description\": \"In a whimsical, dream-like sequence, the Kustang transforms into a radiant star against a backdrop of a starry night sky. The car glows with ethereal light, swirling among fluffy clouds of cotton candy. The driver, dressed in a sparkling tuxedo, reaches out towards the stars, his expression a mix of charm and wistfulness. The scene is bathed in soft pastel hues, creating a magical yet bittersweet contrast between dreams and reality.\",\n",
      "        \"action_sequence\": \"As the Kustang twirls among the clouds, the driver gestures dramatically, attempting to grasp his elusive dreams. Each failed encounter with girls appears as humorous sketches around him, adding a lighthearted touch to his journey.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 96.0,\n",
      "        \"text\": \"With dreams of romance, but alas, known come true\\nHe's the night's brightest star, if only he knew\\nBut the Kustang, oh Kustang, a lair and a skill\\nIn his mind, he's a hero, a Casanova in steel\\nBut reality's harsh, makes a hard deal\\nWith every failed attempt, it's a comedic reel\",\n",
      "        \"scene_description\": \"In a whimsical, dream-like sequence, the Kustang transforms into a radiant star against a backdrop of a starry night sky. The car glows with ethereal light, swirling among fluffy clouds of cotton candy. The driver, dressed in a sparkling tuxedo, reaches out towards the stars, his expression a mix of charm and wistfulness. The scene is bathed in soft pastel hues, creating a magical yet bittersweet contrast between dreams and reality.\",\n",
      "        \"action_sequence\": \"As the Kustang twirls among the clouds, the driver gestures dramatically, attempting to grasp his elusive dreams. Each failed encounter with girls appears as humorous sketches around him, adding a lighthearted touch to his journey.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 114.0,\n",
      "        \"text\": \"With dreams of romance, but alas, known come true\\nHe's the night's brightest star, if only he knew\\nBut the Kustang, oh Kustang, a lair and a skill\\nIn his mind, he's a hero, a Casanova in steel\\nBut reality's harsh, makes a hard deal\\nWith every failed attempt, it's a comedic reel\",\n",
      "        \"scene_description\": \"In a whimsical, dream-like sequence, the Kustang transforms into a radiant star against a backdrop of a starry night sky. The car glows with ethereal light, swirling among fluffy clouds of cotton candy. The driver, dressed in a sparkling tuxedo, reaches out towards the stars, his expression a mix of charm and wistfulness. The scene is bathed in soft pastel hues, creating a magical yet bittersweet contrast between dreams and reality.\",\n",
      "        \"action_sequence\": \"As the Kustang twirls among the clouds, the driver gestures dramatically, attempting to grasp his elusive dreams. Each failed encounter with girls appears as humorous sketches around him, adding a lighthearted touch to his journey.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 132.0,\n",
      "        \"text\": \"With dreams of romance, but alas, known come true\\nHe's the night's brightest star, if only he knew\\nBut the Kustang, oh Kustang, a lair and a skill\\nIn his mind, he's a hero, a Casanova in steel\\nBut reality's harsh, makes a hard deal\\nWith every failed attempt, it's a comedic reel\",\n",
      "        \"scene_description\": \"In a whimsical, dream-like sequence, the Kustang transforms into a radiant star against a backdrop of a starry night sky. The car glows with ethereal light, swirling among fluffy clouds of cotton candy. The driver, dressed in a sparkling tuxedo, reaches out towards the stars, his expression a mix of charm and wistfulness. The scene is bathed in soft pastel hues, creating a magical yet bittersweet contrast between dreams and reality.\",\n",
      "        \"action_sequence\": \"As the Kustang twirls among the clouds, the driver gestures dramatically, attempting to grasp his elusive dreams. Each failed encounter with girls appears as humorous sketches around him, adding a lighthearted touch to his journey.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 150.0,\n",
      "        \"text\": \"With dreams of romance, but alas, known come true\\nHe's the night's brightest star, if only he knew\\nBut the Kustang, oh Kustang, a lair and a skill\\nIn his mind, he's a hero, a Casanova in steel\\nBut reality's harsh, makes a hard deal\\nWith every failed attempt, it's a comedic reel\",\n",
      "        \"scene_description\": \"In a whimsical, dream-like sequence, the Kustang transforms into a radiant star against a backdrop of a starry night sky. The car glows with ethereal light, swirling among fluffy clouds of cotton candy. The driver, dressed in a sparkling tuxedo, reaches out towards the stars, his expression a mix of charm and wistfulness. The scene is bathed in soft pastel hues, creating a magical yet bittersweet contrast between dreams and reality.\",\n",
      "        \"action_sequence\": \"As the Kustang twirls among the clouds, the driver gestures dramatically, attempting to grasp his elusive dreams. Each failed encounter with girls appears as humorous sketches around him, adding a lighthearted touch to his journey.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 168.0,\n",
      "        \"text\": \"With dreams of romance, but alas, known come true\\nHe's the night's brightest star, if only he knew\\nBut the Kustang, oh Kustang, a lair and a skill\\nIn his mind, he's a hero, a Casanova in steel\\nBut reality's harsh, makes a hard deal\\nWith every failed attempt, it's a comedic reel\",\n",
      "        \"scene_description\": \"In a whimsical, dream-like sequence, the Kustang transforms into a radiant star against a backdrop of a starry night sky. The car glows with ethereal light, swirling among fluffy clouds of cotton candy. The driver, dressed in a sparkling tuxedo, reaches out towards the stars, his expression a mix of charm and wistfulness. The scene is bathed in soft pastel hues, creating a magical yet bittersweet contrast between dreams and reality.\",\n",
      "        \"action_sequence\": \"As the Kustang twirls among the clouds, the driver gestures dramatically, attempting to grasp his elusive dreams. Each failed encounter with girls appears as humorous sketches around him, adding a lighthearted touch to his journey.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 180.0,\n",
      "        \"text\": \"So here's to the ride and its final parts parade\\nA legacy in metal that never will fade\\nAnd memories and stories, its legends been made\\nThe Kustang's journey from spotlight to shade\",\n",
      "        \"scene_description\": \"In a poignant moment, the scene shifts to a tranquil junkyard, where the remnants of the Kustang bask in the warm glow of sunset. Car parts lie scattered like relics of a glorious past, each piece whispering tales of adventure. The atmosphere is nostalgic, blending hues of orange and red, evoking a bittersweet farewell as shadows stretch across the ground, celebrating the legacy of the Kustang.\",\n",
      "        \"action_sequence\": \"The camera glides slowly over the remnants of the Kustang, pausing on a wheel and a seat, honoring their history. The sun sets in the background, casting a warm glow, while a gentle breeze rustles through the debris, echoing stories of the past.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 198.0,\n",
      "        \"text\": \"So here's to the ride and its final parts parade\\nA legacy in metal that never will fade\\nAnd memories and stories, its legends been made\\nThe Kustang's journey from spotlight to shade\",\n",
      "        \"scene_description\": \"In a poignant moment, the scene shifts to a tranquil junkyard, where the remnants of the Kustang bask in the warm glow of sunset. Car parts lie scattered like relics of a glorious past, each piece whispering tales of adventure. The atmosphere is nostalgic, blending hues of orange and red, evoking a bittersweet farewell as shadows stretch across the ground, celebrating the legacy of the Kustang.\",\n",
      "        \"action_sequence\": \"The camera glides slowly over the remnants of the Kustang, pausing on a wheel and a seat, honoring their history. The sun sets in the background, casting a warm glow, while a gentle breeze rustles through the debris, echoing stories of the past.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 216.0,\n",
      "        \"text\": \"So here's to the ride and its final parts parade\\nA legacy in metal that never will fade\\nAnd memories and stories, its legends been made\\nThe Kustang's journey from spotlight to shade\",\n",
      "        \"scene_description\": \"In a poignant moment, the scene shifts to a tranquil junkyard, where the remnants of the Kustang bask in the warm glow of sunset. Car parts lie scattered like relics of a glorious past, each piece whispering tales of adventure. The atmosphere is nostalgic, blending hues of orange and red, evoking a bittersweet farewell as shadows stretch across the ground, celebrating the legacy of the Kustang.\",\n",
      "        \"action_sequence\": \"The camera glides slowly over the remnants of the Kustang, pausing on a wheel and a seat, honoring their history. The sun sets in the background, casting a warm glow, while a gentle breeze rustles through the debris, echoing stories of the past.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 234.0,\n",
      "        \"text\": \"So here's to the ride and its final parts parade\\nA legacy in metal that never will fade\\nAnd memories and stories, its legends been made\\nThe Kustang's journey from spotlight to shade\",\n",
      "        \"scene_description\": \"In a poignant moment, the scene shifts to a tranquil junkyard, where the remnants of the Kustang bask in the warm glow of sunset. Car parts lie scattered like relics of a glorious past, each piece whispering tales of adventure. The atmosphere is nostalgic, blending hues of orange and red, evoking a bittersweet farewell as shadows stretch across the ground, celebrating the legacy of the Kustang.\",\n",
      "        \"action_sequence\": \"The camera glides slowly over the remnants of the Kustang, pausing on a wheel and a seat, honoring their history. The sun sets in the background, casting a warm glow, while a gentle breeze rustles through the debris, echoing stories of the past.\"\n",
      "    }\n",
      "]\n",
      "last_end_value: 242.0 timestamp: 20241122_224300\n",
      "Load Flux pipeline and generate images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e87f05c2a4b48259b4d3a72e217e053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (91 > 77). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66878ade7d594d8daf5d07f923b32a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898bd04a6bb3436cb83864872d3523cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d204a1f596445f98283195b7e36862f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae67963e21b46888c21f5fb3a8acd8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca7d297f0664a619fd58d88c1dd8da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087397c49ad94232a523030553c2076b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e900f1de702b427c8612a51477a92c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbb0d156f0c495c812be481d51efaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c46304f48f4c788bc04001bce492dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b381a5da01d443dae555040d554233b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f5e068351f4240ad737257fa3224e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661732a65fcc43d786a0f8a90e8233ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218b5047a6134b33bdc8137bf511f35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f58848b624478fa1d55e7806fac70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a516aa00b26b42fb88d5c1068149d47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Video Pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'target_shift_terminal': 0.1} were passed to RectifiedFlowScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2dc8b211c734f568de3965505d99315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92362f68573143f697f75fdd0dceb0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n",
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1 has 3 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9934b16424b04f82ba2acd09382e7774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/hulk/miniconda3/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\n",
      "WARNING:py.warnings:/home/hulk/repos/SynthesisEngineering/videoSynthesis/xora/pipelines/pipeline_xora_video.py:1071: FutureWarning: Accessing config attribute `in_channels` directly via 'Transformer3DModel' object attribute is deprecated. Please access 'in_channels' over 'Transformer3DModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  out_channels=self.transformer.in_channels\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbd4bf4648e4503820b3b81fbbf3af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n",
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f528f43d623d4977a336de63f5abda7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n",
      "Scene 2 has 3 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93417e52fe4c4bb6b6364d1174fb6fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n",
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf61f020ef34781bfca6891c0a8f0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f4d4cfc5fa4632962b9eb97fe2f9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n",
      "Scene 3 has 3 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bbc3aae86a4f2aac6cd9ac4d28401d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n",
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b83885d56047f997877c6d259d2282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c94d60789bd482daf7f2ee351d4263a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n",
      "Scene 4 has 3 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11a91ba9cb545a7a1baba2812f9ac01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n",
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d3c0191e554222bf8a9b26a2da5ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647571ee9a3b498e91b6d31eece757dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n",
      "Scene 5 has 2 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987cac8fd09a47c78a9ccf0f42f98ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n",
      "WARNING:xora.pipelines.pipeline_xora_video:\n",
      "Setting `clean_caption=True` requires the ftfy library but it was not found in your environment. Checkout the instructions on the\n",
      "installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "WARNING:xora.pipelines.pipeline_xora_video:Setting `clean_caption` to False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Coustang_20241122_224300/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d0c7ab4e3c45a1b879dce6c1815ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run new systems\n",
    "for audio_file in CONFIG[\"audio_files\"]:\n",
    "    create_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f01978-588d-4f2a-910a-68786df670de",
   "metadata": {},
   "source": [
    "### Run previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba626ac1-4c9d-487c-8939-afb86fde6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run saved config\n",
    "scenes_file_path = './images/AlphabetJoy_20241117_152536/scenes.json'\n",
    "audio_images_dir = './images/AlphabetJoy_20241117_152536'\n",
    "audio_videos_dir = './output/AlphabetJoy_20241117_152536'\n",
    "timestamp = '20241117_152536'\n",
    "last_end_value = 199.4\n",
    "\n",
    "with open(scenes_file_path, \"r\") as scenes_file:\n",
    "    scenes = json.load(scenes_file)\n",
    "\n",
    "process_audio_video(CONFIG, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a0750-5e16-4713-8ac3-825cb8713762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee95d39-df0a-49c5-96ca-e5164d3a4785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
