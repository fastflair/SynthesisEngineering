{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f87d72-5305-4778-ba0c-31549333c8db",
   "metadata": {},
   "source": [
    "# Music Video Synthesis\n",
    "* Extract lyrics from song with timestamps\n",
    "* Compose scenes, include timestamps\n",
    "* Construct video text prompt for each scene\n",
    "* Build videos for each scene\n",
    "* Stitch together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b074-e32d-45f2-977f-c923878625e6",
   "metadata": {},
   "source": [
    "# We will use openai whipser for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45c210-fd2b-4381-9fd3-c1eb18feefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet --upgrade pip\n",
    "#!pip install --quiet --upgrade openai-whisper\n",
    "# Ubuntu or Debian\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!pip install setuptools-rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8537e766-eab2-4757-b6f9-fbac4da44930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 21:02:45.757601: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-09 21:02:45.838527: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-09 21:02:45.862794: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 21:02:46.036550: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 21:02:47.684098: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import diffusers\n",
    "import gc\n",
    "import imageio\n",
    "import imageio_ffmpeg\n",
    "import json\n",
    "import math\n",
    "import moviepy.editor as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "import whisper\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from diffusers import AutoencoderKL, AutoencoderKLCogVideoX, AutoPipelineForText2Image, CogVideoXTransformer3DModel, CogVideoXPipeline, CogVideoXDPMScheduler\n",
    "from diffusers import CogVideoXTransformer3DModel, CogVideoXImageToVideoPipeline\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from diffusers.utils import export_to_video, load_video, load_image\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from openai import OpenAI\n",
    "from optimum.quanto import freeze, qfloat8, quantize, requantize\n",
    "from PIL import Image\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_flux1\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from transformers import CLIPTextModel, T5EncoderModel\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "retry_limit = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f54ba32-2208-45c8-8ed2-5fcb1e79aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"openai_api_key\": \"\",\n",
    "    \"openai_model\": \"gpt-4o-mini\",\n",
    "    \"openai_model_large\": \"gpt-4o\",\n",
    "    \"hf_token\": \"\",\n",
    "    \"base_working_dir\": \"./images\",\n",
    "    \"base_video_dir\": \"./output\",\n",
    "    \"audio_files\": [\n",
    "        \"//mnt/d/audio/VampireLament.mp3\",\n",
    "        \"//mnt/d/audio/BeethovenGhost.mp3\",\n",
    "        \"//mnt/d/audio/VampirePriest.mp3\",\n",
    "        \"//mnt/d/audio/PhantomRasta.mp3\",\n",
    "        \"//mnt/d/audio/LonesomeGhost.mp3\",\n",
    "        \"//mnt/d/audio/ChupacabraTease.mp3\",\n",
    "        # Add more audio file paths here\n",
    "    ],\n",
    "    \"device\": device,\n",
    "    \"dtype\": dtype,\n",
    "    \"retry_limit\": retry_limit,\n",
    "    \"MAX_SEED\": MAX_SEED,\n",
    "}\n",
    "\n",
    "# Ensure base directories exist\n",
    "os.makedirs(CONFIG[\"base_working_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"base_video_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34281c1-e231-4472-9d9b-096fa23b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_prompt_response(\n",
    "    prompt: str,\n",
    "    config: dict,\n",
    "    max_tokens: int = 6000,\n",
    "    temperature: float = 0.33,\n",
    "    openai_model: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response with retry logic.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Act as a helpful assistant, you are an expert editor.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=openai_model or config[\"openai_model\"],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < config[\"retry_limit\"]:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            return message_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == config[\"retry_limit\"]:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{config['retry_limit']})\")\n",
    "                time.sleep(1)  # Optional: wait before retrying\n",
    "\n",
    "\n",
    "def load_quanto_transformer(repo_path):\n",
    "    with open(hf_hub_download(repo_path, \"transformer/quantization_map.json\"), \"r\") as f:\n",
    "        quantization_map = json.load(f)\n",
    "    with torch.device(\"meta\"):\n",
    "        transformer = diffusers.FluxTransformer2DModel.from_config(hf_hub_download(repo_path, \"transformer/config.json\")).to(dtype)\n",
    "    state_dict = load_safetensors(hf_hub_download(repo_path, \"transformer/diffusion_pytorch_model.safetensors\"))\n",
    "    requantize(transformer, state_dict, quantization_map, device=torch.device(\"cuda\"))\n",
    "    return transformer\n",
    "\n",
    "\n",
    "def load_quanto_text_encoder_2_longer(repo_path, max_length=512):\n",
    "    with open(hf_hub_download(repo_path, \"text_encoder_2/quantization_map.json\"), \"r\") as f:\n",
    "        quantization_map = json.load(f)\n",
    "    with open(hf_hub_download(repo_path, \"text_encoder_2/config.json\")) as f:\n",
    "        t5_config = transformers.T5Config(**json.load(f))\n",
    "    \n",
    "    # Update the config for longer sequence length\n",
    "    t5_config.max_position_embeddings = max_length\n",
    "    \n",
    "    with torch.device(\"meta\"):\n",
    "        text_encoder_2 = transformers.T5EncoderModel(t5_config).to(dtype)\n",
    "    \n",
    "    state_dict = load_safetensors(hf_hub_download(repo_path, \"text_encoder_2/model.safetensors\"))\n",
    "    requantize(text_encoder_2, state_dict, quantization_map, device=torch.device(\"cuda\"))\n",
    "    \n",
    "    return text_encoder_2\n",
    "\n",
    "\n",
    "def load_flux_pipe():\n",
    "    # Load the main pipeline without the transformer or text_encoder_2 initially\n",
    "    pipe = None\n",
    "    clip_repo = \"zer0int/CLIP-GmP-ViT-L-14\"\n",
    "    text_encoder = CLIPTextModel.from_pretrained(clip_repo, torch_dtype=dtype)\n",
    "    \n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "        \"Disty0/FLUX.1-dev-qint8\", \n",
    "        text_encoder=text_encoder,\n",
    "        transformer=None, \n",
    "        text_encoder_2=None, \n",
    "        torch_dtype=dtype\n",
    "    )\n",
    "    \n",
    "    # Load custom transformer and text encoder with specific configurations\n",
    "    pipe.transformer = load_quanto_transformer(\"Disty0/FLUX.1-dev-qint8\")\n",
    "    pipe.text_encoder_2 = load_quanto_text_encoder_2_longer(\n",
    "        \"Disty0/FLUX.1-dev-qint8\", \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Move the pipeline to CUDA with bfloat16 precision for performance\n",
    "    pipe = pipe.to(\"cuda\", dtype=dtype)\n",
    "\n",
    "    # Enable memory optimizations (attention slicing)\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def gen_flux_image(pipe, prompt, config: dict, height=1024, width=1024, guidance_scale=3.5, num_inference_steps=32, max_sequence_length=512, seed=-1):\n",
    "    \"\"\"\n",
    "    Generates an image based on the provided prompt using the Flux pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "        \n",
    "    prompt_embeds, pooled_prompt_embeds = get_weighted_text_embeddings_flux1(\n",
    "        pipe        = pipe,\n",
    "        prompt    = prompt\n",
    "    )\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt_embeds               = prompt_embeds,\n",
    "        pooled_prompt_embeds      = pooled_prompt_embeds,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        guidance_scale=guidance_scale,\n",
    "        output_type=\"pil\",\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        max_sequence_length=max_sequence_length,\n",
    "        generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "    ).images[0]\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_video_pipeline():\n",
    "    \"\"\"\n",
    "    Loads and configures the video generation pipeline.\n",
    "    \"\"\"\n",
    "    quantization = int8_weight_only\n",
    "\n",
    "    text_encoder = T5EncoderModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\n",
    "    quantize_(text_encoder, quantization())\n",
    "    \n",
    "    transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n",
    "    quantize_(transformer, quantization())\n",
    "    \n",
    "    i2v_transformer = CogVideoXTransformer3DModel.from_pretrained(\n",
    "        \"THUDM/CogVideoX-5b-I2V\", subfolder=\"transformer\", torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"vae\", torch_dtype=torch.bfloat16)\n",
    "    quantize_(vae, quantization())\n",
    "    \n",
    "    # Create pipeline and run inference\n",
    "    pipe = CogVideoXPipeline.from_pretrained(\n",
    "        \"THUDM/CogVideoX-5b\",\n",
    "        text_encoder=text_encoder,\n",
    "        transformer=transformer,\n",
    "        vae=vae,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    #pipe.enable_model_cpu_offload()\n",
    "    pipe.vae.enable_tiling()\n",
    "    \n",
    "    pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "    \n",
    "    i2v_vae=pipe.vae\n",
    "    i2v_scheduler=pipe.scheduler\n",
    "    i2v_tokenizer=pipe.tokenizer\n",
    "    i2v_text_encoder=pipe.text_encoder\n",
    "    \n",
    "    del pipe\n",
    "    gc.collect()\n",
    "    \n",
    "    # Load the pipeline once before the loop\n",
    "    pipe_image = CogVideoXImageToVideoPipeline.from_pretrained(\n",
    "        \"THUDM/CogVideoX-5b-I2V\",\n",
    "        transformer=i2v_transformer,\n",
    "        vae=i2v_vae,\n",
    "        scheduler=i2v_scheduler,\n",
    "        tokenizer=i2v_tokenizer,\n",
    "        text_encoder=i2v_text_encoder,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).to(device)\n",
    "\n",
    "    pipe_image.enable_sequential_cpu_offload()\n",
    "    pipe_image.vae.enable_slicing()\n",
    "    pipe_image.vae.enable_tiling()\n",
    "\n",
    "    return pipe_image\n",
    "\n",
    "\n",
    "def infer(pipe_image, prompt: str, image_input: str, config: dict, num_inference_steps: int = 50, guidance_scale: float = 7.0, seed: int = -1, num_frames: int = 49):\n",
    "    \"\"\"\n",
    "    Generates video frames from an image and prompt using the video pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 255)\n",
    "\n",
    "    image_input = Image.open(image_input).resize(size=(720, 480))  # Convert to PIL\n",
    "    image = load_image(image_input)\n",
    "\n",
    "    video_pt = pipe_image(\n",
    "        image=image,\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        num_videos_per_prompt=1,\n",
    "        use_dynamic_cfg=True,\n",
    "        output_type=\"pt\",\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=torch.Generator(device=\"cpu\").manual_seed(seed),\n",
    "        num_frames=num_frames,\n",
    "    ).frames\n",
    "\n",
    "    return video_pt, seed\n",
    "\n",
    "\n",
    "def generate_video(pipe_image, prompt, image_input, config: dict, seed_value: int = -1, video_filename: str = \"\", num_frames: int = 49):\n",
    "    \"\"\"\n",
    "    Generates and saves a video from the provided image and prompt.\n",
    "    \"\"\"\n",
    "    latents, seed = infer(\n",
    "        pipe_image,\n",
    "        prompt,\n",
    "        image_input,\n",
    "        config,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.0,\n",
    "        seed=seed_value,\n",
    "        num_frames=num_frames,\n",
    "    )\n",
    "    batch_size = latents.shape[0]\n",
    "    batch_video_frames = []\n",
    "    for batch_idx in range(batch_size):\n",
    "        pt_image = latents[batch_idx]\n",
    "        pt_image = torch.stack([pt_image[i] for i in range(pt_image.shape[0])])\n",
    "        image_np = VaeImageProcessor.pt_to_numpy(pt_image)\n",
    "        image_pil = VaeImageProcessor.numpy_to_pil(image_np)\n",
    "        batch_video_frames.append(image_pil)\n",
    "\n",
    "    video_path = save_video(\n",
    "        batch_video_frames[0],\n",
    "        fps=math.ceil((len(batch_video_frames[0]) - 1) / 6),\n",
    "        filename=video_filename\n",
    "    )\n",
    "    return video_path\n",
    "\n",
    "\n",
    "def save_video(frames, fps: int, filename: str):\n",
    "    \"\"\"\n",
    "    Saves a list of frames as a video file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        writer = imageio.get_writer(temp_video_path, fps=fps)\n",
    "        for frame in frames:\n",
    "            writer.append_data(np.array(frame))\n",
    "        writer.close()\n",
    "\n",
    "    os.rename(temp_video_path, filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def convert_to_gif(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a video file to a GIF.\n",
    "    \"\"\"\n",
    "    clip = mp.VideoFileClip(video_path)\n",
    "    clip = clip.set_fps(8)\n",
    "    clip = clip.resize(height=240)\n",
    "    gif_path = video_path.replace(\".mp4\", \".gif\")\n",
    "    clip.write_gif(gif_path, fps=8)\n",
    "    return gif_path\n",
    "\n",
    "\n",
    "def resize_if_unfit(input_video: str) -> str:\n",
    "    \"\"\"\n",
    "    Resizes the video to the target dimensions if it does not match.\n",
    "    \"\"\"\n",
    "    width, height = get_video_dimensions(input_video)\n",
    "\n",
    "    if width == 720 and height == 480:\n",
    "        return input_video\n",
    "    else:\n",
    "        return center_crop_resize(input_video)\n",
    "\n",
    "\n",
    "def get_video_dimensions(input_video_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Retrieves the dimensions of the video.\n",
    "    \"\"\"\n",
    "    reader = imageio_ffmpeg.read_frames(input_video_path)\n",
    "    metadata = next(reader)\n",
    "    return metadata[\"size\"]\n",
    "\n",
    "\n",
    "def center_crop_resize(input_video_path: str, target_width: int = 720, target_height: int = 480) -> str:\n",
    "    \"\"\"\n",
    "    Resizes and center-crops the video to the target dimensions.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    width_factor = target_width / orig_width\n",
    "    height_factor = target_height / orig_height\n",
    "    resize_factor = max(width_factor, height_factor)\n",
    "\n",
    "    inter_width = int(orig_width * resize_factor)\n",
    "    inter_height = int(orig_height * resize_factor)\n",
    "\n",
    "    target_fps = 8\n",
    "    ideal_skip = max(0, math.ceil(orig_fps / target_fps) - 1)\n",
    "    skip = min(5, ideal_skip)  # Cap at 5\n",
    "\n",
    "    while (total_frames / (skip + 1)) < 49 and skip > 0:\n",
    "        skip -= 1\n",
    "\n",
    "    processed_frames = []\n",
    "    frame_count = 0\n",
    "    total_read = 0\n",
    "\n",
    "    while frame_count < 49 and total_read < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if total_read % (skip + 1) == 0:\n",
    "            resized = cv2.resize(frame, (inter_width, inter_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            start_x = (inter_width - target_width) // 2\n",
    "            start_y = (inter_height - target_height) // 2\n",
    "            cropped = resized[start_y:start_y + target_height, start_x:start_x + target_width]\n",
    "\n",
    "            processed_frames.append(cropped)\n",
    "            frame_count += 1\n",
    "\n",
    "        total_read += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(temp_video_path, fourcc, target_fps, (target_width, target_height))\n",
    "\n",
    "        for frame in processed_frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    return temp_video_path\n",
    "\n",
    "\n",
    "def extract_last_frame(video_filename: str, output_image_filename: str):\n",
    "    \"\"\"\n",
    "    Extracts the last frame from a video file and saves it as an image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = imageio.get_reader(video_filename, 'ffmpeg')\n",
    "        last_frame = None\n",
    "        for frame in reader:\n",
    "            last_frame = frame\n",
    "        reader.close()\n",
    "\n",
    "        if last_frame is not None:\n",
    "            imageio.imwrite(output_image_filename, last_frame)\n",
    "            print(f\"Last frame saved successfully as '{output_image_filename}'.\")\n",
    "        else:\n",
    "            print(\"The video contains no frames.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{video_filename}' was not found.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except RuntimeError as re:\n",
    "        print(f\"RuntimeError: {re}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def create_scenes(text: str, video_summary: str, config: dict):\n",
    "    \"\"\"\n",
    "    Creates scenes based on the extracted lyrics using OpenAI's API.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Create a json list of scenes (groupings of text), scene_description (200 words of less), and action_sequence (30 words or less) from the following text.  Scenes should be groups of similar lyrics with new scenes when the context changes.  Text: {text}   \n",
    "The json list should have the start value for the first item in the scene and the text that is combined for all items in the same scene.  \n",
    "The scene_description should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes.\n",
    "The action_sequence should describe the action in the scene.  Scenes should be unique, creative, imaginative, and awe inspiring create a viral hit animation.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"])\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def process_audio_scenes(audio_file: str, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a single audio file through the entire workflow.\n",
    "    \"\"\"\n",
    "    # Create unique identifier based on audio file name\n",
    "    audio_basename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = f\"{audio_basename}_{timestamp}\"\n",
    "\n",
    "    # Create unique directories for images and videos\n",
    "    print(f\"Create unique directories for images and videos\")\n",
    "    audio_images_dir = os.path.join(config[\"base_working_dir\"], unique_id)\n",
    "    audio_videos_dir = os.path.join(config[\"base_video_dir\"], unique_id)\n",
    "    os.makedirs(audio_images_dir, exist_ok=True)\n",
    "    os.makedirs(audio_videos_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Transcribe audio using Whisper\n",
    "    print(f\"Transcribe audio using Whisper\")\n",
    "    model = whisper.load_model(\"turbo\")\n",
    "    result = model.transcribe(audio_file)\n",
    "\n",
    "    # Cleanup Whisper model memory\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    segments = result['segments']\n",
    "\n",
    "    # Extract list of start times and texts\n",
    "    segment_texts_and_start_times = [(segment['text'].strip(), segment['start']) for segment in segments]\n",
    "\n",
    "    # Combine texts\n",
    "    text = \"\"\n",
    "    for segment_text, start in segment_texts_and_start_times:\n",
    "        text += f\"Start: {start}, Text: {segment_text}\\n\"\n",
    "\n",
    "    last_end_value = segments[-1]['end']\n",
    "\n",
    "    # Step 2: Generate video summary using OpenAI\n",
    "    print(f\"Generate video summary using OpenAI\")\n",
    "    video_summary_prompt = f'Create a short summary that describes a music video based on these lyrics: {text}'\n",
    "    video_summary = get_openai_prompt_response(video_summary_prompt, config, openai_model=config[\"openai_model\"])\n",
    "\n",
    "    # Step 3: Create scenes based on lyrics\n",
    "    print(f\"Create scenes based on lyrics\")\n",
    "    scenes = create_scenes(text, video_summary, config)\n",
    "    \n",
    "    return scenes, audio_images_dir, audio_videos_dir\n",
    "\n",
    "def process_audio_images(config: dict, scenes, audio_images_dir):\n",
    "    # Step 4: Load Flux pipeline and generate images\n",
    "    print(f\"Load Flux pipeline and generate images\")\n",
    "    flux_pipe = load_flux_pipe()\n",
    "    height = 480\n",
    "    width = 720\n",
    "    guidance_scale = 3.9\n",
    "    num_inference_steps = 24\n",
    "    max_sequence_length = 512\n",
    "    seed = -1\n",
    "\n",
    "    # Generate images for each scene\n",
    "    image_num = 1\n",
    "    for scene in scenes:\n",
    "        image_prompt = scene['scene_description']\n",
    "        image = gen_flux_image(flux_pipe, image_prompt, config, height, width, guidance_scale, num_inference_steps, max_sequence_length, seed)\n",
    "        filename = f\"image_{str(image_num).zfill(2)}.jpg\"\n",
    "        image_path = os.path.join(audio_images_dir, filename)\n",
    "        image.save(image_path, dpi=(300, 300))\n",
    "        image_num += 1\n",
    "\n",
    "    return\n",
    "\n",
    "def process_audio_video(config: dict, scenes, audio_images_dir, audio_videos_dir):\n",
    "    # Step 6: Load Video Pipeline\n",
    "    print(f\"Load Video Pipeline\")\n",
    "    video_pipe = load_video_pipeline()\n",
    "\n",
    "    # Temporary image path\n",
    "    temp_image = os.path.join(audio_images_dir, \"temp_image.jpg\")\n",
    "    video_num = 1\n",
    "\n",
    "    # Step 7: Generate video sequences\n",
    "    for i, scene in enumerate(scenes):\n",
    "        prompt = scene[\"action_sequence\"]\n",
    "\n",
    "        # Use the initial image for each scene\n",
    "        image_input = os.path.join(audio_images_dir, f\"image_{str(i+1).zfill(2)}.jpg\")\n",
    "\n",
    "        # Calculate duration to keep the video in 6-second increments\n",
    "        if i + 1 < len(scenes):\n",
    "            next_start_time = scenes[i + 1][\"start\"]\n",
    "        else:\n",
    "            next_start_time = last_end_value  # Use the final ending time for the last scene\n",
    "\n",
    "        duration = next_start_time - scene[\"start\"]\n",
    "        num_video_segments = int((duration + 3) // 6)\n",
    "\n",
    "        print(f\"Scene {i} has {num_video_segments} segments\")\n",
    "        for j in range(num_video_segments):\n",
    "            video_name = f\"video_{str(video_num).zfill(2)}_{str(j).zfill(2)}.mp4\"\n",
    "            video_output_path = os.path.join(audio_videos_dir, video_name)\n",
    "            generate_video(video_pipe, prompt, image_input, config, seed_value=-1, video_filename=video_output_path)\n",
    "            time.sleep(1)  # Pause for 1 second\n",
    "\n",
    "            # After generating the video, extract the last frame to use as input for the next segment\n",
    "            extract_last_frame(video_output_path, temp_image)\n",
    "\n",
    "            # Use the last frame as input for the next video segment in the same scene\n",
    "            image_input = temp_image\n",
    "\n",
    "            video_num += 1  # Increment video number for the next segment\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def process_all_audios(audio_files: list, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a list of audio files through the workflow.\n",
    "    \"\"\"\n",
    "    for audio_file in audio_files:\n",
    "        print(f\"Processing audio file: {audio_file}\")\n",
    "        scenes, audio_images_dir, audio_videos_dir = process_audio_scenes(audio_file, config)\n",
    "        # Cleanup Video pipeline memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        # Create starting images for scenes\n",
    "        process_audio_images(config, scenes, audio_images_dir)\n",
    "        # Cleanup Video pipeline memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        # Create starting images for scenes\n",
    "        process_audio_video(config, scenes, audio_images_dir, audio_videos_dir)\n",
    "        # Cleanup Video pipeline memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0cc96-e609-4cd2-9c67-dcd63562988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file: //mnt/d/audio/VampireLament.mp3\n",
      "Create unique directories for images and videos\n",
      "Transcribe audio using Whisper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/gensyn/anaconda3/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate video summary using OpenAI\n",
      "Create scenes based on lyrics\n",
      "Load Flux pipeline and generate images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c51aa2aa1d47928801c9dca36a0350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "WARNING:py.warnings:/home/gensyn/anaconda3/lib/python3.11/site-packages/diffusers/configuration_utils.py:245: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'diffusers.models.transformers.transformer_flux.FluxTransformer2DModel'>.load_config(...) followed by <class 'diffusers.models.transformers.transformer_flux.FluxTransformer2DModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "process_all_audios(CONFIG[\"audio_files\"], CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bbb275-c2d2-4e31-a66e-c9cd9bc46941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eed4c1-9a20-431e-92ca-a050203bd2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
