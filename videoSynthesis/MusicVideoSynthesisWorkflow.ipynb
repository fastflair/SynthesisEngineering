{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f87d72-5305-4778-ba0c-31549333c8db",
   "metadata": {},
   "source": [
    "# Music Video Synthesis\n",
    "* Extract lyrics from song with timestamps\n",
    "* Compose scenes, include timestamps\n",
    "* Construct video text prompt for each scene\n",
    "* Build videos for each scene\n",
    "* Stitch together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b074-e32d-45f2-977f-c923878625e6",
   "metadata": {},
   "source": [
    "# We will use openai whipser for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45c210-fd2b-4381-9fd3-c1eb18feefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet --upgrade pip\n",
    "#!pip install --quiet --upgrade openai-whisper openai\n",
    "# Ubuntu or Debian\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!pip install setuptools-rust\n",
    "#!pip install -U diffusers imageio imageio_ffmpeg opencv-python moviepy transformers huggingface-hub optimum pillow safetensors optimum-quanto accelerate\n",
    "#!pip install --upgrade optimum-quanto torchao --extra-index-url https://download.pytorch.org/whl/cu124 # full options are cpu/cu118/cu121/cu124\n",
    "#!pip install git+https://github.com/xhinker/sd_embed.git@main\n",
    "#!pip install accelerate flash_attention -U\n",
    "#!pip install flash_attn --no-build-isolation\n",
    "#!pip install -r requirements.txt -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537e766-eab2-4757-b6f9-fbac4da44930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import diffusers\n",
    "import gc\n",
    "import imageio\n",
    "import imageio_ffmpeg\n",
    "import json\n",
    "import math\n",
    "import moviepy.editor as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import whisper\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from diffusers import AutoencoderKL, AutoencoderKLCogVideoX, AutoPipelineForText2Image, CogVideoXTransformer3DModel, CogVideoXPipeline, CogVideoXDPMScheduler\n",
    "from diffusers import CogVideoXTransformer3DModel, CogVideoXImageToVideoPipeline, FlowMatchEulerDiscreteScheduler\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from diffusers.utils import export_to_video, load_video, load_image\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from model4 import T5EncoderModel as m_T5EncoderModel, FluxTransformer2DModel\n",
    "from numba import cuda\n",
    "from openai import OpenAI\n",
    "from optimum.quanto import freeze, qfloat8, quantize, requantize\n",
    "from PIL import Image\n",
    "from safetensors.torch import load_file as load_safetensors, save_file as save_safetensors\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_flux1\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5TokenizerFast, T5EncoderModel as t_T5EncoderModel\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Define the paths where quantized weights will be saved\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "retry_limit = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54ba32-2208-45c8-8ed2-5fcb1e79aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"openai_api_key\": \"\",\n",
    "    \"openai_model\": \"gpt-4o-mini\",\n",
    "    \"openai_model_large\": \"gpt-4o\",\n",
    "    \"hf_token\": \"\",\n",
    "    \"base_working_dir\": \"./images\",\n",
    "    \"base_video_dir\": \"./output\",\n",
    "    \"audio_files\": [\n",
    "        \"//mnt/d/audio/LonesomeGhost.mp3\",\n",
    "        \"//mnt/d/audio/ChupacabraTease.mp3\",\n",
    "        # Add more audio file paths here\n",
    "    ],\n",
    "    \"device\": device,\n",
    "    \"dtype\": dtype,\n",
    "    \"retry_limit\": retry_limit,\n",
    "    \"MAX_SEED\": MAX_SEED,\n",
    "}\n",
    "\n",
    "# Ensure base directories exist\n",
    "os.makedirs(CONFIG[\"base_working_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"base_video_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34281c1-e231-4472-9d9b-096fa23b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_gpu_memory(*objects):\n",
    "    for obj in objects:\n",
    "        del obj\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "def get_openai_prompt_response(\n",
    "    prompt: str,\n",
    "    config: dict,\n",
    "    max_tokens: int = 6000,\n",
    "    temperature: float = 0.33,\n",
    "    openai_model: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response with retry logic.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Act as a helpful assistant, you are an expert editor.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=openai_model or config[\"openai_model\"],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < config[\"retry_limit\"]:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            return message_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == config[\"retry_limit\"]:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{config['retry_limit']})\")\n",
    "                time.sleep(1)  # Optional: wait before retrying\n",
    "\n",
    "@contextmanager\n",
    "def load_flux_pipe():\n",
    "    try:\n",
    "        text_encoder_2 = m_T5EncoderModel.from_pretrained(\n",
    "            \"HighCWu/FLUX.1-dev-4bit\",\n",
    "            subfolder=\"text_encoder_2\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            # hqq_4bit_compute_dtype=torch.float32,\n",
    "        )\n",
    "        \n",
    "        transformer = FluxTransformer2DModel.from_pretrained(\n",
    "            \"HighCWu/FLUX.1-dev-4bit\",\n",
    "            subfolder=\"transformer\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        \n",
    "        pipe = FluxPipeline.from_pretrained(\n",
    "            \"black-forest-labs/FLUX.1-dev\",\n",
    "            text_encoder_2=text_encoder_2,\n",
    "            transformer=transformer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        #pipe.enable_model_cpu_offload() # with cpu offload, it cost 8.5GB vram\n",
    "        pipe.remove_all_hooks()\n",
    "        pipe = pipe.to('cuda') # without cpu offload, it cost 11GB vram\n",
    "        yield pipe\n",
    "    finally:\n",
    "        del(text_encoder_2)\n",
    "        del(transformer)\n",
    "        del(pipe)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def gen_flux_image(pipe, prompt, config: dict, height=1024, width=1024, guidance_scale=3.5, num_inference_steps=4, max_sequence_length=512, seed=-1):\n",
    "    \"\"\"\n",
    "    Generates an image based on the provided prompt using the Flux pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "        \n",
    "    prompt_embeds, pooled_prompt_embeds = get_weighted_text_embeddings_flux1(\n",
    "        pipe        = pipe,\n",
    "        prompt    = prompt\n",
    "    )\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt_embeds               = prompt_embeds,\n",
    "        pooled_prompt_embeds      = pooled_prompt_embeds,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        guidance_scale=guidance_scale,\n",
    "        output_type=\"pil\",\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        max_sequence_length=max_sequence_length,\n",
    "        generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "    ).images[0]\n",
    "    \n",
    "    return image\n",
    "\n",
    "@contextmanager\n",
    "def load_video_pipeline():\n",
    "    \"\"\"\n",
    "    Loads and configures the video generation pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        quantization = int8_weight_only\n",
    "\n",
    "        text_encoder = t_T5EncoderModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\n",
    "        quantize_(text_encoder, quantization())\n",
    "        \n",
    "        transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n",
    "        quantize_(transformer, quantization())\n",
    "        \n",
    "        i2v_transformer = CogVideoXTransformer3DModel.from_pretrained(\n",
    "            \"THUDM/CogVideoX-5b-I2V\", subfolder=\"transformer\", torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"vae\", torch_dtype=torch.bfloat16)\n",
    "        quantize_(vae, quantization())\n",
    "        \n",
    "        # Create pipeline and run inference\n",
    "        pipe = CogVideoXPipeline.from_pretrained(\n",
    "            \"THUDM/CogVideoX-5b\",\n",
    "            text_encoder=text_encoder,\n",
    "            transformer=transformer,\n",
    "            vae=vae,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        #pipe.enable_model_cpu_offload()\n",
    "        pipe.vae.enable_tiling()\n",
    "        \n",
    "        pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "        \n",
    "        i2v_vae=pipe.vae\n",
    "        i2v_scheduler=pipe.scheduler\n",
    "        i2v_tokenizer=pipe.tokenizer\n",
    "        i2v_text_encoder=pipe.text_encoder\n",
    "        \n",
    "        del pipe\n",
    "        gc.collect()\n",
    "        \n",
    "        # Load the pipeline once before the loop\n",
    "        pipe_image = CogVideoXImageToVideoPipeline.from_pretrained(\n",
    "            \"THUDM/CogVideoX-5b-I2V\",\n",
    "            transformer=i2v_transformer,\n",
    "            vae=i2v_vae,\n",
    "            scheduler=i2v_scheduler,\n",
    "            tokenizer=i2v_tokenizer,\n",
    "            text_encoder=i2v_text_encoder,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        ).to(device)\n",
    "    \n",
    "        yield pipe_image\n",
    "    finally:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def infer(pipe_image, prompt: str, image_input: str, config: dict, num_inference_steps: int = 50, guidance_scale: float = 7.0, seed: int = -1, num_frames: int = 49):\n",
    "    \"\"\"\n",
    "    Generates video frames from an image and prompt using the video pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 255)\n",
    "\n",
    "    image_input = Image.open(image_input).resize(size=(720, 480))  # Convert to PIL\n",
    "    image = load_image(image_input)\n",
    "\n",
    "    video_pt = pipe_image(\n",
    "        image=image,\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        num_videos_per_prompt=1,\n",
    "        use_dynamic_cfg=True,\n",
    "        output_type=\"pt\",\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=torch.Generator(device=\"cpu\").manual_seed(seed),\n",
    "        num_frames=num_frames,\n",
    "    ).frames\n",
    "\n",
    "    return video_pt, seed\n",
    "\n",
    "\n",
    "def generate_video(pipe_image, prompt, image_input, config: dict, seed_value: int = -1, video_filename: str = \"\", num_frames: int = 49):\n",
    "    \"\"\"\n",
    "    Generates and saves a video from the provided image and prompt.\n",
    "    \"\"\"\n",
    "    latents, seed = infer(\n",
    "        pipe_image,\n",
    "        prompt,\n",
    "        image_input,\n",
    "        config,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.0,\n",
    "        seed=seed_value,\n",
    "        num_frames=num_frames,\n",
    "    )\n",
    "    batch_size = latents.shape[0]\n",
    "    batch_video_frames = []\n",
    "    for batch_idx in range(batch_size):\n",
    "        pt_image = latents[batch_idx]\n",
    "        pt_image = torch.stack([pt_image[i] for i in range(pt_image.shape[0])])\n",
    "        image_np = VaeImageProcessor.pt_to_numpy(pt_image)\n",
    "        image_pil = VaeImageProcessor.numpy_to_pil(image_np)\n",
    "        batch_video_frames.append(image_pil)\n",
    "\n",
    "    video_path = save_video(\n",
    "        batch_video_frames[0],\n",
    "        fps=math.ceil((len(batch_video_frames[0]) - 1) / 6),\n",
    "        filename=video_filename\n",
    "    )\n",
    "    return video_path\n",
    "\n",
    "\n",
    "def save_video(frames, fps: int, filename: str):\n",
    "    \"\"\"\n",
    "    Saves a list of frames as a video file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        writer = imageio.get_writer(temp_video_path, fps=fps)\n",
    "        for frame in frames:\n",
    "            writer.append_data(np.array(frame))\n",
    "        writer.close()\n",
    "\n",
    "    os.rename(temp_video_path, filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def convert_to_gif(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a video file to a GIF.\n",
    "    \"\"\"\n",
    "    clip = mp.VideoFileClip(video_path)\n",
    "    clip = clip.set_fps(8)\n",
    "    clip = clip.resize(height=240)\n",
    "    gif_path = video_path.replace(\".mp4\", \".gif\")\n",
    "    clip.write_gif(gif_path, fps=8)\n",
    "    return gif_path\n",
    "\n",
    "\n",
    "def resize_if_unfit(input_video: str) -> str:\n",
    "    \"\"\"\n",
    "    Resizes the video to the target dimensions if it does not match.\n",
    "    \"\"\"\n",
    "    width, height = get_video_dimensions(input_video)\n",
    "\n",
    "    if width == 720 and height == 480:\n",
    "        return input_video\n",
    "    else:\n",
    "        return center_crop_resize(input_video)\n",
    "\n",
    "\n",
    "def get_video_dimensions(input_video_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Retrieves the dimensions of the video.\n",
    "    \"\"\"\n",
    "    reader = imageio_ffmpeg.read_frames(input_video_path)\n",
    "    metadata = next(reader)\n",
    "    return metadata[\"size\"]\n",
    "\n",
    "\n",
    "def center_crop_resize(input_video_path: str, target_width: int = 720, target_height: int = 480) -> str:\n",
    "    \"\"\"\n",
    "    Resizes and center-crops the video to the target dimensions.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    width_factor = target_width / orig_width\n",
    "    height_factor = target_height / orig_height\n",
    "    resize_factor = max(width_factor, height_factor)\n",
    "\n",
    "    inter_width = int(orig_width * resize_factor)\n",
    "    inter_height = int(orig_height * resize_factor)\n",
    "\n",
    "    target_fps = 8\n",
    "    ideal_skip = max(0, math.ceil(orig_fps / target_fps) - 1)\n",
    "    skip = min(5, ideal_skip)  # Cap at 5\n",
    "\n",
    "    while (total_frames / (skip + 1)) < 49 and skip > 0:\n",
    "        skip -= 1\n",
    "\n",
    "    processed_frames = []\n",
    "    frame_count = 0\n",
    "    total_read = 0\n",
    "\n",
    "    while frame_count < 49 and total_read < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if total_read % (skip + 1) == 0:\n",
    "            resized = cv2.resize(frame, (inter_width, inter_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            start_x = (inter_width - target_width) // 2\n",
    "            start_y = (inter_height - target_height) // 2\n",
    "            cropped = resized[start_y:start_y + target_height, start_x:start_x + target_width]\n",
    "\n",
    "            processed_frames.append(cropped)\n",
    "            frame_count += 1\n",
    "\n",
    "        total_read += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(temp_video_path, fourcc, target_fps, (target_width, target_height))\n",
    "\n",
    "        for frame in processed_frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    return temp_video_path\n",
    "\n",
    "\n",
    "def extract_last_frame(video_filename: str, output_image_filename: str):\n",
    "    \"\"\"\n",
    "    Extracts the last frame from a video file and saves it as an image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = imageio.get_reader(video_filename, 'ffmpeg')\n",
    "        last_frame = None\n",
    "        for frame in reader:\n",
    "            last_frame = frame\n",
    "        reader.close()\n",
    "\n",
    "        if last_frame is not None:\n",
    "            imageio.imwrite(output_image_filename, last_frame)\n",
    "            print(f\"Last frame saved successfully as '{output_image_filename}'.\")\n",
    "        else:\n",
    "            print(\"The video contains no frames.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{video_filename}' was not found.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except RuntimeError as re:\n",
    "        print(f\"RuntimeError: {re}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def create_scenes(text: str, video_summary: str, config: dict):\n",
    "    \"\"\"\n",
    "    Creates scenes based on the extracted lyrics using OpenAI's API.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Create a json list of diverse, unique scenes (groupings of text), scene_description (200 words of less), and action_sequence (30 words or less) from the following text.  Scenes should be groups of lyrics with new scenes when the lyric context changes.  Text: {text}   \n",
    "The json list should have the start value for the first item in the scene and the text that is combined for all items in the same scene.  \n",
    "The scene_description should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes.\n",
    "The action_sequence should describe the action in the scene.  Scenes should be unique, creative, imaginative, and awe inspiring to create an amazing and wonderful video.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"])\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def process_audio_scenes(audio_file: str, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a single audio file through the entire workflow.\n",
    "    \"\"\"\n",
    "    # Create unique identifier based on audio file name\n",
    "    audio_basename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = f\"{audio_basename}_{timestamp}\"\n",
    "\n",
    "    # Create unique directories for images and videos\n",
    "    print(f\"Create unique directories for images and videos\")\n",
    "    audio_images_dir = os.path.join(config[\"base_working_dir\"], unique_id)\n",
    "    audio_videos_dir = os.path.join(config[\"base_video_dir\"], unique_id)\n",
    "    os.makedirs(audio_images_dir, exist_ok=True)\n",
    "    os.makedirs(audio_videos_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Transcribe audio using Whisper\n",
    "    print(f\"Transcribe audio using Whisper\")\n",
    "    model = whisper.load_model(\"turbo\")\n",
    "    result = model.transcribe(audio_file)\n",
    "\n",
    "    # Cleanup Whisper model memory\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    segments = result['segments']\n",
    "\n",
    "    # Extract list of start times and texts\n",
    "    segment_texts_and_start_times = [(segment['text'].strip(), segment['start']) for segment in segments]\n",
    "\n",
    "    # Combine texts\n",
    "    text = \"\"\n",
    "    for segment_text, start in segment_texts_and_start_times:\n",
    "        text += f\"Start: {start}, Text: {segment_text}\\n\"\n",
    "\n",
    "    last_end_value = segments[-1]['end']\n",
    "\n",
    "    # Step 2: Generate video summary using OpenAI\n",
    "    print(f\"Generate video summary using OpenAI\")\n",
    "    video_summary_prompt = f'Create a short summary that describes a music video based on these lyrics: {text}'\n",
    "    video_summary = get_openai_prompt_response(video_summary_prompt, config, openai_model=config[\"openai_model\"])\n",
    "\n",
    "    # Step 3: Create scenes based on lyrics\n",
    "    print(f\"Create scenes based on lyrics\")\n",
    "    scenes = create_scenes(text, video_summary, config)\n",
    "\n",
    "    # we don't want scenes longer than 18 seconds\n",
    "    new_scenes = []\n",
    "    for i in range(len(scenes)):\n",
    "        scene = scenes[i]\n",
    "        if i == 0:\n",
    "            start_time = 0\n",
    "        else:\n",
    "            start_time = scene['start']\n",
    "        # Determine the end time\n",
    "        if i < len(scenes) - 1:\n",
    "            end_time = scenes[i + 1]['start']\n",
    "        else:\n",
    "            # Assuming a default duration of 18 seconds for the last scene\n",
    "            end_time = start_time + 18\n",
    "        duration = end_time - start_time\n",
    "        # Split the scene if duration exceeds 18 seconds\n",
    "        while duration > 18:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "            start_time += 18\n",
    "            duration = end_time - start_time\n",
    "        # Append the remaining part of the scene\n",
    "        if duration > 0:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "    # Replace the original scenes with the new list\n",
    "    scenes = new_scenes\n",
    "    \n",
    "    return scenes, audio_images_dir, audio_videos_dir, last_end_value\n",
    "\n",
    "def process_audio_images(config: dict, scenes, audio_images_dir):\n",
    "    # Step 4: Load Flux pipeline and generate images\n",
    "    print(f\"Load Flux pipeline and generate images\")\n",
    "    with load_flux_pipe() as flux_pipe:\n",
    "        height = 480\n",
    "        width = 720\n",
    "        guidance_scale = 3.9\n",
    "        num_inference_steps = 32\n",
    "        max_sequence_length = 512\n",
    "        seed = -1\n",
    "\n",
    "        # Generate images for each scene\n",
    "        image_num = 1\n",
    "        for scene in scenes:\n",
    "            image_prompt = scene['scene_description']\n",
    "            image = gen_flux_image(flux_pipe, image_prompt, config, height, width, guidance_scale, num_inference_steps, max_sequence_length, seed)\n",
    "            filename = f\"image_{str(image_num).zfill(2)}.jpg\"\n",
    "            image_path = os.path.join(audio_images_dir, filename)\n",
    "            image.save(image_path, dpi=(300, 300))\n",
    "            image_num += 1\n",
    "\n",
    "    # At this point, flux_pipe is automatically deleted\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "def process_audio_video(config: dict, scenes, audio_images_dir, audio_videos_dir, last_end_value):\n",
    "    # Step 6: Load Video Pipeline\n",
    "    print(f\"Load Video Pipeline\")\n",
    "    with load_video_pipeline() as video_pipe:\n",
    "\n",
    "        # Temporary image path\n",
    "        temp_image = os.path.join(audio_images_dir, \"temp_image.jpg\")\n",
    "        video_num = 1\n",
    "    \n",
    "        # Step 7: Generate video sequences\n",
    "        for i, scene in enumerate(scenes):\n",
    "            prompt = scene[\"action_sequence\"]\n",
    "    \n",
    "            # Use the initial image for each scene\n",
    "            image_input = os.path.join(audio_images_dir, f\"image_{str(i+1).zfill(2)}.jpg\")\n",
    "    \n",
    "            # Calculate duration to keep the video in 6-second increments\n",
    "            if i + 1 < len(scenes):\n",
    "                next_start_time = scenes[i + 1][\"start\"]\n",
    "            else:\n",
    "                next_start_time = last_end_value  # Use the final ending time for the last scene\n",
    "    \n",
    "            if i == 0:\n",
    "                duration = next_start_time\n",
    "            else:\n",
    "                duration = next_start_time - scene[\"start\"]\n",
    "            num_video_segments = int((duration + 3) // 6)\n",
    "    \n",
    "            print(f\"Scene {i+1} has {num_video_segments} segments\")\n",
    "            for j in range(num_video_segments):\n",
    "                video_name = f\"video_{str(video_num).zfill(2)}_{str(j+1).zfill(2)}.mp4\"\n",
    "                video_output_path = os.path.join(audio_videos_dir, video_name)\n",
    "                generate_video(video_pipe, prompt, image_input, config, seed_value=-1, video_filename=video_output_path)\n",
    "                time.sleep(1)  # Pause for 1 second\n",
    "    \n",
    "                # After generating the video, extract the last frame to use as input for the next segment\n",
    "                extract_last_frame(video_output_path, temp_image)\n",
    "    \n",
    "                # Use the last frame as input for the next video segment in the same scene\n",
    "                image_input = temp_image\n",
    "    \n",
    "                video_num += 1  # Increment video number for the next segment\n",
    "        \n",
    "    # At this point, video_pipe is automatically deleted\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def process_all_audios(audio_file, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a list of audio files through the workflow.\n",
    "    \"\"\"\n",
    "    print(f\"Processing audio file: {audio_file}\")\n",
    "    scenes, audio_images_dir, audio_videos_dir, last_end_value = process_audio_scenes(audio_file, config)\n",
    "    print(f'{len(scenes)} scenes: {scenes}')\n",
    "    # Cleanup Video pipeline memory\n",
    "    if torch.cuda.is_available():\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    # Create starting images for scenes\n",
    "    process_audio_images(config, scenes, audio_images_dir)\n",
    "    # Cleanup Video pipeline memory\n",
    "    if torch.cuda.is_available():\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return config, scenes, audio_images_dir, audio_videos_dir, last_end_value\n",
    "        \n",
    "def synthesize_video(config, scenes, audio_images_dir, audio_videos_dir, last_end_value):\n",
    "    \n",
    "    # Create starting images for scenes\n",
    "    process_audio_video(config, scenes, audio_images_dir, audio_videos_dir, last_end_value)\n",
    "    # Cleanup Video pipeline memory\n",
    "    if torch.cuda.is_available():\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0cc96-e609-4cd2-9c67-dcd63562988a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run\n",
    "for audio_file in CONFIG['audio_files']:\n",
    "    config, scenes, audio_images_dir, audio_videos_dir, last_end_value = process_all_audios(audio_file, CONFIG)\n",
    "    cuda.select_device(0) # choosing second GPU \n",
    "    cuda.close()\n",
    "    synthesize_video(config, scenes, audio_images_dir, audio_videos_dir, last_end_value)\n",
    "    cuda.select_device(0) # choosing second GPU \n",
    "    cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bbb275-c2d2-4e31-a66e-c9cd9bc46941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0aedf1-9baa-4cd6-b109-a085ec8c15d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a348d48-8ccb-4c58-ad15-274bedd9f108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
