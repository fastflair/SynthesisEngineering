{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f87d72-5305-4778-ba0c-31549333c8db",
   "metadata": {},
   "source": [
    "# Music Video Synthesis\n",
    "* Extract lyrics from song with timestamps\n",
    "* Compose scenes, include timestamps\n",
    "* Construct video text prompt for each scene\n",
    "* Build videos for each scene\n",
    "* Stitch together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b074-e32d-45f2-977f-c923878625e6",
   "metadata": {},
   "source": [
    "# We will use openai whipser for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45c210-fd2b-4381-9fd3-c1eb18feefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet --upgrade pip\n",
    "#!pip install --quiet --upgrade openai-whisper\n",
    "# Ubuntu or Debian\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!pip install setuptools-rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e00f4c7-050c-4738-9ee7-8b7eeb38e5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 06:21:47.187749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-08 06:21:47.220964: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-08 06:21:47.234675: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-08 06:21:47.280973: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-08 06:21:48.612976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import gc \n",
    "import diffusers\n",
    "import imageio\n",
    "import imageio_ffmpeg\n",
    "import json\n",
    "import math\n",
    "import moviepy.editor as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import threading\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "import utils\n",
    "import whisper\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from diffusers import AutoencoderKL, DPMSolverMultistepScheduler, FlowMatchEulerDiscreteScheduler, KDPM2DiscreteScheduler, StableDiffusionXLPipeline\n",
    "from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from openai import OpenAI\n",
    "# pip install diffusers optimum-quanto\n",
    "from optimum.quanto import freeze, qfloat8, quantize, requantize\n",
    "from PIL import Image\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_flux1\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoImageProcessor, CLIPConfig, CLIPFeatureExtractor, CLIPModel, CLIPProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModel, Swin2SRForImageSuperResolution, T5EncoderModel, T5TokenizerFast\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "retry_limit = 3\n",
    "\n",
    "openai_api_key = \"\"\n",
    "openai_model = 'gpt-4o-mini'\n",
    "openai_model_large = 'gpt-4o'\n",
    "hg_token = ''\n",
    "working_dir = './images2'\n",
    "video_dir = \"./output2\"\n",
    "audio_file = '//mnt/d/audio/VampireLament.mp3'\n",
    "\n",
    "os.makedirs(working_dir, exist_ok=True)\n",
    "os.makedirs(video_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05111aed-2c0b-4dcc-8251-cd87bee5cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_prompt_response(prompt: str, max_tokens: int = 6000, temperature: float = 0.33, openai_model: str = ''):\n",
    " \n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens = max_tokens,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"Act as a helpful assistant, you are an expert editor.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"{prompt}\"\"\"}\n",
    "        ],\n",
    "        model=openai_model,\n",
    "        temperature = temperature\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < retry_limit:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            break  # If successful, break out of the retry loop\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == retry_limit:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{retry_limit})\")\n",
    "    \n",
    "    return message_content\n",
    "    \n",
    "def load_quanto_transformer(repo_path):\n",
    "    with open(hf_hub_download(repo_path, \"transformer/quantization_map.json\"), \"r\") as f:\n",
    "        quantization_map = json.load(f)\n",
    "    with torch.device(\"meta\"):\n",
    "        transformer = diffusers.FluxTransformer2DModel.from_config(hf_hub_download(repo_path, \"transformer/config.json\")).to(dtype)\n",
    "    state_dict = load_safetensors(hf_hub_download(repo_path, \"transformer/diffusion_pytorch_model.safetensors\"))\n",
    "    requantize(transformer, state_dict, quantization_map, device=torch.device(\"cuda\"))\n",
    "    return transformer\n",
    "\n",
    "\n",
    "def load_quanto_text_encoder_2(repo_path):\n",
    "    with open(hf_hub_download(repo_path, \"text_encoder_2/quantization_map.json\"), \"r\") as f:\n",
    "        quantization_map = json.load(f)\n",
    "    with open(hf_hub_download(repo_path, \"text_encoder_2/config.json\")) as f:\n",
    "        t5_config = transformers.T5Config(**json.load(f))\n",
    "    with torch.device(\"meta\"):\n",
    "        text_encoder_2 = transformers.T5EncoderModel(t5_config).to(dtype)\n",
    "    state_dict = load_safetensors(hf_hub_download(repo_path, \"text_encoder_2/model.safetensors\"))\n",
    "    requantize(text_encoder_2, state_dict, quantization_map, device=torch.device(\"cuda\"))\n",
    "    return text_encoder_2\n",
    "\n",
    "def load_quanto_text_encoder_2_longer(repo_path, max_length=512):\n",
    "    with open(hf_hub_download(repo_path, \"text_encoder_2/quantization_map.json\"), \"r\") as f:\n",
    "        quantization_map = json.load(f)\n",
    "    with open(hf_hub_download(repo_path, \"text_encoder_2/config.json\")) as f:\n",
    "        t5_config = transformers.T5Config(**json.load(f))\n",
    "    \n",
    "    # Update the config for longer sequence length\n",
    "    t5_config.max_position_embeddings = max_length\n",
    "    \n",
    "    with torch.device(\"meta\"):\n",
    "        text_encoder_2 = transformers.T5EncoderModel(t5_config).to(dtype)\n",
    "    \n",
    "    state_dict = load_safetensors(hf_hub_download(repo_path, \"text_encoder_2/model.safetensors\"))\n",
    "    requantize(text_encoder_2, state_dict, quantization_map, device=torch.device(\"cuda\"))\n",
    "    \n",
    "    return text_encoder_2\n",
    "    \n",
    "def load_flux_pipe():\n",
    "    # Load the main pipeline without the transformer or text_encoder_2 initially\n",
    "    pipe = None\n",
    "    clip_repo = \"zer0int/CLIP-GmP-ViT-L-14\"\n",
    "    text_encoder = CLIPTextModel.from_pretrained(clip_repo, torch_dtype=dtype)\n",
    "    \n",
    "    pipe = diffusers.AutoPipelineForText2Image.from_pretrained(\n",
    "        \"Disty0/FLUX.1-dev-qint8\", \n",
    "        text_encoder=text_encoder,\n",
    "        transformer=None, \n",
    "        text_encoder_2=None, \n",
    "        torch_dtype=dtype\n",
    "    )\n",
    "    \n",
    "    # Load custom transformer and text encoder with specific configurations\n",
    "    pipe.transformer = load_quanto_transformer(\"Disty0/FLUX.1-dev-qint8\")\n",
    "    pipe.text_encoder_2 = load_quanto_text_encoder_2_longer(\n",
    "        \"Disty0/FLUX.1-dev-qint8\", \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Move the pipeline to CUDA with bfloat16 precision for performance\n",
    "    pipe = pipe.to(\"cuda\", dtype=dtype)\n",
    "\n",
    "    # Enable memory optimizations (attention slicing)\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "    return pipe\n",
    "\n",
    "def gen_flux_image(pipe, prompt, height=1024, width=1024, guidance_scale=3.5, num_inference_steps=32, max_sequence_length=512, seed=-1):\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "        \n",
    "    prompt_embeds, pooled_prompt_embeds = get_weighted_text_embeddings_flux1(\n",
    "        pipe        = pipe,\n",
    "        prompt    = prompt\n",
    "    )\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt_embeds               = prompt_embeds,\n",
    "        pooled_prompt_embeds      = pooled_prompt_embeds,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        guidance_scale=guidance_scale,\n",
    "        output_type=\"pil\",\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        max_sequence_length=max_sequence_length,\n",
    "        generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "    ).images[0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2f7d73-db8c-4b82-b49d-4a94ec133b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/gensyn/anaconda3/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Music Meet that streaks that roam the streets Hunting for some youthful treats See the teens all gathered round My heartbeats slowing down Oh, their blood is full of fries Energy drinks And sugary pies I can't take a single bite Their poor health just isn't right Used to savor youthful veins Now it's all polluted stains Fast food feasts and lack of sleep Makes their life buds are too cheap Craving pure and wholesome flow But where did all the healthy go? A healthy go Oh, their blood is full of fries Video games and neon skies Fast food feasts and lack of food I can't take a single bite Their poor health just isn't right Graving pure and wholesome flow But where did all the healthy go? Wow Wow Wow Wow Oh, their blood is full of fries Oh, their blood is full of fries Video games and neon skies Oh, their blood is full of fries Video games and neon skies Fast food feasts and lack of sleep Makes their life blood far too cheap Small energy Cricks like Enth various Leuten Rooms and Perdice HelloCom implants Thans Toy metro Marketing Polini mentors PCs omegaindung Never see the light of day, vitamin D is running low Makes it hard for fangs to go Maybe I'll start a wellness blog, get these kids to ditch the smog Organic meals and yoga mats, save a vampire from these fats Oh, their blood is full of fries Processed snacks and apple pies I just want a healthy bite Is that such a crazy plight? So if you see me late at night Don't be filled with fear or fright Eat your veggies, get some rest Help a vampire be his best C puedan some breakfast Because of your blood You hit me up It will be chodzi You pay yourmedim tulos It will be Ray Arts etc That's crazy It will take a dot Especially for eating Us Off Is Or To You\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = whisper.load_model(\"turbo\")\n",
    "result = model.transcribe(audio_file)\n",
    "print(result[\"text\"])\n",
    "# done with model\n",
    "del model\n",
    "# If using GPU, clear the GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ebce332-5797-4045-9e14-c3b2ed7fd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = result['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f641d03-82e9-4f42-b430-729aa79fff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting list of start times and texts\n",
    "segment_texts_and_start_times = [(segment['text'].strip(), segment['start']) for segment in segments]\n",
    "\n",
    "# Assuming 'segment_texts_and_start_times' is a list of tuples (text, start)\n",
    "# Printing the results\n",
    "text = \"\"\n",
    "for segment_text, start in segment_texts_and_start_times:\n",
    "    text += f\"Start: {start}, Text: {segment_text}\\n\"\n",
    "    #print(f\"Start: {start}, Text: {segment_text}\")\n",
    "\n",
    "last_end_value = segments[-1]['end']\n",
    "#print(f\"The last end value is: {last_end_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6c2b36-884c-4fbc-88cd-f9eedb5190b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_summary_prompt = f'Create a short summary to describe an overall scene for a music video based on these lyrics: {text}'\n",
    "video_summary = get_openai_prompt_response(video_summary_prompt, openai_model=openai_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae53cbba-a4d6-4dc6-afe8-873b2ec17f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''Create a json list of scenes from the following text.  scenes should be groups of similar lyrics with new scenes when the context changes.  Text: {text}   The json list should have the start value for the first item in the scene and the text that is combined for all items in the same scene.  Return only the json list, less jargon. '''\n",
    "result = get_openai_prompt_response(prompt, openai_model=openai_model)\n",
    "result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06918fc5-6e8c-4def-a310-4fbe3578020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f2a0032-e308-425b-9b87-dd8e84539045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through scenes\n",
    "for scene in scenes:\n",
    "    # synthesize the scene composition\n",
    "    prompt = f'''Create an imaginative and vivid video scene descriptive caption based on the following overall music video description and scene lyrics. Include rich details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene.  An example of a scene description is: \"A radiant woman stands on a deserted beach, arms outstretched, wearing a beige trench coat, white blouse, light blue jeans, and chic boots, against a backdrop of soft sky and sea. Moments later, she is seen mid-twirl, arms exuberant, with the lighting suggesting dawn or dusk. Then, she runs along the beach, her attire complemented by an off-white scarf and black ankle boots, the tranquil sea behind her. Finally, she holds a paper airplane, her pose reflecting joy and freedom, with the ocean's gentle waves and the sky's soft pastel hues enhancing the serene ambiance.\"\n",
    "Another example is: \"A determined man in athletic attire, including a blue long-sleeve shirt, black shorts, and blue socks, jogs around a snow-covered soccer field, showcasing his solitary exercise in a quiet, overcast setting. His long dreadlocks, focused expression, and the serene winter backdrop highlight his dedication to fitness. As he moves, his attire, consisting of a blue sports sweatshirt, black athletic pants, gloves, and sneakers, grips the snowy ground. He is seen running past a chain-link fence enclosing the playground area, with a basketball hoop and children's slide, suggesting a moment of solitary exercise amidst the empty field.\"\n",
    "Music video description: {video_summary}\n",
    "Scene lyrics: {scene[\"text\"]}\n",
    "Return only the scene description, less jargon.'''\n",
    "    result = get_openai_prompt_response(prompt, openai_model=openai_model)\n",
    "    scene[\"scene_description\"] = result\n",
    "    # synthesize the action sequence\n",
    "    prompt = f'''This is the overall theme for a music video: {video_summary}\n",
    "This is a scene description for a scene of the music video, which we have a starting image for: {scene[\"scene_description\"]}\n",
    "We want to create a short animation prompt for the starting image, please create this short animation prompt.  Return only the prompt, less jargon.  For example, an image of a woman riding a bike might have the short animation prompt: \"A woman is riding a bicycle at high speed. Focused, detailed, realistic.\"  An image of a starry night painting might have the short animation prompt: \"Starry sky slowly rotating.\"'''\n",
    "    result = get_openai_prompt_response(prompt, openai_model=openai_model)\n",
    "    scene[\"action_sequence\"] = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2288e38d-e35b-4dbe-aa03-960f705b8313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 16.0,\n",
       "  'text': \"Meet that streaks that roam the streets Hunting for some youthful treats See the teens all gathered round My heartbeats slowing down Oh, their blood is full of fries Energy drinks And sugary pies I can't take a single bite Their poor health just isn't right Used to savor youthful veins Now it's all polluted stains Fast food feasts and lack of sleep Makes their life buds are too cheap Craving pure and wholesome flow But where did all the healthy go? A healthy go\",\n",
       "  'scene_description': \"In a vibrant urban landscape alive with pulsating beats, a vampire stands at the edge of a bustling street, cloaked in a sleek, dark trench coat that contrasts sharply with the neon lights flickering around them. Their pale skin glows under the electric hues of pink and blue, while sharp, expressive eyes scan the scene filled with groups of teens. The air is thick with laughter and the enticing aroma of fast food, as the young crowd indulges in greasy fries and sugary drinks, their carefree energy palpable.\\n\\nThe camera captures a close-up of the vampire's face, a mix of nostalgia and concern etched into their features. They watch as teens gather around a nearby arcade, their joy infectious yet tinged with the weight of unhealthy choices. The vampire's expression shifts from longing to determination, as they envision a world brimming with fresh fruits and vibrant salads, contrasting sharply with the junk food frenzy before them.\\n\\nAs the lyrics unfold, the scene transitions to a dreamlike sequence where the vampire imagines a sunlit park filled with laughter and vitality. Here, the teens are engaged in wholesome activities—cooking together in a colorful kitchen, practicing yoga on vibrant mats, and enjoying nature's beauty. The lighting is warm and inviting, casting a golden glow that symbolizes hope and renewal.\\n\\nThe vampire, now animated and passionate, gestures enthusiastically, encouraging the teens to embrace healthier choices. Their movements are fluid and expressive, radiating a newfound energy that captivates the young crowd. The scene culminates in a lively montage of laughter, fresh ingredients, and shared moments, leaving a lingering message of positivity and the promise of a brighter, healthier future.\",\n",
       "  'action_sequence': '\"A vampire stands at the edge of a bustling street, cloaked in a dark trench coat, observing teens indulging in fast food under neon lights. Their pale skin glows in electric hues, with a mix of nostalgia and concern on their face.\"'},\n",
       " {'start': 64.0,\n",
       "  'text': \"Oh, their blood is full of fries Video games and neon skies Fast food feasts and lack of food I can't take a single bite Their poor health just isn't right Graving pure and wholesome flow But where did all the healthy go? Wow Wow Wow Wow\",\n",
       "  'scene_description': 'In a vibrant urban landscape, pulsating with energy, the scene opens with a kaleidoscope of neon lights illuminating the night. Groups of teens, clad in colorful streetwear—baggy hoodies, graphic tees, and distressed jeans—gather around food trucks, their laughter ringing out like music against the backdrop of flashing arcade signs. The air is thick with the scent of greasy fries and sugary drinks, as they indulge in fast food feasts, their faces glowing with the thrill of the moment.\\n\\nAmidst the lively chaos, the protagonist—a pale, ethereal vampire—stands slightly apart, dressed in a sleek, dark ensemble that contrasts sharply with the bright surroundings. Their long, flowing coat billows gently in the night breeze, and their expressive eyes reflect a mix of nostalgia and concern as they observe the scene. The vibrant energy of the youth is palpable, yet the vampire’s somber expression hints at a deeper understanding of the consequences of their choices.\\n\\nAs the lyrics echo, the camera shifts to capture the teens’ carefree revelry, juxtaposed with fleeting glimpses of their declining vitality—yawning faces, sluggish movements, and the remnants of half-eaten meals. The lighting dims slightly, casting shadows that emphasize the weight of their unhealthy habits. The vampire, feeling the burden of their poor health, envisions a brighter world filled with fresh fruits, yoga mats, and laughter in the sun.\\n\\nSuddenly, the mood shifts as the vampire takes a decisive step forward, a spark of determination lighting their features. They pull out a sleek laptop, the screen glowing with the promise of a wellness blog, ready to inspire the teens to embrace healthier choices. The scene bursts into a colorful montage, showcasing the teens cooking together in a sunlit kitchen, vibrant vegetables dancing in the air, and laughter echoing through parks as they engage in playful exercise. The vampire, now a guiding figure, encourages them with a warm smile, urging them to savor the goodness of life.\\n\\nAs the scene draws to a close, the camera pulls back to reveal a hopeful horizon, the neon lights fading into a soft dawn. The message is clear: it’s never too late to make positive changes, and the future is bright with the promise of health and happiness.',\n",
       "  'action_sequence': '\"Vibrant urban landscape with neon lights flickering. Groups of teens laughing and enjoying fast food around food trucks. A pale vampire stands apart, observing with a mix of nostalgia and concern. Dynamic, colorful, lively.\"'},\n",
       " {'start': 92.0,\n",
       "  'text': 'Oh, their blood is full of fries Oh, their blood is full of fries Video games and neon skies Oh, their blood is full of fries Video games and neon skies Fast food feasts and lack of sleep Makes their life blood far too cheap Small energy',\n",
       "  'scene_description': 'In a vibrant urban setting, a lively street buzzes with energy as neon lights flicker overhead, casting colorful reflections on the pavement. Groups of teenagers, clad in oversized graphic tees, ripped jeans, and colorful sneakers, gather around a food truck, their laughter ringing out like music. The air is thick with the scent of fries and burgers, and the glow of video game arcades illuminates their animated faces, filled with excitement and carefree joy.\\n\\nAmidst this scene, the protagonist—a striking vampire with porcelain skin and deep crimson lips—stands slightly apart, dressed in a sleek black leather jacket over a fitted white shirt, paired with dark jeans and combat boots. Their piercing gaze, a mix of longing and concern, scans the youthful revelry, highlighting the stark contrast between their eternal youth and the fleeting vitality of the teens indulging in fast food and late-night gaming.\\n\\nAs the lyrics echo, \"Oh, their blood is full of fries,\" the camera captures the vampire’s wistful expression, a hint of sadness in their eyes as they envision the consequences of these choices. The vibrant colors around them fade momentarily, replaced by a muted palette that reflects the weight of unhealthy habits—fast food wrappers scattered like fallen leaves and tired faces illuminated by the glow of screens.\\n\\nSuddenly, the mood shifts as the vampire, inspired, begins to imagine a brighter future. The scene transforms into a lively montage of the teens cooking together in a sunlit kitchen, vibrant vegetables and fruits filling the counter, laughter spilling into the air. They are seen practicing yoga in a lush park, the sunlight filtering through the trees, casting playful shadows on their joyful faces.\\n\\nThe vampire, now animated and encouraging, gestures enthusiastically, urging the teens to embrace healthier choices. The final shot captures a radiant sunset, casting a warm glow over the group as they share a wholesome meal, their smiles bright and full of life. The scene closes with the vampire standing among them, a proud smile on their face, embodying the hopeful message that change is always possible, no matter the past.',\n",
       "  'action_sequence': '\"Vibrant urban street filled with teens laughing around a food truck, neon lights flickering overhead, a striking vampire observing with a mix of longing and concern.\"'},\n",
       " {'start': 116.0,\n",
       "  'text': 'Cricks like Enth various Leuten Rooms and Perdice Hello Com implants Thans Toy metro Marketing Polini mentors PCs omega indung Never see the light of day, vitamin D is running low Makes it hard for fangs to go',\n",
       "  'scene_description': \"In the heart of a vibrant urban landscape, the scene opens with a kaleidoscope of neon lights flickering against the night sky. A group of teens, clad in colorful streetwear—baggy graphic tees, distressed jeans, and bright sneakers—laugh and share oversized burgers and sugary drinks, their carefree energy palpable. The camera zooms in on their animated expressions, eyes sparkling with mischief as they huddle around a nearby video game arcade, the sounds of digital battles echoing in the background.\\n\\nAmidst this lively chaos stands the protagonist, a vampire dressed in a sleek black leather jacket over a fitted white shirt, paired with dark jeans and polished boots. Their pale skin glows under the neon lights, contrasting sharply with the vibrant surroundings. With a wistful gaze, they observe the scene, a hint of concern etched on their face as they take in the unhealthy habits of the youth around them. The flickering lights cast playful shadows, emphasizing their longing for a healthier lifestyle.\\n\\nAs the lyrics unfold, the mood shifts. The camera captures the vampire's expression of determination as they envision a brighter world—a dreamscape filled with lush greens, yoga mats, and colorful bowls of fresh fruits and vegetables. The lighting softens, bathing the scene in warm hues, symbolizing hope and transformation.\\n\\nSuddenly, the energy surges as the vampire takes action. The scene transitions to a lively montage: teens cooking together in a sunlit kitchen, laughter mingling with the sizzling of healthy dishes; others practicing yoga in a serene park, their movements fluid and graceful, surrounded by blooming flowers. The camera captures the joy on their faces as they embrace these new, wholesome activities, the vampire cheering them on with a proud smile.\\n\\nThe video culminates in a vibrant celebration of health and vitality, the teens dancing under a canopy of twinkling fairy lights, their spirits lifted. The scene closes with the vampire, arms wide open, inviting everyone to join in this journey towards a brighter, healthier future, the backdrop of the city now illuminated with a hopeful glow.\",\n",
       "  'action_sequence': '\"A vibrant urban scene with neon lights flickering, a group of teens laughing and sharing oversized burgers, while a vampire observes with a concerned expression.\"'},\n",
       " {'start': 150.0,\n",
       "  'text': \"Maybe I'll start a wellness blog, get these kids to ditch the smog Organic meals and yoga mats, save a vampire from these fats Oh, their blood is full of fries Processed snacks and apple pies I just want a healthy bite Is that such a crazy plight? So if you see me late at night Don't be filled with fear or fright Eat your veggies, get some rest Help a vampire be his best\",\n",
       "  'scene_description': \"In a vibrant urban landscape, the scene bursts to life under a kaleidoscope of neon lights that flicker and dance against the night sky. Groups of teens, clad in colorful streetwear—baggy hoodies, distressed jeans, and bright sneakers—gather around food trucks, their laughter ringing out like music amidst the bustling atmosphere. The camera captures them indulging in greasy burgers and oversized sodas, their carefree energy contrasting sharply with the protagonist, a vampire dressed in a sleek black leather jacket over a fitted white shirt, dark jeans, and polished boots. With pale skin and strikingly sharp features, the vampire leans against a graffiti-covered wall, eyes wide with a mix of nostalgia and concern as they observe the scene.\\n\\nAs the lyrics unfold, the lighting shifts to a more somber tone, casting shadows that hint at the consequences of the teens’ choices. The vampire’s expression turns contemplative, reflecting a longing for a healthier lifestyle. In a dreamy sequence, the scene transforms into a sunlit kitchen filled with fresh fruits and vegetables, yoga mats rolled out on a grassy park, and laughter echoing in a serene nature setting. The vampire envisions a world where wholesome meals replace junk food, their face lighting up with hope.\\n\\nThe energy shifts again as the vampire takes action, now animatedly typing on a laptop, surrounded by colorful posters promoting wellness. The teens, inspired by the vampire's passion, join in a lively montage of cooking together, exercising in the park, and enjoying the outdoors, their faces glowing with vitality. The scene closes with the vampire, arms outstretched in encouragement, a warm smile breaking through their usual stoic demeanor, as they remind the viewers to embrace healthy choices. The vibrant backdrop of the city fades into a hopeful twilight, symbolizing the promise of positive change and a brighter future.\",\n",
       "  'action_sequence': '\"Teens gather around food trucks in a vibrant urban landscape, laughing and enjoying oversized burgers and sodas, while a concerned vampire observes from a graffiti-covered wall.\"'},\n",
       " {'start': 192.0,\n",
       "  'text': \"C puedan some breakfast Because of your blood You hit me up It will be chodzi You pay yourmedim tulos It will be Ray Arts etc That's crazy It will take a dot Especially for eating Us Off Is Or To You\",\n",
       "  'scene_description': \"In a vibrant urban setting, the scene opens with a kaleidoscope of neon lights reflecting off the slick pavement, illuminating the faces of a group of teens gathered around a bustling food truck. They are dressed in a mix of colorful streetwear—baggy graphic tees, distressed jeans, and bright sneakers—each outfit a testament to their carefree youth. The air is filled with the enticing aroma of greasy fast food, and their laughter rings out, harmonizing with the pulsating beats of the music.\\n\\nAs the camera zooms in, we catch a glimpse of the protagonist, a stylish vampire clad in a sleek black leather jacket over a fitted white shirt, dark jeans, and polished boots. Their pale skin glows under the neon lights, and their expressive eyes reveal a blend of nostalgia and concern as they observe the scene. The vampire leans against a lamppost, arms crossed, a slight frown forming as they watch the teens indulge in sugary drinks and greasy snacks.\\n\\nThe mood shifts as the scene transitions to a more somber tone, with dimmer lighting casting shadows on the vampire's face. They envision a world filled with wholesome foods and vibrant greens—yoga mats spread out in a sunlit park, fresh fruits and vegetables displayed at a colorful farmer’s market. The vampire’s expression transforms from worry to determination as they resolve to inspire change.\\n\\nSuddenly, the energy bursts back to life, and the scene brightens with upbeat music. The teens are now seen cooking together in a bright kitchen, laughter filling the air as they chop vegetables and mix colorful ingredients. The vampire, now animated and enthusiastic, gestures excitedly, encouraging them to embrace healthier choices. The camera captures moments of joy—teens stretching on yoga mats, running through a sun-drenched park, and sharing vibrant meals, their faces glowing with vitality.\\n\\nAs the scene draws to a close, the lighting softens to a warm golden hue, symbolizing hope and renewal. The vampire stands at the forefront, a proud smile on their face, as the teens cheerfully munch on fresh salads and smoothies, embodying the message that it’s never too late to make positive changes for a brighter, healthier future.\",\n",
       "  'action_sequence': '\"Teens gather around a food truck, laughing and enjoying fast food under vibrant neon lights, with a stylish vampire observing them, arms crossed, a mix of nostalgia and concern on their face.\"'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0ea4e2e-ee17-4b68-a840-77c6b809ed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892ca7f75f29439daaef343ce8b7ceaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "WARNING:py.warnings:/home/gensyn/anaconda3/lib/python3.11/site-packages/diffusers/configuration_utils.py:245: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'diffusers.models.transformers.transformer_flux.FluxTransformer2DModel'>.load_config(...) followed by <class 'diffusers.models.transformers.transformer_flux.FluxTransformer2DModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = load_flux_pipe()\n",
    "height = 480\n",
    "width = 720\n",
    "guidance_scale=3.9\n",
    "num_inference_steps=24\n",
    "max_sequence_length=512  \n",
    "seed=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2662a8a-fb9c-4dd5-8a39-85985ec60b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 77). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (960 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69812a8a1204600b4b42ae852fa2424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create starting images for video\n",
    "image_num = 1\n",
    "for scene in scenes:\n",
    "    image_prompt = scene['scene_description']\n",
    "    image = gen_flux_image(pipe, prompt, height, width, guidance_scale, num_inference_steps, max_sequence_length, seed)\n",
    "    filename = \"image_\" + str(image_num).zfill(2) + \".jpg\"\n",
    "    image_name = working_dir + \"/\" + filename\n",
    "    image.save(image_name,  dpi=(300, 300))\n",
    "    image_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ff5bb-8ebf-4c0a-9b16-4a403d643bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del diffusers\n",
    "del pipe\n",
    "del transformers\n",
    "# If using GPU, clear the GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b346d0a-c61f-4e13-811e-b20029e3e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "import transformers\n",
    "\n",
    "from diffusers import AutoencoderKLCogVideoX, CogVideoXTransformer3DModel, CogVideoXPipeline, CogVideoXDPMScheduler\n",
    "from diffusers import CogVideoXVideoToVideoPipeline, CogVideoXImageToVideoPipeline\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.utils import export_to_video, load_video, load_image\n",
    "from transformers import T5EncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb29e98-dc92-4ce4-a493-79a34dff9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def convert_to_gif(video_path):\n",
    "    clip = mp.VideoFileClip(video_path)\n",
    "    clip = clip.set_fps(8)\n",
    "    clip = clip.resize(height=240)\n",
    "    gif_path = video_path.replace(\".mp4\", \".gif\")\n",
    "    clip.write_gif(gif_path, fps=8)\n",
    "    return gif_path\n",
    "\n",
    "def resize_if_unfit(input_video):\n",
    "    width, height = get_video_dimensions(input_video)\n",
    "\n",
    "    if width == 720 and height == 480:\n",
    "        processed_video = input_video\n",
    "    else:\n",
    "        processed_video = center_crop_resize(input_video)\n",
    "    return processed_video\n",
    "\n",
    "\n",
    "def get_video_dimensions(input_video_path):\n",
    "    reader = imageio_ffmpeg.read_frames(input_video_path)\n",
    "    metadata = next(reader)\n",
    "    return metadata[\"size\"]\n",
    "\n",
    "\n",
    "def center_crop_resize(input_video_path, target_width=720, target_height=480):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    width_factor = target_width / orig_width\n",
    "    height_factor = target_height / orig_height\n",
    "    resize_factor = max(width_factor, height_factor)\n",
    "\n",
    "    inter_width = int(orig_width * resize_factor)\n",
    "    inter_height = int(orig_height * resize_factor)\n",
    "\n",
    "    target_fps = 8\n",
    "    ideal_skip = max(0, math.ceil(orig_fps / target_fps) - 1)\n",
    "    skip = min(5, ideal_skip)  # Cap at 5\n",
    "\n",
    "    while (total_frames / (skip + 1)) < 49 and skip > 0:\n",
    "        skip -= 1\n",
    "\n",
    "    processed_frames = []\n",
    "    frame_count = 0\n",
    "    total_read = 0\n",
    "\n",
    "    while frame_count < 49 and total_read < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if total_read % (skip + 1) == 0:\n",
    "            resized = cv2.resize(frame, (inter_width, inter_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            start_x = (inter_width - target_width) // 2\n",
    "            start_y = (inter_height - target_height) // 2\n",
    "            cropped = resized[start_y : start_y + target_height, start_x : start_x + target_width]\n",
    "\n",
    "            processed_frames.append(cropped)\n",
    "            frame_count += 1\n",
    "\n",
    "        total_read += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(temp_video_path, fourcc, target_fps, (target_width, target_height))\n",
    "\n",
    "        for frame in processed_frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    return temp_video_path\n",
    "\n",
    "def extract_last_frame(video_filename, output_image_filename):\n",
    "    \"\"\"\n",
    "    Extracts the last frame from a video file and saves it as an image.\n",
    "\n",
    "    Parameters:\n",
    "    - video_filename (str): Path to the input video file.\n",
    "    - output_image_filename (str): Path where the last frame image will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a reader object for the video\n",
    "        reader = imageio.get_reader(video_filename, 'ffmpeg')\n",
    "        \n",
    "        # Initialize last_frame to None\n",
    "        last_frame = None\n",
    "        \n",
    "        # Iterate through all frames to get the last one\n",
    "        for frame in reader:\n",
    "            last_frame = frame\n",
    "        \n",
    "        # Close the reader to free resources\n",
    "        reader.close()\n",
    "        \n",
    "        if last_frame is not None:\n",
    "            # Save the last frame as an image\n",
    "            imageio.imwrite(output_image_filename, last_frame)\n",
    "            print(f\"Last frame saved successfully as '{output_image_filename}'.\")\n",
    "        else:\n",
    "            print(\"The video contains no frames.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{video_filename}' was not found.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except RuntimeError as re:\n",
    "        print(f\"RuntimeError: {re}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25869c-23e6-4f85-a87d-8c5c5a08d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization = int8_weight_only\n",
    "\n",
    "text_encoder = T5EncoderModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\n",
    "quantize_(text_encoder, quantization())\n",
    "\n",
    "transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n",
    "quantize_(transformer, quantization())\n",
    "\n",
    "i2v_transformer = CogVideoXTransformer3DModel.from_pretrained(\n",
    "    \"THUDM/CogVideoX-5b-I2V\", subfolder=\"transformer\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"vae\", torch_dtype=torch.bfloat16)\n",
    "quantize_(vae, quantization())\n",
    "\n",
    "# Create pipeline and run inference\n",
    "pipe = CogVideoXPipeline.from_pretrained(\n",
    "    \"THUDM/CogVideoX-5b\",\n",
    "    text_encoder=text_encoder,\n",
    "    transformer=transformer,\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "#pipe.enable_model_cpu_offload()\n",
    "pipe.vae.enable_tiling()\n",
    "\n",
    "pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "i2v_vae=pipe.vae\n",
    "i2v_scheduler=pipe.scheduler\n",
    "i2v_tokenizer=pipe.tokenizer\n",
    "i2v_text_encoder=pipe.text_encoder\n",
    "\n",
    "del pipe\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d8678-d947-4fa6-9cbc-872982a7a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline once before the loop\n",
    "pipe_image = CogVideoXImageToVideoPipeline.from_pretrained(\n",
    "    \"THUDM/CogVideoX-5b-I2V\",\n",
    "    transformer=i2v_transformer,\n",
    "    vae=i2v_vae,\n",
    "    scheduler=i2v_scheduler,\n",
    "    tokenizer=i2v_tokenizer,\n",
    "    text_encoder=i2v_text_encoder,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(device)\n",
    "\n",
    "def infer(\n",
    "        pipe_image,\n",
    "        prompt: str,\n",
    "        image_input: str,\n",
    "        num_inference_steps: int,\n",
    "        guidance_scale: float,\n",
    "        seed: int = -1,\n",
    "        num_frames: int = 49,\n",
    "    ):\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 2**8 - 1)\n",
    "\n",
    "    image_input = Image.open(image_input).resize(size=(720, 480))  # Convert to PIL\n",
    "    image = load_image(image_input)\n",
    "\n",
    "    video_pt = pipe_image(\n",
    "        image=image,\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        num_videos_per_prompt=1,\n",
    "        use_dynamic_cfg=True,\n",
    "        output_type=\"pt\",\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=torch.Generator(device=\"cpu\").manual_seed(seed),\n",
    "        num_frames=num_frames,\n",
    "    ).frames\n",
    "    return (video_pt, seed)\n",
    "\n",
    "def generate(\n",
    "    pipe_image,\n",
    "    prompt,\n",
    "    image_input,\n",
    "    seed_value: int = -1,\n",
    "    video_filename: str = \"\",\n",
    "    num_frames: int = 49,\n",
    "):\n",
    "    latents, seed = infer(\n",
    "        pipe_image,\n",
    "        prompt,\n",
    "        image_input,\n",
    "        num_inference_steps=50, \n",
    "        guidance_scale=7.0,\n",
    "        seed=seed_value,\n",
    "        num_frames=num_frames,\n",
    "    )\n",
    "    batch_size = latents.shape[0]\n",
    "    batch_video_frames = []\n",
    "    for batch_idx in range(batch_size):\n",
    "        pt_image = latents[batch_idx]\n",
    "        pt_image = torch.stack([pt_image[i] for i in range(pt_image.shape[0])])\n",
    "        image_np = VaeImageProcessor.pt_to_numpy(pt_image)\n",
    "        image_pil = VaeImageProcessor.numpy_to_pil(image_np)\n",
    "        batch_video_frames.append(image_pil)\n",
    "    video_path = utils.save_video(\n",
    "        batch_video_frames[0],\n",
    "        fps=math.ceil((len(batch_video_frames[0]) - 1) / 6),\n",
    "        filename=video_filename\n",
    "    )\n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d1ff6-5127-4f4b-9a3c-327554fbf102",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_num = 1\n",
    "temp_image = f'{working_dir}/temp_image.jpg'\n",
    "\n",
    "for i, scene in enumerate(scenes):\n",
    "    prompt = scene[\"action_sequence\"]\n",
    "    \n",
    "    # Use the initial image for each scene\n",
    "    image_input = f\"{working_dir}/image_{str(i+1).zfill(2)}.jpg\"  # First time use an initial image\n",
    "    \n",
    "    # Calculate duration to keep the video in 6-second increments\n",
    "    if i + 1 < len(scenes):\n",
    "        next_start_time = scenes[i + 1][\"start\"]\n",
    "    else:\n",
    "        next_start_time = last_end_value  # Use the final ending time for the last scene\n",
    "    \n",
    "    duration = next_start_time - scene[\"start\"]\n",
    "    num_video_segments = int((duration + 3) // 6)\n",
    "\n",
    "    print(f'scene {i} has {num_video_segments} segments')\n",
    "    for j in range(num_video_segments):\n",
    "        video_name = f\"video_{str(video_num).zfill(2)}_{str(j).zfill(2)}\"\n",
    "        video_output = generate(pipe_image, prompt, image_input, -1, video_dir+\"/\"+video_name)\n",
    "        time.sleep(1)  # Pause for 1 second\n",
    "        # After generating the video, extract the last frame to use as input for the next segment\n",
    "        extract_last_frame(f'{video_dir}/{video_name}.mp4', temp_image)\n",
    "        \n",
    "        # Use the last frame in as input for the next video segment in the same scene\n",
    "        image_input = temp_image\n",
    " \n",
    "        video_num += 1  # Increment video number for the next segment\n",
    "\n",
    "# Clean up the pipeline after use\n",
    "del diffusers\n",
    "del pipe_image\n",
    "del transformers\n",
    "# If using GPU, clear the GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436e58f-0a61-4f74-a2a4-de0ad75e64db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61523b-9f2b-4beb-8a64-8083575a0689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
