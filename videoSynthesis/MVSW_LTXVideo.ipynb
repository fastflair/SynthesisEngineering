{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f87d72-5305-4778-ba0c-31549333c8db",
   "metadata": {},
   "source": [
    "# Music Video Synthesis\n",
    "* Extract lyrics from song with timestamps\n",
    "* Compose scenes, include timestamps\n",
    "* Construct video text prompt for each scene\n",
    "* Build videos for each scene\n",
    "* Stitch together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b074-e32d-45f2-977f-c923878625e6",
   "metadata": {},
   "source": [
    "# We will use openai whipser for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45c210-fd2b-4381-9fd3-c1eb18feefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet --upgrade pip\n",
    "#!pip3 install torch torchvision torchaudio torchao xformers --index-url https://download.pytorch.org/whl/cu128 -U\n",
    "#!pip install --quiet --upgrade openai-whisper openai optimum-quanto \n",
    "# Ubuntu or Debian\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!pip install setuptools-rust -U\n",
    "#!pip install -U diffusers imageio imageio_ffmpeg opencv-python moviepy transformers huggingface-hub optimum pillow safetensors ftfy -U\n",
    "#!pip install -U sentencepiece einops beautifulsoup4 -U\n",
    "#!pip install git+https://github.com/xhinker/sd_embed.git@main -U\n",
    "#!pip install accelerate flash_attention numba -U\n",
    "#!pip install flash_attn --no-build-isolation -U\n",
    "#!pip install -r requirements.txt -U\n",
    "#!pip install numpy==1.26.4\n",
    "#!pip install peft==0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537e766-eab2-4757-b6f9-fbac4da44930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import diffusers\n",
    "import gc\n",
    "import imageio\n",
    "import imageio_ffmpeg\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import whisper\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from diffusers import AutoencoderKL, FlowMatchEulerDiscreteScheduler, LTXImageToVideoPipeline, LTXPipeline\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from diffusers.utils import export_to_video, load_video, load_image\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from model4 import T5EncoderModel as m_T5EncoderModel, FluxTransformer2DModel\n",
    "from numba import cuda\n",
    "from openai import OpenAI\n",
    "from optimum.quanto import freeze, qfloat8, quantize, requantize\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pipeline_stg_ltx_image2video import LTXImageToVideoSTGPipeline\n",
    "from safetensors.torch import load_file as load_safetensors, save_file as save_safetensors\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_flux1\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5TokenizerFast, T5EncoderModel\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Define the paths where quantized weights will be saved\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "retry_limit = 3\n",
    "quantization = int8_weight_only\n",
    "\n",
    "WIDTH=768\n",
    "HEIGHT=512\n",
    "FRAME_RATE = 25\n",
    "NUM_FRAMES = 151\n",
    "NUM_INFERENCE_STEPS=70\n",
    "GUIDANCE_SCALE=4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54ba32-2208-45c8-8ed2-5fcb1e79aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "THEME = \"music video animation, erotic, beautiful, sensual, creative, imaginative, people angry about loss of freedom due to government control.\"\n",
    "CONFIG = {\n",
    "    \"openai_api_key\": \"\",\n",
    "    \"openai_model\": \"gpt-4o-mini\",\n",
    "    \"openai_model_small_reasoning\": \"o1-mini\",\n",
    "    \"openai_model_large\": \"gpt-4o\",\n",
    "    \"hf_token\": \"\",\n",
    "    \"base_working_dir\": \"./images\",\n",
    "    \"base_video_dir\": \"./output\",\n",
    "    \"audio_files\": [\n",
    "        \"/mnt/d/audio/Pawns No More.flac\",\n",
    "        \"/mnt/d/audio/Pawns No More.flac\",\n",
    "    ],\n",
    "    \"device\": device,\n",
    "    \"dtype\": dtype,\n",
    "    \"retry_limit\": retry_limit,\n",
    "    \"MAX_SEED\": MAX_SEED,\n",
    "}\n",
    "\n",
    "# Ensure base directories exist\n",
    "os.makedirs(CONFIG[\"base_working_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"base_video_dir\"], exist_ok=True)\n",
    "\n",
    "ACTION_SEQUENCES = f'''The camera pans over a snow-covered mountain range, revealing a vast expanse of snow-capped peaks and valleys.The mountains are covered in a thick layer of snow, with some areas appearing almost white while others have a slightly darker, almost grayish hue. The peaks are jagged and irregular, with some rising sharply into the sky while others are more rounded. The valleys are deep and narrow, with steep slopes that are also covered in snow. The trees in the foreground are mostly bare, with only a few leaves remaining on their branches. The sky is overcast, with thick clouds obscuring the sun. The overall impression is one of peace and tranquility, with the snow-covered mountains standing as a testament to the power and beauty of nature.,\n",
    "A woman with blood on her face and a white tank top looks down and to her right, then back up as she speaks. She has dark hair pulled back, light skin, and her face and chest are covered in blood. The camera angle is a close-up, focused on the woman's face and upper torso. The lighting is dim and blue-toned, creating a somber and intense atmosphere. The scene appears to be from a movie or TV show.\n",
    "The waves crash against the jagged rocks of the shoreline, sending spray high into the air.The rocks are a dark gray color, with sharp edges and deep crevices. The water is a clear blue-green, with white foam where the waves break against the rocks. The sky is a light gray, with a few white clouds dotting the horizon.\n",
    "A woman with short brown hair, wearing a maroon sleeveless top and a silver necklace, walks through a room while talking, then a woman with pink hair and a white shirt appears in the doorway and yells. The first woman walks from left to right, her expression serious; she has light skin and her eyebrows are slightly furrowed. The second woman stands in the doorway, her mouth open in a yell; she has light skin and her eyes are wide. The room is dimly lit, with a bookshelf visible in the background. The camera follows the first woman as she walks, then cuts to a close-up of the second woman's face. The scene is captured in real-life footage.\n",
    "A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.\n",
    "A prison guard unlocks and opens a cell door to reveal a young man sitting at a table with a woman. The guard, wearing a dark blue uniform with a badge on his left chest, unlocks the cell door with a key held in his right hand and pulls it open; he has short brown hair, light skin, and a neutral expression. The young man, wearing a black and white striped shirt, sits at a table covered with a white tablecloth, facing the woman; he has short brown hair, light skin, and a neutral expression. The woman, wearing a dark blue shirt, sits opposite the young man, her face turned towards him; she has short blonde hair and light skin. The camera remains stationary, capturing the scene from a medium distance, positioned slightly to the right of the guard. The room is dimly lit, with a single light fixture illuminating the table and the two figures. The walls are made of large, grey concrete blocks, and a metal door is visible in the background. The scene is captured in real-life footage.\n",
    "A woman walks away from a white Jeep parked on a city street at night, then ascends a staircase and knocks on a door. The woman, wearing a dark jacket and jeans, walks away from the Jeep parked on the left side of the street, her back to the camera; she walks at a steady pace, her arms swinging slightly by her sides; the street is dimly lit, with streetlights casting pools of light on the wet pavement; a man in a dark jacket and jeans walks past the Jeep in the opposite direction; the camera follows the woman from behind as she walks up a set of stairs towards a building with a green door; she reaches the top of the stairs and turns left, continuing to walk towards the building; she reaches the door and knocks on it with her right hand; the camera remains stationary, focused on the doorway; the scene is captured in real-life footage.\n",
    "'''\n",
    "\n",
    "SCENE_DESCRIPTIONS = '''scene_description: boris vallejo style, frank frazetta style, 8k high quality digital painting, masterpiece, very detailed, ultra realistic, (best quality) very detailed epic masterpiece, detailed face, full body, wrinkly wizard toad reading an ancient scroll in a swamp, best quality, epic scene, Dungeons and dragons atmosphere, heroic fantasy, realistic, realism, full body\n",
    "scene_description: masterpiece, best quality, amazing quality ,solo, holding, closed_mouth, sitting, outdoors, sky, day, cloud, water, blurry, blue_sky, tree, orange_eyes, no_humans, blurry_background, fish, reflection, mountain, animal_focus, lake, fishing_rod, reflective_water, fishing, holding_fishing_rod, fishing_line,A digital illustration shoot from the side about a cute cartoon fish character sitting on a wooden pier by a calm lake, holding a fishing rod. the image also shows a serene mountain landscape with tall trees and a clear blue sky. on the middle of the image, a no human, furry, blue and orange fish with large, expressive eyes and a happy expression is sitting on the wooden pier. the fish appears to be a chubby, cartoonish creature with a slim body and a closed mouth. it is facing the viewer with its eyes looking to the side. the creature is holding the fishing rod in its right hand and its left hand is resting on the edge of the water. the background features a mountain range with a few clouds in the sky, and the water is calm and still. the lighting is soft and natural, creating a peaceful and serene atmosphere. solo, looking at viewer, closed mouth, sitting, brown eyes, outdoors, sky, day, cloud, holding, tree, blue sky, water, tree branch, holding stick, mountain, fish, pond, holding fishing rod\n",
    "scene_description: masterpiece, best quality, good quality, very awa, newest, highres, absurdres, 1girl, solo, dress, standing, flower, outdoors, water, white flower, pink flower, scenery, reflection, rain, dark, ripples, yellow flower, puddle, colorful, abstract, standing on liquid, very Wide Shot, limited palette,\n",
    "scene_description: masterpiece, best quality, amazing quality, klskx, nsfw, explicit, 1girl, redhead, open fridge, dim blue fridge light, nude, oversized t-shirt slipping off shoulder, panties, barefoot, messy hair, licking fingers, one hand on hip, standing by fridge, looking back, midnight snack indoors, kitchen, wooden floor, open fridge door, scattered snacks, napkin, night, dark shadows, high contrast, volumetric lighting, intricate details, blurry background, depth of field\n",
    "scene_description: time travel, holding coffee,, hdr, 8k, absurdres, shiny, outdoors, reflection, blurry, blurry background, tokyo lights,tokyo street, neon lights, cyberpunk, high-contrast lighting, intricate details, vibrant colors, reflective surfaces, futuristic urban environment, glowing neon signs, cybernetic enhancements, punk aesthetic, dynamic pose, dynamic composition, depth of field, dark_theme, detailed backgroud, foreshortening, blurry edges, vignetting\n",
    "scene_description: masterpiece, best quality, amazing quality ,solo, yellow_eyes, flower, outdoors, sky, cloud, tree, no_humans, night, animal, facial_mark, moon, cat, star_\\(sky\\), night_sky, full_moon, starry_sky, animal_focus, architecture, black_cat, east_asian_architecture, whiskers, black_fur, huge_moon,A digital illustration shoot from a front camera angle about a cute black cat sitting on a moss-covered branch under a large full moon, surrounded by a fantasy setting with tall pagodas and flowers. on the middle of the image, a 1girl, who appears to be a cat, is sitting, looking at the viewer with large, expressive brown eyes. the cat has black fur with yellow stripes, and its ears are perked up, giving it a curious expression. it is positioned on the branch, with its body facing the viewer, giving a clear view of its full body. in the background, a full moon is visible, with stars twinkling in the night sky, and fluffy white clouds are visible, adding to the magical atmosphere of the scene. the overall style is whimsical and fantastical, with a focus on the cat's curious expression and the intricate details of its markings on its body. solo, smile, brown eyes, flower, outdoors, sky, day, cloud, no humans, animal, night, animal focus, star \\(sky\\), moon, cat, full moon\n",
    "scene_description: hyper realistic, a majestic A gemstone stag slowly blooming into life, moss and flowers sprouting from cracks in its crystalline body as it awakens, its eyes, initially dull stones, begin to glow with an inner emerald light, dawn light filters through a forest, illuminating the stags nascent awakening, wide shot capturing the stags full form and the blooming flora with golden antlers standing in a sunlit clearing, surrounded by ethereal forest spirits, glowing flora, magical atmosphere, extremely high-resolution details, photographic, realism pushed to extreme, fine texture, incredibly lifelike, Cinematic, beautiful, vibrant, masterpiece, 32k, ultra HD, ultra-detailed, amazing quality, amazing artist, sharp edges, detailed textures, full view, atmospheric lighting, amazing visuals\n",
    "scene_description: impressive and grotesque scenery on a distant world, famous artwork inspired by jordan grimmer, dramatic scene, fractal art, 1990s fantasy style, dynamic angle, this image shows the enormous transcendent hydra-like beast known as the powerful jadesnap as it wriggleflomps from the izzled depths of a churning, otherworldly glowing sea under a dramatic tempestuous sky, it is surrounded by the typical jagged fractal rock formations on the crinkled shore of the water-rich planet \"zoffeldirly quartus,\" superstitious life forms call it \"the bringer of lost keys\", creature focus, very aesthetic, extremely detailed, ultra high resolution, 8k, 4k, harmonising colors, light beige and chartreuse and bordeaux red and indigo blue and byzantium purple and ebony black, ovg, in the style of ck-ovf, amanoer, arsmjstyle, dnddarkestfantasy, aidmafluxpro1.1\n",
    "scene_description: A realistic toilet, completely engulfed in flames, inferno, blazing, concept art, masterpiece, perfect lighting, purple and pink flames, realistic flames, 8k, absurdres, massive fire - rendered in the highest quality, realistic bathroom background, A3ther\n",
    "scene_description: intricate linework with expressive contrasts, soft lighting with dynamic highlights, young woman wearing flight goggles, aviator leather jacket, long loose platinum hair, standing next to a 1930s biplane on an airstrip surrounded by tropical jungle, sunset, in orange hues\n",
    "scene_description: 1 girl , ghost girl , grave stone , hugging , kneeling , tear , raining , masterpiece, best quality, good quality, very awa, newest, highres, absurdres\n",
    "scene_description: Indi_and_Digo,1girl,solo,furry,tail,source_furry,,red hair,purple fur, purple eyes, child, kid, masterpiece, best quality, female hand holding a small umbrella, miniature wet tiny mouse standing on the path, rain, drops, funny, intricate details, hyper-realistic, hyper-detailed, professional photoshoot, colorful, ultra-sharp, vivid color, chiaroscuro lighting, macro\n",
    "scene_description: masterpiece, best quality, good quality, very aesthetic, absurdres, newest, 8K, depth of field, in the style of cknc, artist:moriimee, in the style of cksc, 1girl, short golden hair, bob cut, blue eyes, large breasts, bouncy, baggy red tank top, sagging, oversized leggings, sweat-soaked, soft moan, looking up, from below, hiking on ridge, pressing breasts through tank, nipples outlined BREAK mountain ridge, rocky path, whistling wind, pine scent BREAK warm light, high contrast, earthy tones, depth of field, golden hour lighting, rich details, rugged allure\n",
    "scene_description: masterpiece, best quality, good quality, very aesthetic, absurdres, newest, 8K, depth of field, in the style of cknc, artist:moriimee, in the style of cksc, 1girl, short green hair, tangled, hazel eyes, medium breasts, firm, hooded cloak, glossy skin, picking lock, smirking, looking at viewer, from side BREAK twilight forest, mossy ruins, owls hooting BREAK dynamic light, green tones, high contrast, twilight glow, rich details, sneaky atmosphere\n",
    "scene_description: A digital art splash in the style of bo-cyborgsplash, a mysterious raven character positioned in the center of the frame, directly facing the viewer, the raven's upper body is close to the camera, showcasing its dark, ornate attire adorned with intricate details and glowing purple gemstones, its long beak is visible, and its piercing pink eyes seem to lock onto the viewer with a sense of intensity, the background is a dark, neon-lit space with glowing elements, creating a mystical atmosphere, the character is adorned with numerous gemstone necklaces, adding a touch of opulence to the overall design, the overall effect is one of intrigue and mystery, a fantastic abstract colorful art splash, high quality, ultra detailed\n",
    "scene_description: masterpiece, best quality, good quality, very aesthetic, absurdres, newest, 8K, depth of field, in the style of cknc, artist:moriimee, in the style of cksc, 1girl, long grey hair, wild, stormy eyes, large breasts, round, thunderbolt crown, glossy skin, summoning lightning, looking at viewer, from below BREAK stormy cliff, dark clouds, waves crashing BREAK dynamic light, blue tones, high contrast, electric glow, rich details, dramatic atmosphere\n",
    "scene_description: masterpiece, hyper detailed, high quality v3, (ultra-HD details), 16k, midjourneyv6.1, (Pencil_Sketch:1.2, messy lines, greyscale, traditional media, sketch), anime, manga, sketch, unfinished, hatching (texture), fullbody portrait, long legs, mustard XXDFace head, creepy smile, x-eyes, robot joints, wearing ((creepy beige victorian tattered, broken, cracked, dirt, ripped dress)) at night, in 22@SIT_BCN wide street, creepy red moonlight, perfect anatomy\n",
    "scene_description: masterpiece, best quality, good quality, very awa, newest, highres, absurdres, 1girl, solo, short hair, blonde hair, red eyes, holding, standing, belt, hood, water, scarf, arm up, torn clothes, plant, hood up, wading, partially submerged, mittens, pillar, torch, holding torch, limited palette\n",
    "scene_description: anthropomorphic corgi knight, corgi head, on one knee, planted sword, holding sword, plate armor, scowl, v-shaped eyebrows ,cloudy, godrays, sunshine, riverbank background, holy halo ,wide shot, depth of field, realism, no humans, animal focus, corpses,covered in blood, battlefield\n",
    "scene_description: 1girl,solo,furry,pink fur,tail,ears,source_furry, child, kid, masterpiece, best quality, long hair, twin braids, farmer outfit,(steampunk), goggles, googles on head, bag, wheat field, outdoors, wind, accordion, holding instrument, playing instrument, gloves, wheat, sunset, farm, house, scenery, landscape, blurry, blurry background, looking at viewer, smile\n",
    "scene_description: masterpiece, best quality, amazing quality, solo, sitting, no humans, glowing, wariza, robot, science fiction, on floor, electricity, cable, joints, robot joints, damaged, mechanical parts, wire, humanoid robot, screw, bolt, Countless lightning, electric shock, open mouth, Open five fingers,hands up, Bent back, Low Angle\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34281c1-e231-4472-9d9b-096fa23b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop_and_resize(frame, target_height, target_width):\n",
    "    h, w, _ = frame.shape\n",
    "    aspect_ratio_target = target_width / target_height\n",
    "    aspect_ratio_frame = w / h\n",
    "    if aspect_ratio_frame > aspect_ratio_target:\n",
    "        new_width = int(h * aspect_ratio_target)\n",
    "        x_start = (w - new_width) // 2\n",
    "        frame_cropped = frame[:, x_start : x_start + new_width]\n",
    "    else:\n",
    "        new_height = int(w / aspect_ratio_target)\n",
    "        y_start = (h - new_height) // 2\n",
    "        frame_cropped = frame[y_start : y_start + new_height, :]\n",
    "    frame_resized = cv2.resize(frame_cropped, (target_width, target_height))\n",
    "    return frame_resized\n",
    "\n",
    "\n",
    "def load_video_to_tensor_with_resize(video_path, target_height, target_width):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        if target_height is not None:\n",
    "            frame_resized = center_crop_and_resize(\n",
    "                frame_rgb, target_height, target_width\n",
    "            )\n",
    "        else:\n",
    "            frame_resized = frame_rgb\n",
    "        frames.append(frame_resized)\n",
    "    cap.release()\n",
    "    video_np = (np.array(frames) / 127.5) - 1.0\n",
    "    video_tensor = torch.tensor(video_np).permute(3, 0, 1, 2).float()\n",
    "    return video_tensor\n",
    "\n",
    "\n",
    "def load_image_to_tensor_with_resize(image_path, target_height=512, target_width=768):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_np = np.array(image)\n",
    "    frame_resized = center_crop_and_resize(image_np, target_height, target_width)\n",
    "    frame_tensor = torch.tensor(frame_resized).permute(2, 0, 1).float()\n",
    "    frame_tensor = (frame_tensor / 127.5) - 1.0\n",
    "    # Create 5D tensor: (batch_size=1, channels=3, num_frames=1, height, width)\n",
    "    return frame_tensor.unsqueeze(0).unsqueeze(2)\n",
    "    \n",
    "def reset_memory(device):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    \n",
    "def get_openai_prompt_response(\n",
    "    prompt: str,\n",
    "    config: dict,\n",
    "    max_tokens: int = 6000,\n",
    "    temperature: float = 0.33,\n",
    "    openai_model: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response with retry logic.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Act as a helpful assistant, you are an expert editor.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=openai_model,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < config[\"retry_limit\"]:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            return message_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == config[\"retry_limit\"]:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{config['retry_limit']})\")\n",
    "                time.sleep(1)  # Optional: wait before retrying\n",
    "\n",
    "def get_openai_prompt_response_reasoning(\n",
    "    prompt: str,\n",
    "    config: dict,\n",
    "    max_tokens: int = 6000,\n",
    "    openai_model: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response with retry logic.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "    response = client.chat.completions.create(\n",
    "        max_completion_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=openai_model,\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < config[\"retry_limit\"]:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            return message_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == config[\"retry_limit\"]:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{config['retry_limit']})\")\n",
    "                time.sleep(1)  # Optional: wait before retrying\n",
    "\n",
    "\n",
    "def load_flux_pipe():\n",
    "    bfl_repo = \"black-forest-labs/FLUX.1-dev\"\n",
    "    revision = \"refs/pr/3\"\n",
    "    adapter_id = \"alimama-creative/FLUX.1-Turbo-Alpha\"\n",
    "\n",
    "    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(bfl_repo, subfolder=\"scheduler\", revision=revision)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "    text_encoder_2 = T5EncoderModel.from_pretrained(bfl_repo, subfolder=\"text_encoder_2\", torch_dtype=dtype, revision=revision)\n",
    "    tokenizer_2 = T5TokenizerFast.from_pretrained(bfl_repo, subfolder=\"tokenizer_2\", torch_dtype=dtype, revision=revision)\n",
    "    vae = AutoencoderKL.from_pretrained(bfl_repo, subfolder=\"vae\", torch_dtype=dtype, revision=revision)\n",
    "    transformer = FluxTransformer2DModel.from_pretrained(bfl_repo, subfolder=\"transformer\", torch_dtype=dtype, revision=revision)\n",
    "    \n",
    "    quantize_(transformer, quantization())\n",
    "    quantize_(text_encoder_2, quantization())\n",
    "    pipe = FluxPipeline(\n",
    "        scheduler=scheduler,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        text_encoder_2=text_encoder_2,\n",
    "        tokenizer_2=tokenizer_2,\n",
    "        vae=vae,\n",
    "        transformer=transformer,\n",
    "    )\n",
    "\n",
    "    pipe = pipe.to('cuda')\n",
    "    pipe.load_lora_weights(adapter_id)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def gen_flux_image(pipe, prompt, config: dict, height=1080, width=1920, guidance_scale=3.5, num_inference_steps=8, max_sequence_length=512, seed=-1):\n",
    "    \"\"\"\n",
    "    Generates an image based on the provided prompt using the Flux pipeline.\n",
    "    \"\"\"\n",
    "    prompt = \"masterpiece, \" + prompt\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds, pooled_prompt_embeds = get_weighted_text_embeddings_flux1(\n",
    "            pipe        = pipe,\n",
    "            prompt    = prompt\n",
    "        )\n",
    "        \n",
    "        image = pipe(\n",
    "            prompt_embeds               = prompt_embeds,\n",
    "            pooled_prompt_embeds      = pooled_prompt_embeds,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            guidance_scale=guidance_scale,\n",
    "            output_type=\"pil\",\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "        ).images[0]\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "def load_video_pipeline():\n",
    "    \"\"\"\n",
    "    Loads and configures the video generation pipeline.\n",
    "    \"\"\"\n",
    "    #pipe = LTXImageToVideoPipeline.from_pretrained(\"Lightricks/LTX-Video\", torch_dtype=torch.bfloat16)\n",
    "    pipe = LTXImageToVideoSTGPipeline.from_pretrained(\"Lightricks/LTX-Video\", torch_dtype=torch.bfloat16)\n",
    "    pipe.to(\"cuda\")\n",
    "\n",
    "    return pipe\n",
    "\n",
    "def generate_video(pipeline, prompt, image_input, config: dict, seed_value: int = -1, video_filename: str = \"\", num_frames: int = 151):\n",
    "    \"\"\"\n",
    "    Generates and saves a video from the provided image and prompt.\n",
    "    \"\"\"\n",
    "    negative_prompt = \"low quality, worst quality, deformed, distorted, disfigured, inconsistent motion, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly, fast motion, jittery, text, subtitles, captions, dark figures, silhouette, transition, scene change, inconsistent motion, blurry, watermarks\"\n",
    "    if seed_value == -1:\n",
    "        seed_value = random.randint(0, 255)\n",
    "\n",
    "    stg_mode = \"STG-R\" # STG-A, STG-R\n",
    "    # set to 2 orig, default is 8, 14 is best\n",
    "    stg_applied_layers_idx = [14] # 0~27\n",
    "    stg_scale = 0.8 # 0.0 for CFG\n",
    "\n",
    "    image = load_image(image_input)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            video = pipeline(\n",
    "                image=image,\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                width=WIDTH,\n",
    "                height=HEIGHT,\n",
    "                num_frames=num_frames,\n",
    "                num_inference_steps=NUM_INFERENCE_STEPS,\n",
    "                guidance_scale=GUIDANCE_SCALE,\n",
    "                generator=torch.manual_seed(seed_value),\n",
    "                stg_mode=stg_mode,\n",
    "                stg_applied_layers_idx=stg_applied_layers_idx,\n",
    "                stg_scale=stg_scale,\n",
    "        ).frames[0]\n",
    "        export_to_video(video, video_filename, fps=24)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating the video. Please try again. Error: {e}\")\n",
    "    finally:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return video_filename\n",
    "\n",
    "\n",
    "def save_video(frames, fps: int, filename: str):\n",
    "    \"\"\"\n",
    "    Saves a list of frames as a video file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        writer = imageio.get_writer(temp_video_path, fps=fps)\n",
    "        for frame in frames:\n",
    "            writer.append_data(np.array(frame))\n",
    "        writer.close()\n",
    "\n",
    "    os.rename(temp_video_path, filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def convert_to_gif(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a video file to a GIF.\n",
    "    \"\"\"\n",
    "    clip = mp.VideoFileClip(video_path)\n",
    "    clip = clip.set_fps(8)\n",
    "    clip = clip.resize(height=240)\n",
    "    gif_path = video_path.replace(\".mp4\", \".gif\")\n",
    "    clip.write_gif(gif_path, fps=8)\n",
    "    return gif_path\n",
    "\n",
    "\n",
    "def resize_if_unfit(input_video: str) -> str:\n",
    "    \"\"\"\n",
    "    Resizes the video to the target dimensions if it does not match.\n",
    "    \"\"\"\n",
    "    width, height = get_video_dimensions(input_video)\n",
    "\n",
    "    if width == 720 and height == 480:\n",
    "        return input_video\n",
    "    else:\n",
    "        return center_crop_resize(input_video)\n",
    "\n",
    "\n",
    "def get_video_dimensions(input_video_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Retrieves the dimensions of the video.\n",
    "    \"\"\"\n",
    "    reader = imageio_ffmpeg.read_frames(input_video_path)\n",
    "    metadata = next(reader)\n",
    "    return metadata[\"size\"]\n",
    "\n",
    "\n",
    "def center_crop_resize(input_video_path: str, target_width: int = 720, target_height: int = 480) -> str:\n",
    "    \"\"\"\n",
    "    Resizes and center-crops the video to the target dimensions.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    width_factor = target_width / orig_width\n",
    "    height_factor = target_height / orig_height\n",
    "    resize_factor = max(width_factor, height_factor)\n",
    "\n",
    "    inter_width = int(orig_width * resize_factor)\n",
    "    inter_height = int(orig_height * resize_factor)\n",
    "\n",
    "    target_fps = 8\n",
    "    ideal_skip = max(0, math.ceil(orig_fps / target_fps) - 1)\n",
    "    skip = min(5, ideal_skip)  # Cap at 5\n",
    "\n",
    "    while (total_frames / (skip + 1)) < 49 and skip > 0:\n",
    "        skip -= 1\n",
    "\n",
    "    processed_frames = []\n",
    "    frame_count = 0\n",
    "    total_read = 0\n",
    "\n",
    "    while frame_count < 49 and total_read < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if total_read % (skip + 1) == 0:\n",
    "            resized = cv2.resize(frame, (inter_width, inter_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            start_x = (inter_width - target_width) // 2\n",
    "            start_y = (inter_height - target_height) // 2\n",
    "            cropped = resized[start_y:start_y + target_height, start_x:start_x + target_width]\n",
    "\n",
    "            processed_frames.append(cropped)\n",
    "            frame_count += 1\n",
    "\n",
    "        total_read += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(temp_video_path, fourcc, target_fps, (target_width, target_height))\n",
    "\n",
    "        for frame in processed_frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    return temp_video_path\n",
    "\n",
    "\n",
    "def extract_last_frame(video_filename: str, output_image_filename: str):\n",
    "    \"\"\"\n",
    "    Extracts the last frame from a video file and saves it as an image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = imageio.get_reader(video_filename, 'ffmpeg')\n",
    "        last_frame = None\n",
    "        for frame in reader:\n",
    "            last_frame = frame\n",
    "        reader.close()\n",
    "\n",
    "        if last_frame is not None:\n",
    "            imageio.imwrite(output_image_filename, last_frame)\n",
    "            print(f\"Last frame saved successfully as '{output_image_filename}'.\")\n",
    "        else:\n",
    "            print(\"The video contains no frames.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{video_filename}' was not found.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except RuntimeError as re:\n",
    "        print(f\"RuntimeError: {re}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def create_scenes(text: str, video_summary: str, config: dict):\n",
    "    \"\"\"\n",
    "    Creates scenes based on the extracted lyrics using OpenAI's API.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Create a json list of diverse, unique scenes (groupings of text), scene_description, and action_sequence (100 words or less) from the following text.  Scenes should be groups of lyrics with new scenes when the lyric context changes.  Text: {text}   \n",
    "The json list should have the start value for the first item in the scene and the text that is combined for all items in the same scene.  \n",
    "The scene_description should include sensory rich details such as attire, setting, mood, lighting, and changing compositions, painting a clear visual scene consistent with the video theme and different from other scenes.  \n",
    "Use theme descriptions, such as graphic novel, water color, render, oil painting, etc.  When people are present in a scene, focus on a closeup of a single person, preferably anthropormorphic characters.\n",
    "Favor symbolism and artistic illustrations of concepts and feeling, emotion.  Favor nudity, semi-nude like see through and sheer, and natural beauty, including artistic horror when relevant.\n",
    "Evoke strong emotions with atmospheric lighting and tone. Use descriptive phrases to capture the mood (e.g., flickering neon, golden twilight, ethereal glow). Use religious references and analogies.\n",
    "Use special effects and CGI in scene descriptions, like glowing eyes, magical environments, electric auras, spiritual energy, etc. Use lots of CGI, special effects for a magical experience.\n",
    "The scene description should include spiritual aspects, symbolism, and artistic concepts with genius expression. Avoid scenes with more than a single person in a closeup.\n",
    "The action_sequence should describe the action in the scene.  Scenes should be unique, creative, imaginative, and awe-inspiring to create an amazing video.  Create beautiful and mesmerizing scene descriptions that are creative, unique, artistic, and imaginative. Each scene must be unique, imaginative, and visually captivating, blending creativity with artistic flair. Use powerful, descriptive language to craft scenes that are awe-inspiring and leave the audience in wonder. These scenes should evoke a sense of beauty, grandeur, mystery, or anything emotional, drawing from both realistic and fantastical elements. Ensure the descriptions are immersive, emotionally resonant, and filled with unexpected twists that engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  \n",
    "Action should avoid sudden or fast movement or zooms, avoid any fast camera movement.  Avoid human movements like walking, dancing, shopping, etc.  Avoid scene transitions.\n",
    "Make sure the scene desriptions and action sequences result in an artistic cinematic masterpiece full of inspiration, wonder, creativity, and imagination.\n",
    "The overall theme is: {THEME}\n",
    "These are example scene_descriptions: {SCENE_DESCRIPTIONS}\n",
    "Here are some examples of action_sequences: {ACTION_SEQUENCES}\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.66)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "def revise_scenes(scenes, config: dict):\n",
    "    \"\"\"\n",
    "    Revise scenes based on the extracted scenes.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Revise the JSON scenes to update the scene_description and action_sequence to engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  We want unique scenes, even ones in the same sequence. Use descriptions of special effects in the scenes.  JSON scenes: {scenes}   \n",
    "The scene_description should include sensory rich details such as attire, setting, mood, lighting, and changing compositions, painting a clear visual scene consistent with the video theme and different from other scenes.  \n",
    "Use theme descriptions, such as graphic novel, water color, render, oil painting, etc.  When people are present in a scene, focus on a closeup of a single person, preferably anthropormorphic characters.\n",
    "Favor symbolism and artistic illustrations of concepts and feeling, emotion.  Favor nudity, semi-nude like see through and sheer, and natural beauty, including artistic horror when relevant.\n",
    "Evoke strong emotions with atmospheric lighting and tone. Use descriptive phrases to capture the mood (e.g., flickering neon, golden twilight, ethereal glow). Use religious references and analogies.\n",
    "Use special effects and CGI in scene descriptions, like glowing eyes, magical environments, electric auras, spiritual energy, etc. Use lots of CGI, special effects for a magical experience.\n",
    "The scene description should include spiritual aspects, symbolism, and artistic concepts with genius expression. Avoid scenes with more than a single person in a closeup.\n",
    "The action_sequence should describe the action in the scene.  Scenes should be unique, creative, imaginative, and awe-inspiring to create an amazing video.  Create beautiful and mesmerizing scene descriptions that are creative, unique, artistic, and imaginative. Each scene must be unique, imaginative, and visually captivating, blending creativity with artistic flair. Use powerful, descriptive language to craft scenes that are awe-inspiring and leave the audience in wonder. These scenes should evoke a sense of beauty, grandeur, mystery, or anything emotional, drawing from both realistic and fantastical elements. Ensure the descriptions are immersive, emotionally resonant, and filled with unexpected twists that engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  \n",
    "Action should avoid sudden or fast movement or zooms, avoid any fast camera movement.  Avoid human movements like walking, dancing, shopping, etc.  Avoid scene transitions.\n",
    "Make sure the scene desriptions and action sequences result in an artistic cinematic masterpiece full of inspiration, wonder, creativity, and imagination.\n",
    "The overall theme is: {THEME}\n",
    "These are example scene_descriptions: {SCENE_DESCRIPTIONS}\n",
    "Here are some examples of action_sequences: {ACTION_SEQUENCES}\n",
    "The action_sequence (100 words or less) should describe the action in the scene.  The goal is to create input to create a stunning, cinematic video experience.   \n",
    "Action should avoid sudden or fast movements or zooms, avoid any fast movements, avoid any fast camera movements. \n",
    "Action sequences should include camera instructions.\n",
    "Only update the scene_description and action_sequence. We do not want to have similar scene_descriptions and action_sequences for consecutive scenes, we want unique scenes that tell a brilliant, cohesive story.  Please update the scene_description and action_sequence to be different, creative, and consistent.  \n",
    "Do not delete any items as having scenes with the given start times are important. \n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response_reasoning(prompt, config, openai_model=config[\"openai_model_small_reasoning\"])\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def process_audio_scenes(audio_file: str, config: dict):\n",
    "    # set maximum duration for an image basis, should be in intervals of video generation length\n",
    "    video_gen_length = 6\n",
    "    max_duration_seconds  = video_gen_length * 2\n",
    "    \"\"\"\n",
    "    Processes a single audio file through the entire workflow.\n",
    "    \"\"\"\n",
    "    # Create unique identifier based on audio file name\n",
    "    audio_basename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = f\"{audio_basename}_{timestamp}\"\n",
    "\n",
    "    # Create unique directories for images and videos\n",
    "    print(f\"Create unique directories for images and videos\")\n",
    "    audio_images_dir = os.path.join(config[\"base_working_dir\"], unique_id)\n",
    "    audio_videos_dir = os.path.join(config[\"base_video_dir\"], unique_id)\n",
    "    os.makedirs(audio_images_dir, exist_ok=True)\n",
    "    os.makedirs(audio_videos_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Transcribe audio using Whisper\n",
    "    print(f\"Transcribe audio using Whisper\")\n",
    "    model = whisper.load_model(\"turbo\")\n",
    "    result = model.transcribe(audio_file)\n",
    "\n",
    "    # Cleanup Whisper model memory\n",
    "    del model\n",
    "    reset_memory(device)\n",
    "\n",
    "    segments = result['segments']\n",
    "\n",
    "    # Extract list of start times and texts\n",
    "    segment_texts_and_start_times = [(segment['text'].strip(), segment['start']) for segment in segments]\n",
    "\n",
    "    # Combine texts\n",
    "    text = \"\"\n",
    "    for segment_text, start in segment_texts_and_start_times:\n",
    "        text += f\"Start: {start}, Text: {segment_text}\\n\"\n",
    "\n",
    "    last_end_value = segments[-1]['end']\n",
    "\n",
    "    # Path to scenes.json file\n",
    "    scenes_file_path = os.path.join(audio_images_dir, \"scenes.json\")\n",
    "\n",
    "    # Check if scenes.json exists\n",
    "    if os.path.exists(scenes_file_path):\n",
    "        print(f\"Scenes file already exists at {scenes_file_path}. Skipping scene generation.\")\n",
    "        with open(scenes_file_path, \"r\") as scenes_file:\n",
    "            scenes = json.load(scenes_file)\n",
    "        return scenes, audio_images_dir, audio_videos_dir, last_end_value\n",
    "\n",
    "    # Step 2: Generate video summary using OpenAI\n",
    "    print(f\"Generate video summary using OpenAI\")\n",
    "    video_summary_prompt = f'Create a short summary that describes a music video based on these lyrics: {text}'\n",
    "    video_summary = get_openai_prompt_response(video_summary_prompt, config, openai_model=config[\"openai_model\"])\n",
    "\n",
    "    # Step 3: Create scenes based on lyrics\n",
    "    print(f\"Create scenes based on lyrics\")\n",
    "    try:\n",
    "        scenes = create_scenes(text, video_summary, config)\n",
    "    except:\n",
    "        try:\n",
    "            scenes = create_scenes(text, video_summary, config)\n",
    "        except:\n",
    "            try:\n",
    "                scenes = create_scenes(text, video_summary, config)\n",
    "            except: \n",
    "                return \"\", audio_images_dir, audio_videos_dir, last_end_value\n",
    "            \n",
    "    # we don't want scenes longer than 18 seconds\n",
    "    new_scenes = []\n",
    "    for i in range(len(scenes)):\n",
    "        scene = scenes[i]\n",
    "        if i == 0:\n",
    "            start_time = 0\n",
    "        else:\n",
    "            start_time = scene['start']\n",
    "        # Determine the end time\n",
    "        if i < len(scenes) - 1:\n",
    "            end_time = scenes[i + 1]['start']\n",
    "        else:\n",
    "            end_time = last_end_value\n",
    "        duration = end_time - start_time\n",
    "        # Split the scene if duration exceeds 18 seconds\n",
    "        while duration > 18:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "            start_time += max_duration_seconds\n",
    "            duration = end_time - start_time\n",
    "        # Append the remaining part of the scene\n",
    "        if duration > 0:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "    # Replace the original scenes with the new list\n",
    "    scenes = new_scenes\n",
    "    # improve the scenes with a revision\n",
    "    try:\n",
    "        scenes_revised = revise_scenes(scenes, config)\n",
    "        scenes = scenes_revised\n",
    "        print(f'revised scenes')\n",
    "    except:\n",
    "        try:\n",
    "            scenes_revised = revise_scenes(scenes, config)\n",
    "            scenes = scenes_revised\n",
    "            print(f'revised scenes')\n",
    "        except:\n",
    "            print('cannot revise scenes')\n",
    "            \n",
    "    \n",
    "    # Save the scenes to scenes.json\n",
    "    with open(scenes_file_path, \"w\") as scenes_file:\n",
    "        json.dump(scenes, scenes_file)\n",
    "        \n",
    "    return scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def process_audio_images(config: dict, scenes, audio_images_dir):\n",
    "    # Step 4: Load Flux pipeline and generate images\n",
    "    print(f\"Load Flux pipeline and generate images\")\n",
    "    flux_pipe = load_flux_pipe()\n",
    "    height = HEIGHT\n",
    "    width = WIDTH\n",
    "    guidance_scale = 3.9\n",
    "    num_inference_steps = 16\n",
    "    max_sequence_length = 512\n",
    "    seed = -1\n",
    "\n",
    "    # Generate images for each scene\n",
    "    image_num = 1\n",
    "    for scene in scenes:\n",
    "        image_prompt = scene['scene_description']\n",
    "        image = gen_flux_image(flux_pipe, image_prompt, config, height, width, guidance_scale, num_inference_steps, max_sequence_length, seed)\n",
    "        filename = f\"image_{str(image_num).zfill(2)}.jpg\"\n",
    "        image_path = os.path.join(audio_images_dir, filename)\n",
    "        image.save(image_path, dpi=(300, 300))\n",
    "        image_num += 1\n",
    "\n",
    "    # Move the pipeline back to CPU and delete it\n",
    "    flux_pipe.to('cpu')\n",
    "    del flux_pipe\n",
    "    reset_memory(device)\n",
    "    return\n",
    "\n",
    "def process_audio_video(config: dict, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp):\n",
    "    # Step 6: Load Video Pipeline\n",
    "    print(f\"Load Video Pipeline\")\n",
    "    video_pipe = load_video_pipeline()\n",
    "\n",
    "    # Temporary image path\n",
    "    temp_image = os.path.join(audio_images_dir, \"temp_image.jpg\")\n",
    "    video_num = 1\n",
    "\n",
    "    # Step 7: Generate video sequences\n",
    "    for i, scene in enumerate(scenes):\n",
    "        prompt = scene[\"action_sequence\"]\n",
    "\n",
    "        # Use the initial image for each scene\n",
    "        image_input = os.path.join(audio_images_dir, f\"image_{str(i+1).zfill(2)}.jpg\")\n",
    "\n",
    "        # Calculate duration to keep the video in 6-second increments\n",
    "        if i + 1 < len(scenes):\n",
    "            next_start_time = scenes[i + 1][\"start\"]\n",
    "        else:\n",
    "            next_start_time = last_end_value  # Use the final ending time for the last scene\n",
    "\n",
    "        if i == 0:\n",
    "            duration = next_start_time\n",
    "        else:\n",
    "            duration = next_start_time - scene[\"start\"]\n",
    "        num_video_segments = int((duration + 3) // 6)\n",
    "\n",
    "        print(f\"Scene {i+1} has {num_video_segments} segments\")\n",
    "        for j in range(num_video_segments):\n",
    "            video_name = f\"v_ltx_{str(video_num).zfill(2)}_{str(i+1)}_{str(j+1).zfill(2)}_{timestamp}.mp4\"\n",
    "            video_output_path = os.path.join(audio_videos_dir, video_name)\n",
    "            generate_video(video_pipe, prompt, image_input, config, seed_value=-1, video_filename=video_output_path)\n",
    "            time.sleep(1)  # Pause for 1 second\n",
    "\n",
    "            # After generating the video, extract the last frame to use as input for the next segment\n",
    "            extract_last_frame(video_output_path, temp_image)\n",
    "\n",
    "            # Use the last frame as input for the next video segment in the same scene\n",
    "            image_input = temp_image\n",
    "\n",
    "            video_num += 1  # Increment video number for the next segment\n",
    "\n",
    "    # Move the pipeline back to CPU before deleting\n",
    "    video_pipe.to('cpu')\n",
    "    del video_pipe\n",
    "    reset_memory(device)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def process_all_audios(audio_file, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a list of audio files through the workflow.\n",
    "    \"\"\"\n",
    "    print(f\"Processing audio file: {audio_file}\")\n",
    "    scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_audio_scenes(audio_file, config)\n",
    "    print(f'{len(scenes)} scenes:\\n{json.dumps(scenes, indent=4)}')\n",
    "    print(f'last_end_value: {last_end_value} timestamp: {timestamp}')\n",
    "    # Create starting images for scenes\n",
    "    process_audio_images(config, scenes, audio_images_dir)\n",
    "    return config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def create_video():\n",
    "    config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_all_audios(audio_file, CONFIG)\n",
    "    process_audio_video(config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp)\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efd2eb-8304-4027-a8c3-eaafe04ade14",
   "metadata": {},
   "source": [
    "### Run new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0cc96-e609-4cd2-9c67-dcd63562988a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run new systems\n",
    "for audio_file in CONFIG[\"audio_files\"]:\n",
    "    create_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f01978-588d-4f2a-910a-68786df670de",
   "metadata": {},
   "source": [
    "### Run previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba626ac1-4c9d-487c-8939-afb86fde6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run saved config\n",
    "scenes_file_path = './images/DefundUnsound_20250221_191350/scenes.json'\n",
    "audio_images_dir = './images/DefundUnsound_20250221_191350'\n",
    "audio_videos_dir = './output/DefundUnsound_20250221_191350'\n",
    "timestamp = '20250221_191350'\n",
    "last_end_value = 232.5\n",
    "\n",
    "with open(scenes_file_path, \"r\") as scenes_file:\n",
    "    scenes = json.load(scenes_file)\n",
    "\n",
    "process_audio_video(CONFIG, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a0750-5e16-4713-8ac3-825cb8713762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634076e-3b8f-4b7a-8b4a-85c8549e8bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
