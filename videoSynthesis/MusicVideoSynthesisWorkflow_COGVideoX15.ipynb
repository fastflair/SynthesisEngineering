{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f87d72-5305-4778-ba0c-31549333c8db",
   "metadata": {},
   "source": [
    "# Music Video Synthesis\n",
    "* Extract lyrics from song with timestamps\n",
    "* Compose scenes, include timestamps\n",
    "* Construct video text prompt for each scene\n",
    "* Build videos for each scene\n",
    "* Stitch together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b074-e32d-45f2-977f-c923878625e6",
   "metadata": {},
   "source": [
    "# We will use openai whipser for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b45c210-fd2b-4381-9fd3-c1eb18feefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt install ffmpeg\n",
    "#!pip install --quiet --upgrade pip\n",
    "#!pip3 install torch torchvision torchaudio optimum-quanto torchao xformers\n",
    "#!pip install --quiet --upgrade openai-whisper openai\n",
    "# Ubuntu or Debian\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!pip install setuptools-rust\n",
    "#!pip install -U diffusers imageio imageio_ffmpeg opencv-python moviepy transformers huggingface-hub optimum pillow safetensors\n",
    "#!pip install git+https://github.com/xhinker/sd_embed.git@main\n",
    "#!pip install accelerate flash_attention numba -U\n",
    "#!pip install flash_attn --no-build-isolation\n",
    "#!pip install -r requirements.txt -U\n",
    "#!pip install numpy==1.26.4\n",
    "#!pip install git+https://github.com/zRzRzRzRzRzRzR/diffusers.git@cogvideox1.1-5b -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8537e766-eab2-4757-b6f9-fbac4da44930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 22:49:06.618298: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-08 22:49:06.638223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739076546.648629   37988 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739076546.651647   37988 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-08 22:49:06.664146: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import diffusers\n",
    "import gc\n",
    "import imageio\n",
    "import imageio_ffmpeg\n",
    "import json\n",
    "import math\n",
    "import moviepy as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import whisper\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from diffusers import AutoencoderKL, AutoencoderKLCogVideoX, AutoPipelineForText2Image, CogVideoXTransformer3DModel, CogVideoXPipeline, CogVideoXDPMScheduler\n",
    "from diffusers import CogVideoXTransformer3DModel, CogVideoXImageToVideoPipeline, FlowMatchEulerDiscreteScheduler, CogVideoXDPMScheduler\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from diffusers.utils import export_to_video, load_video, load_image\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from numba import cuda\n",
    "from openai import OpenAI\n",
    "from optimum.quanto import freeze, qfloat8, quantize, requantize\n",
    "from PIL import Image\n",
    "from safetensors.torch import load_file as load_safetensors, save_file as save_safetensors\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_flux1\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5TokenizerFast, T5EncoderModel as t_T5EncoderModel\n",
    "from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Define the paths where quantized weights will be saved\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "retry_limit = 3\n",
    "quantization = int8_weight_only\n",
    "\n",
    "WIDTH=1360\n",
    "HEIGHT=768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f54ba32-2208-45c8-8ed2-5fcb1e79aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"openai_api_key\": \"\",\n",
    "    \"openai_model\": \"gpt-4o-mini\",\n",
    "    \"openai_model_large\": \"gpt-4o\",\n",
    "    \"hf_token\": \"\",\n",
    "    \"base_working_dir\": \"./images\",\n",
    "    \"base_video_dir\": \"./output\",\n",
    "    \"audio_files\": [\n",
    "        \"/mnt/d/Share/Audio/Lightlines.mp3\",\n",
    "        \"/mnt/d/Share/Audio/Lightlines.mp3\",\n",
    "        \"/mnt/d/Share/Audio/Lightlines.mp3\",\n",
    "    ],\n",
    "    \"device\": device,\n",
    "    \"dtype\": dtype,\n",
    "    \"retry_limit\": retry_limit,\n",
    "    \"MAX_SEED\": MAX_SEED,\n",
    "}\n",
    "\n",
    "# Ensure base directories exist\n",
    "os.makedirs(CONFIG[\"base_working_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"base_video_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34281c1-e231-4472-9d9b-096fa23b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_memory(device):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    \n",
    "def get_openai_prompt_response(\n",
    "    prompt: str,\n",
    "    config: dict,\n",
    "    max_tokens: int = 6000,\n",
    "    temperature: float = 0.33,\n",
    "    openai_model: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response with retry logic.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Act as a helpful assistant, you are an expert editor.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=openai_model or config[\"openai_model\"],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < config[\"retry_limit\"]:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            return message_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == config[\"retry_limit\"]:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{config['retry_limit']})\")\n",
    "                time.sleep(1)  # Optional: wait before retrying\n",
    "\n",
    "\n",
    "def load_flux_pipe():\n",
    "    bfl_repo = \"black-forest-labs/FLUX.1-dev\"\n",
    "    revision = \"refs/pr/3\"\n",
    "    adapter_id = \"alimama-creative/FLUX.1-Turbo-Alpha\"\n",
    "\n",
    "    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(bfl_repo, subfolder=\"scheduler\", revision=revision)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "    text_encoder_2 = t_T5EncoderModel.from_pretrained(bfl_repo, subfolder=\"text_encoder_2\", torch_dtype=dtype, revision=revision)\n",
    "    tokenizer_2 = T5TokenizerFast.from_pretrained(bfl_repo, subfolder=\"tokenizer_2\", torch_dtype=dtype, revision=revision)\n",
    "    vae = AutoencoderKL.from_pretrained(bfl_repo, subfolder=\"vae\", torch_dtype=dtype, revision=revision)\n",
    "    transformer = FluxTransformer2DModel.from_pretrained(bfl_repo, subfolder=\"transformer\", torch_dtype=dtype, revision=revision)\n",
    "    \n",
    "    quantize_(transformer, quantization())\n",
    "    quantize_(text_encoder_2, quantization())\n",
    "    pipe = FluxPipeline(\n",
    "        scheduler=scheduler,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        text_encoder_2=text_encoder_2,\n",
    "        tokenizer_2=tokenizer_2,\n",
    "        vae=vae,\n",
    "        transformer=transformer,\n",
    "    )\n",
    "\n",
    "    pipe = pipe.to('cuda')\n",
    "    pipe.load_lora_weights(adapter_id)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def gen_flux_image(pipe, prompt, config: dict, height=1024, width=1024, guidance_scale=3.5, num_inference_steps=8, max_sequence_length=512, seed=-1):\n",
    "    \"\"\"\n",
    "    Generates an image based on the provided prompt using the Flux pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds, pooled_prompt_embeds = get_weighted_text_embeddings_flux1(\n",
    "            pipe        = pipe,\n",
    "            prompt    = prompt\n",
    "        )\n",
    "        \n",
    "        image = pipe(\n",
    "            prompt_embeds               = prompt_embeds,\n",
    "            pooled_prompt_embeds      = pooled_prompt_embeds,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            guidance_scale=guidance_scale,\n",
    "            output_type=\"pil\",\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "        ).images[0]\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "def load_video_pipeline():\n",
    "    \"\"\"\n",
    "    Loads and configures the video generation pipeline.\n",
    "    \"\"\"\n",
    "    text_encoder = t_T5EncoderModel.from_pretrained(\"THUDM/CogVideoX1.5-5B-I2V\", subfolder=\"text_encoder\", torch_dtype=dtype)\n",
    "    quantize_(text_encoder, quantization())\n",
    "\n",
    "    transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX1.5-5B-I2V\", subfolder=\"transformer\", torch_dtype=dtype)\n",
    "    quantize_(transformer, quantization())\n",
    "\n",
    "    vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX1.5-5B-I2V\", subfolder=\"vae\", torch_dtype=dtype)\n",
    "    quantize_(vae, quantization())\n",
    "\n",
    "    # Create pipeline and run inference\n",
    "    pipe = CogVideoXImageToVideoPipeline.from_pretrained(\n",
    "        \"THUDM/CogVideoX1.5-5B-I2V\",\n",
    "        text_encoder=text_encoder,\n",
    "        transformer=transformer,\n",
    "        vae=vae,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).to(device)\n",
    "    # If you're using with lora, add this code\n",
    "    #if lora_path:\n",
    "    #    pipe.load_lora_weights(lora_path, weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=\"test_1\")\n",
    "    #    pipe.fuse_lora(lora_scale=1 / lora_rank)    \n",
    "    pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "    #pipe.enable_model_cpu_offload()\n",
    "    pipe.vae.enable_tiling()\n",
    "    #pipe.vae.enable_slicing()\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def infer(pipe_image, prompt: str, image_input: str, config: dict, num_inference_steps: int = 50, guidance_scale: float = 7.0, seed: int = -1, num_frames: int = 49):\n",
    "    \"\"\"\n",
    "    Generates video frames from an image and prompt using the video pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 255)\n",
    "\n",
    "    image_input = Image.open(image_input).resize(size=(720, 480))  # Convert to PIL\n",
    "    image = load_image(image_input)\n",
    "\n",
    "    video_pt = pipe_image(\n",
    "        image=image,\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        num_videos_per_prompt=1,\n",
    "        use_dynamic_cfg=True,\n",
    "        output_type=\"pt\",\n",
    "        guidance_scale=guidance_scale,\n",
    "        height=HEIGHT,\n",
    "        width=WIDTH,\n",
    "        generator=torch.Generator(device=\"cpu\").manual_seed(seed),\n",
    "        num_frames=num_frames,\n",
    "    ).frames\n",
    "\n",
    "    return video_pt, seed\n",
    "\n",
    "\n",
    "def generate_video(pipe_image, prompt, image_input, config: dict, seed_value: int = -1, video_filename: str = \"\", num_frames: int = 65):\n",
    "    \"\"\"\n",
    "    Generates and saves a video from the provided image and prompt.\n",
    "    \"\"\"\n",
    "    prompt = \"Slow movements, slow camera. \" + prompt\n",
    "    latents, seed = infer(\n",
    "        pipe_image,\n",
    "        prompt,\n",
    "        image_input,\n",
    "        config,\n",
    "        num_inference_steps=60,\n",
    "        guidance_scale=6.0,\n",
    "        seed=seed_value,\n",
    "        num_frames=num_frames,\n",
    "    )\n",
    "    batch_size = latents.shape[0]\n",
    "    batch_video_frames = []\n",
    "    for batch_idx in range(batch_size):\n",
    "        pt_image = latents[batch_idx]\n",
    "        pt_image = torch.stack([pt_image[i] for i in range(pt_image.shape[0])])\n",
    "        image_np = VaeImageProcessor.pt_to_numpy(pt_image)\n",
    "        image_pil = VaeImageProcessor.numpy_to_pil(image_np)\n",
    "        batch_video_frames.append(image_pil)\n",
    "\n",
    "    video_path = save_video(\n",
    "        batch_video_frames[0],\n",
    "        fps=16,\n",
    "        filename=video_filename\n",
    "    )\n",
    "    # After processing frames\n",
    "    del latents\n",
    "    del batch_video_frames\n",
    "    reset_memory(device)\n",
    "    return video_path\n",
    "\n",
    "\n",
    "def save_video(frames, fps: int, filename: str):\n",
    "    \"\"\"\n",
    "    Saves a list of frames as a video file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        writer = imageio.get_writer(temp_video_path, fps=fps)\n",
    "        for frame in frames:\n",
    "            writer.append_data(np.array(frame))\n",
    "        writer.close()\n",
    "\n",
    "    os.rename(temp_video_path, filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def convert_to_gif(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a video file to a GIF.\n",
    "    \"\"\"\n",
    "    clip = mp.VideoFileClip(video_path)\n",
    "    clip = clip.set_fps(8)\n",
    "    clip = clip.resize(height=240)\n",
    "    gif_path = video_path.replace(\".mp4\", \".gif\")\n",
    "    clip.write_gif(gif_path, fps=8)\n",
    "    return gif_path\n",
    "\n",
    "\n",
    "def resize_if_unfit(input_video: str) -> str:\n",
    "    \"\"\"\n",
    "    Resizes the video to the target dimensions if it does not match.\n",
    "    \"\"\"\n",
    "    width, height = get_video_dimensions(input_video)\n",
    "\n",
    "    if width == 720 and height == 480:\n",
    "        return input_video\n",
    "    else:\n",
    "        return center_crop_resize(input_video)\n",
    "\n",
    "\n",
    "def get_video_dimensions(input_video_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Retrieves the dimensions of the video.\n",
    "    \"\"\"\n",
    "    reader = imageio_ffmpeg.read_frames(input_video_path)\n",
    "    metadata = next(reader)\n",
    "    return metadata[\"size\"]\n",
    "\n",
    "\n",
    "def center_crop_resize(input_video_path: str, target_width: int = 720, target_height: int = 480) -> str:\n",
    "    \"\"\"\n",
    "    Resizes and center-crops the video to the target dimensions.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    width_factor = target_width / orig_width\n",
    "    height_factor = target_height / orig_height\n",
    "    resize_factor = max(width_factor, height_factor)\n",
    "\n",
    "    inter_width = int(orig_width * resize_factor)\n",
    "    inter_height = int(orig_height * resize_factor)\n",
    "\n",
    "    target_fps = 8\n",
    "    ideal_skip = max(0, math.ceil(orig_fps / target_fps) - 1)\n",
    "    skip = min(5, ideal_skip)  # Cap at 5\n",
    "\n",
    "    while (total_frames / (skip + 1)) < 49 and skip > 0:\n",
    "        skip -= 1\n",
    "\n",
    "    processed_frames = []\n",
    "    frame_count = 0\n",
    "    total_read = 0\n",
    "\n",
    "    while frame_count < 49 and total_read < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if total_read % (skip + 1) == 0:\n",
    "            resized = cv2.resize(frame, (inter_width, inter_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            start_x = (inter_width - target_width) // 2\n",
    "            start_y = (inter_height - target_height) // 2\n",
    "            cropped = resized[start_y:start_y + target_height, start_x:start_x + target_width]\n",
    "\n",
    "            processed_frames.append(cropped)\n",
    "            frame_count += 1\n",
    "\n",
    "        total_read += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(temp_video_path, fourcc, target_fps, (target_width, target_height))\n",
    "\n",
    "        for frame in processed_frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    return temp_video_path\n",
    "\n",
    "\n",
    "def extract_last_frame(video_filename: str, output_image_filename: str):\n",
    "    \"\"\"\n",
    "    Extracts the last frame from a video file and saves it as an image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = imageio.get_reader(video_filename, 'ffmpeg')\n",
    "        last_frame = None\n",
    "        for frame in reader:\n",
    "            last_frame = frame\n",
    "        reader.close()\n",
    "\n",
    "        if last_frame is not None:\n",
    "            imageio.imwrite(output_image_filename, last_frame)\n",
    "            print(f\"Last frame saved successfully as '{output_image_filename}'.\")\n",
    "        else:\n",
    "            print(\"The video contains no frames.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{video_filename}' was not found.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except RuntimeError as re:\n",
    "        print(f\"RuntimeError: {re}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def create_scenes(text: str, video_summary: str, config: dict):\n",
    "    \"\"\"\n",
    "    Creates scenes based on the extracted lyrics using OpenAI's API.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "   prompt = f'''Create a json list of diverse, unique scenes (groupings of text), scene_description (200 words or less), and action_sequence (30 words or less) from the following text.  Scenes should be groups of lyrics with new scenes when the lyric context changes.  Text: {text}   \n",
    "The json list should have the start value for the first item in the scene and the text that is combined for all items in the same scene.  \n",
    "The scene_description should include sensory rich details such as attire, setting, mood, lighting, and changing compositions, painting a clear visual scene consistent with the video theme and different from other scenes.  Use theme descriptions, such as graphic novel, water color, render, oil painting, etc.  Scenes should avoid depictions of literal people, unless they are close up of a single person.  Favor symbolism and artistic illustrations of concepts and feeling, emotion.  Avoid depections of literal people. Evoke strong emotions with atmospheric lighting and tone. Use descriptive phrases to capture the mood (e.g., flickering neon, golden twilight, ethereal glow).\n",
    "Avoid scenes with many people moving.\n",
    "The action_sequence should describe the action in the scene.  Scenes should be unique, creative, imaginative, and awe-inspiring to create an amazing video.  Create beautiful and mesmerizing scene descriptions that are creative, unique, artistic, and imaginative. Each scene must be unique, imaginative, and visually captivating, blending creativity with artistic flair. Use powerful, descriptive language to craft scenes that are awe-inspiring and leave the audience in wonder. These scenes should evoke a sense of beauty, grandeur, mystery, or anything emotional, drawing from both realistic and fantastical elements. Ensure the descriptions are immersive, emotionally resonant, and filled with unexpected twists that engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  \n",
    "Action should avoid sudden or fast movement or zooms, avoid any fast camera movement.  Avoid human movements like walking, dancing, shopping, etc.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.66)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "def revise_scenes(scenes, config: dict):\n",
    "    \"\"\"\n",
    "    Revise scenes based on the extracted scenes.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Revise the JSON scenes to update the scene_description and action_sequence to engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  We want unique scenes, even ones in the same sequence. Use descriptions of special effects in the scenes.  JSON scenes: {scenes}   \n",
    "The scene_description (200 words or less) should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes. Use theme descriptions, such as graphic novel, water color, render, oil painting, etc.  Scenes should avoid depictions of literal people, unless they are close up of a single person.  Favor symbolism and artistic illustrations of concepts and feeling, emotion.  Avoid depections of literal people. Evoke strong emotions with atmospheric lighting and tone. Use descriptive phrases to capture the mood (e.g., flickering neon, golden twilight, ethereal glow).\n",
    "The action_sequence (30 words or less) should describe the action in the scene.  The goal is to create input to create a stunning, cinematic video experience.   \n",
    "Action should avoid sudden or fast movement or zooms, avoid any fast camera movement. Avoid human movements like walking, dancing, shopping, etc.\n",
    "Only update the scene_description and action_sequence. We do not want to have similar scene_descriptions and action_sequences for consecutive scenes, we want unique scenes that tell a brilliant, cohesive story.  Please update the scene_description and action_sequence to be differemt, creative, and consistent.  \n",
    "Do not delete any items as having scenes with the given start times are important. \n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.33)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def process_audio_scenes(audio_file: str, config: dict):\n",
    "    # set maximum duration for an image basis, should be in intervals of video generation length\n",
    "    video_gen_length = 4\n",
    "    max_duration_seconds  = video_gen_length * 2\n",
    "    \"\"\"\n",
    "    Processes a single audio file through the entire workflow.\n",
    "    \"\"\"\n",
    "    # Create unique identifier based on audio file name\n",
    "    audio_basename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = f\"{audio_basename}_{timestamp}\"\n",
    "\n",
    "    # Create unique directories for images and videos\n",
    "    print(f\"Create unique directories for images and videos\")\n",
    "    audio_images_dir = os.path.join(config[\"base_working_dir\"], unique_id)\n",
    "    audio_videos_dir = os.path.join(config[\"base_video_dir\"], unique_id)\n",
    "    os.makedirs(audio_images_dir, exist_ok=True)\n",
    "    os.makedirs(audio_videos_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Transcribe audio using Whisper\n",
    "    print(f\"Transcribe audio using Whisper\")\n",
    "    model = whisper.load_model(\"turbo\")\n",
    "    result = model.transcribe(audio_file)\n",
    "\n",
    "    # Cleanup Whisper model memory\n",
    "    del model\n",
    "    reset_memory(device)\n",
    "\n",
    "    segments = result['segments']\n",
    "\n",
    "    # Extract list of start times and texts\n",
    "    segment_texts_and_start_times = [(segment['text'].strip(), segment['start']) for segment in segments]\n",
    "\n",
    "    # Combine texts\n",
    "    text = \"\"\n",
    "    for segment_text, start in segment_texts_and_start_times:\n",
    "        text += f\"Start: {start}, Text: {segment_text}\\n\"\n",
    "\n",
    "    last_end_value = segments[-1]['end']\n",
    "\n",
    "    # Path to scenes.json file\n",
    "    scenes_file_path = os.path.join(audio_images_dir, \"scenes.json\")\n",
    "\n",
    "    # Check if scenes.json exists\n",
    "    if os.path.exists(scenes_file_path):\n",
    "        print(f\"Scenes file already exists at {scenes_file_path}. Skipping scene generation.\")\n",
    "        with open(scenes_file_path, \"r\") as scenes_file:\n",
    "            scenes = json.load(scenes_file)\n",
    "        return scenes, audio_images_dir, audio_videos_dir, last_end_value\n",
    "\n",
    "    # Step 2: Generate video summary using OpenAI\n",
    "    print(f\"Generate video summary using OpenAI\")\n",
    "    video_summary_prompt = f'Create a short summary that describes a music video based on these lyrics: {text}'\n",
    "    video_summary = get_openai_prompt_response(video_summary_prompt, config, openai_model=config[\"openai_model\"])\n",
    "\n",
    "    # Step 3: Create scenes based on lyrics\n",
    "    print(f\"Create scenes based on lyrics\")\n",
    "    try:\n",
    "        scenes = create_scenes(text, video_summary, config)\n",
    "    except:\n",
    "        try:\n",
    "            scenes = create_scenes(text, video_summary, config)\n",
    "        except:\n",
    "            try:\n",
    "                scenes = create_scenes(text, video_summary, config)\n",
    "            except: \n",
    "                return \"\", audio_images_dir, audio_videos_dir, last_end_value\n",
    "            \n",
    "    # we don't want scenes longer than 18 seconds\n",
    "    new_scenes = []\n",
    "    for i in range(len(scenes)):\n",
    "        scene = scenes[i]\n",
    "        if i == 0:\n",
    "            start_time = 0\n",
    "        else:\n",
    "            start_time = scene['start']\n",
    "        # Determine the end time\n",
    "        if i < len(scenes) - 1:\n",
    "            end_time = scenes[i + 1]['start']\n",
    "        else:\n",
    "            end_time = last_end_value\n",
    "        duration = end_time - start_time\n",
    "        # Split the scene if duration exceeds 18 seconds\n",
    "        while duration > 18:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "            start_time += max_duration_seconds\n",
    "            duration = end_time - start_time\n",
    "        # Append the remaining part of the scene\n",
    "        if duration > 0:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "    # Replace the original scenes with the new list\n",
    "    scenes = new_scenes\n",
    "    # improve the scenes with a revision\n",
    "    try:\n",
    "        scenes_revised = revise_scenes(scenes, config)\n",
    "        scenes = scenes_revised\n",
    "        print(f'revised scenes')\n",
    "    except:\n",
    "        try:\n",
    "            scenes_revised = revise_scenes(scenes, config)\n",
    "            scenes = scenes_revised\n",
    "            print(f'revised scenes')\n",
    "        except:\n",
    "            print('cannot revise scenes')\n",
    "            \n",
    "    \n",
    "    # Save the scenes to scenes.json\n",
    "    with open(scenes_file_path, \"w\") as scenes_file:\n",
    "        json.dump(scenes, scenes_file)\n",
    "        \n",
    "    return scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def process_audio_images(config: dict, scenes, audio_images_dir):\n",
    "    # Step 4: Load Flux pipeline and generate images\n",
    "    print(f\"Load Flux pipeline and generate images\")\n",
    "    flux_pipe = load_flux_pipe()\n",
    "    height = HEIGHT\n",
    "    width = WIDTH\n",
    "    guidance_scale = 3.9\n",
    "    num_inference_steps = 8\n",
    "    max_sequence_length = 512\n",
    "    seed = -1\n",
    "\n",
    "    # Generate images for each scene\n",
    "    image_num = 1\n",
    "    for scene in scenes:\n",
    "        image_prompt = scene['scene_description']\n",
    "        image = gen_flux_image(flux_pipe, image_prompt, config, height, width, guidance_scale, num_inference_steps, max_sequence_length, seed)\n",
    "        filename = f\"image_{str(image_num).zfill(2)}.jpg\"\n",
    "        image_path = os.path.join(audio_images_dir, filename)\n",
    "        image.save(image_path, dpi=(300, 300))\n",
    "        image_num += 1\n",
    "\n",
    "    # Move the pipeline back to CPU and delete it\n",
    "    flux_pipe.to('cpu')\n",
    "    del flux_pipe\n",
    "    reset_memory(device)\n",
    "    return\n",
    "\n",
    "def process_audio_video(config: dict, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp):\n",
    "    # Step 6: Load Video Pipeline\n",
    "    print(f\"Load Video Pipeline\")\n",
    "    video_pipe = load_video_pipeline()\n",
    "\n",
    "    # Temporary image path\n",
    "    temp_image = os.path.join(audio_images_dir, \"temp_image.jpg\")\n",
    "    video_num = 1\n",
    "\n",
    "    # Step 7: Generate video sequences\n",
    "    for i, scene in enumerate(scenes):\n",
    "        prompt = scene[\"action_sequence\"]\n",
    "\n",
    "        # Use the initial image for each scene\n",
    "        image_input = os.path.join(audio_images_dir, f\"image_{str(i+1).zfill(2)}.jpg\")\n",
    "\n",
    "        # Calculate duration to keep the video in 6-second increments\n",
    "        if i + 1 < len(scenes):\n",
    "            next_start_time = scenes[i + 1][\"start\"]\n",
    "        else:\n",
    "            next_start_time = last_end_value  # Use the final ending time for the last scene\n",
    "\n",
    "        if i == 0:\n",
    "            duration = next_start_time\n",
    "        else:\n",
    "            duration = next_start_time - scene[\"start\"]\n",
    "        num_video_segments = int((duration + 3) // 6)\n",
    "\n",
    "        print(f\"Scene {i+1} has {num_video_segments} segments\")\n",
    "        for j in range(num_video_segments):\n",
    "            video_name = f\"video_{str(video_num).zfill(2)}_{str(i+1)}_{str(j+1).zfill(2)}_{timestamp}.mp4\"\n",
    "            video_output_path = os.path.join(audio_videos_dir, video_name)\n",
    "            generate_video(video_pipe, prompt, image_input, config, seed_value=-1, video_filename=video_output_path)\n",
    "            time.sleep(1)  # Pause for 1 second\n",
    "\n",
    "            # After generating the video, extract the last frame to use as input for the next segment\n",
    "            extract_last_frame(video_output_path, temp_image)\n",
    "\n",
    "            # Use the last frame as input for the next video segment in the same scene\n",
    "            image_input = temp_image\n",
    "\n",
    "            video_num += 1  # Increment video number for the next segment\n",
    "\n",
    "    # Move the pipeline back to CPU before deleting\n",
    "    video_pipe.to('cpu')\n",
    "    del video_pipe\n",
    "    reset_memory(device)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def process_all_audios(audio_file, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a list of audio files through the workflow.\n",
    "    \"\"\"\n",
    "    print(f\"Processing audio file: {audio_file}\")\n",
    "    scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_audio_scenes(audio_file, config)\n",
    "    print(f'{len(scenes)} scenes:\\n{json.dumps(scenes, indent=4)}')\n",
    "    print(f'last_end_value: {last_end_value} timestamp: {timestamp}')\n",
    "    # Create starting images for scenes\n",
    "    process_audio_images(config, scenes, audio_images_dir)\n",
    "    return config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def create_video():\n",
    "    config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_all_audios(audio_file, CONFIG)\n",
    "    process_audio_video(config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp)\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efd2eb-8304-4027-a8c3-eaafe04ade14",
   "metadata": {},
   "source": [
    "### Run new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a0cc96-e609-4cd2-9c67-dcd63562988a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file: /mnt/d/Share/Audio/Lightlines.mp3\n",
      "Create unique directories for images and videos\n",
      "Transcribe audio using Whisper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silwa/anaconda3/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate video summary using OpenAI\n",
      "Create scenes based on lyrics\n",
      "revised scenes\n",
      "26 scenes:\n",
      "[\n",
      "    {\n",
      "        \"start\": 0,\n",
      "        \"text\": \"Angels on shapes, they're the cracks in the rust\\nWe drew swords on shadows in that warehouse of lies\\nSaw baby wings trembling, just demons in disguise\\nYou think heaven's a battlefield, nah, it's a sight\\nA thread through the needle of a blood-red sky\",\n",
      "        \"scene_description\": \"In a hauntingly atmospheric warehouse, shadows stretch like dark fingers across rusted beams. Ethereal angels flicker, their gossamer wings glinting in the dim light. Clad in tattered garments, hooded warriors stand poised, swords glimmering like silver in the eerie glow of a blood-red sky filtering through cracked windows. The air is thick with tension, a palpable silence hanging over the scene, as if time itself has paused. The mood is both mystical and foreboding, capturing a moment suspended between light and darkness.\",\n",
      "        \"action_sequence\": \"Angels shimmer in and out of view, while warriors engage in silent standoffs, swords raised, shadows swirling around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 8,\n",
      "        \"text\": \"Angels on shapes, they're the cracks in the rust\\nWe drew swords on shadows in that warehouse of lies\\nSaw baby wings trembling, just demons in disguise\\nYou think heaven's a battlefield, nah, it's a sight\\nA thread through the needle of a blood-red sky\",\n",
      "        \"scene_description\": \"In a hauntingly atmospheric warehouse, shadows stretch like dark fingers across rusted beams. Ethereal angels flicker, their gossamer wings glinting in the dim light. Clad in tattered garments, hooded warriors stand poised, swords glimmering like silver in the eerie glow of a blood-red sky filtering through cracked windows. The air is thick with tension, a palpable silence hanging over the scene, as if time itself has paused. The mood is both mystical and foreboding, capturing a moment suspended between light and darkness.\",\n",
      "        \"action_sequence\": \"Angels shimmer in and out of view, while warriors engage in silent standoffs, swords raised, shadows swirling around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 16,\n",
      "        \"text\": \"Angels on shapes, they're the cracks in the rust\\nWe drew swords on shadows in that warehouse of lies\\nSaw baby wings trembling, just demons in disguise\\nYou think heaven's a battlefield, nah, it's a sight\\nA thread through the needle of a blood-red sky\",\n",
      "        \"scene_description\": \"In a hauntingly atmospheric warehouse, shadows stretch like dark fingers across rusted beams. Ethereal angels flicker, their gossamer wings glinting in the dim light. Clad in tattered garments, hooded warriors stand poised, swords glimmering like silver in the eerie glow of a blood-red sky filtering through cracked windows. The air is thick with tension, a palpable silence hanging over the scene, as if time itself has paused. The mood is both mystical and foreboding, capturing a moment suspended between light and darkness.\",\n",
      "        \"action_sequence\": \"Angels shimmer in and out of view, while warriors engage in silent standoffs, swords raised, shadows swirling around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 30.0,\n",
      "        \"text\": \"We're all ghosts, keeping score in a silent war\\nAngels bleach the rage from the devil's core\\nAngels don't fall, they dissolve in a trust\\nLight lines, not halos in the dusk\",\n",
      "        \"scene_description\": \"In a dreamlike twilight landscape, ghostly figures float gracefully, their forms shimmering with an otherworldly glow. Ethereal light bathes the scene, casting soft shadows that dance across the ground. The air is thick with a serene yet melancholic energy, as angels weave through the shadows, their essence blending into the dusk. Wisps of light trace their movements, creating a celestial tapestry that speaks of trust and redemption. The mood is contemplative, evoking a blend of hope and sorrow, reminiscent of an oil painting.\",\n",
      "        \"action_sequence\": \"Ghostly figures glide seamlessly, as angels weave through shadows, leaving trails of light that dissipate into the twilight.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 38.0,\n",
      "        \"text\": \"We're all ghosts, keeping score in a silent war\\nAngels bleach the rage from the devil's core\\nAngels don't fall, they dissolve in a trust\\nLight lines, not halos in the dusk\",\n",
      "        \"scene_description\": \"In a dreamlike twilight landscape, ghostly figures float gracefully, their forms shimmering with an otherworldly glow. Ethereal light bathes the scene, casting soft shadows that dance across the ground. The air is thick with a serene yet melancholic energy, as angels weave through the shadows, their essence blending into the dusk. Wisps of light trace their movements, creating a celestial tapestry that speaks of trust and redemption. The mood is contemplative, evoking a blend of hope and sorrow, reminiscent of an oil painting.\",\n",
      "        \"action_sequence\": \"Ghostly figures glide seamlessly, as angels weave through shadows, leaving trails of light that dissipate into the twilight.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 54.879999999999995,\n",
      "        \"text\": \"Lucifer's the pill that makes the pain fade slower\\nAngels hum, you're enough where the static rolls over\\nAngels hum, you're enough where the static rolls over\\nAngels hum, you're enough where the static rolls over\",\n",
      "        \"scene_description\": \"In a surreal room alive with swirling colors, static patterns ripple across the walls like waves in an ocean of light. Angels, draped in flowing robes, hover softly, their serene faces glowing with warmth. The lighting transitions from cool blues to vibrant golds, creating a mesmerizing atmosphere. The mood is tranquil yet powerful, as the angels' gentle hum resonates, enveloping the space in a soothing embrace. Wisps of energy flow like gentle currents, inviting the viewer into a state of calm, reminiscent of a vivid dream.\",\n",
      "        \"action_sequence\": \"Angels hover gracefully, emitting a soft hum, while static patterns undulate rhythmically across the walls.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 62.879999999999995,\n",
      "        \"text\": \"Lucifer's the pill that makes the pain fade slower\\nAngels hum, you're enough where the static rolls over\\nAngels hum, you're enough where the static rolls over\\nAngels hum, you're enough where the static rolls over\",\n",
      "        \"scene_description\": \"In a surreal room alive with swirling colors, static patterns ripple across the walls like waves in an ocean of light. Angels, draped in flowing robes, hover softly, their serene faces glowing with warmth. The lighting transitions from cool blues to vibrant golds, creating a mesmerizing atmosphere. The mood is tranquil yet powerful, as the angels' gentle hum resonates, enveloping the space in a soothing embrace. Wisps of energy flow like gentle currents, inviting the viewer into a state of calm, reminiscent of a vivid dream.\",\n",
      "        \"action_sequence\": \"Angels hover gracefully, emitting a soft hum, while static patterns undulate rhythmically across the walls.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 76.34,\n",
      "        \"text\": \"10% flesh, 90 divine, stumbling blind\\nKarma's a code of sec where gods realign\\nLimbo's just a disco from a shape and souls\\nDancing with the baggage that the daylight stole\",\n",
      "        \"scene_description\": \"In a vibrant, pulsating limbo, colors burst and swirl like a cosmic disco. Ethereal figures embodying both flesh and spirit sway to an unseen rhythm, their movements fluid and graceful. The atmosphere is electric, filled with a kaleidoscope of lights that shimmer and shift, creating a visual symphony. The mood is celebratory yet introspective, as each figure dances with their shadows, representing burdens lost to the daylight. The setting feels alive, a blend of the celestial and the earthly, reminiscent of a graphic novel come to life.\",\n",
      "        \"action_sequence\": \"Figures sway and twirl, merging flesh and spirit, with lights pulsating in time to an unseen cosmic beat.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 84.34,\n",
      "        \"text\": \"10% flesh, 90 divine, stumbling blind\\nKarma's a code of sec where gods realign\\nLimbo's just a disco from a shape and souls\\nDancing with the baggage that the daylight stole\",\n",
      "        \"scene_description\": \"In a vibrant, pulsating limbo, colors burst and swirl like a cosmic disco. Ethereal figures embodying both flesh and spirit sway to an unseen rhythm, their movements fluid and graceful. The atmosphere is electric, filled with a kaleidoscope of lights that shimmer and shift, creating a visual symphony. The mood is celebratory yet introspective, as each figure dances with their shadows, representing burdens lost to the daylight. The setting feels alive, a blend of the celestial and the earthly, reminiscent of a graphic novel come to life.\",\n",
      "        \"action_sequence\": \"Figures sway and twirl, merging flesh and spirit, with lights pulsating in time to an unseen cosmic beat.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 92.34,\n",
      "        \"text\": \"10% flesh, 90 divine, stumbling blind\\nKarma's a code of sec where gods realign\\nLimbo's just a disco from a shape and souls\\nDancing with the baggage that the daylight stole\",\n",
      "        \"scene_description\": \"In a vibrant, pulsating limbo, colors burst and swirl like a cosmic disco. Ethereal figures embodying both flesh and spirit sway to an unseen rhythm, their movements fluid and graceful. The atmosphere is electric, filled with a kaleidoscope of lights that shimmer and shift, creating a visual symphony. The mood is celebratory yet introspective, as each figure dances with their shadows, representing burdens lost to the daylight. The setting feels alive, a blend of the celestial and the earthly, reminiscent of a graphic novel come to life.\",\n",
      "        \"action_sequence\": \"Figures sway and twirl, merging flesh and spirit, with lights pulsating in time to an unseen cosmic beat.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 100.34,\n",
      "        \"text\": \"10% flesh, 90 divine, stumbling blind\\nKarma's a code of sec where gods realign\\nLimbo's just a disco from a shape and souls\\nDancing with the baggage that the daylight stole\",\n",
      "        \"scene_description\": \"In a vibrant, pulsating limbo, colors burst and swirl like a cosmic disco. Ethereal figures embodying both flesh and spirit sway to an unseen rhythm, their movements fluid and graceful. The atmosphere is electric, filled with a kaleidoscope of lights that shimmer and shift, creating a visual symphony. The mood is celebratory yet introspective, as each figure dances with their shadows, representing burdens lost to the daylight. The setting feels alive, a blend of the celestial and the earthly, reminiscent of a graphic novel come to life.\",\n",
      "        \"action_sequence\": \"Figures sway and twirl, merging flesh and spirit, with lights pulsating in time to an unseen cosmic beat.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 112.9,\n",
      "        \"text\": \"Hearts, turns and weeds, Gabriel is a hush\\nFaith not a weapon, it's a crutch\\nReal static, where the light will be\",\n",
      "        \"scene_description\": \"In a serene garden overrun with wildflowers, the air is thick with the sweet scent of blooming petals. A figure resembling Gabriel stands gracefully, cloaked in soft pastels, embodying tranquility. The golden light of dusk filters through the foliage, casting a warm glow that caresses the scene. The mood is peaceful and reflective, as whispers of faith linger like a gentle breeze, while the weeds symbolize resilience, thriving amidst beauty. The garden feels alive, a canvas of hope and serenity.\",\n",
      "        \"action_sequence\": \"Gabriel stands still, surrounded by wildflowers, as whispers of faith weave through the air like a gentle breeze.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 120.9,\n",
      "        \"text\": \"Hearts, turns and weeds, Gabriel is a hush\\nFaith not a weapon, it's a crutch\\nReal static, where the light will be\",\n",
      "        \"scene_description\": \"In a serene garden overrun with wildflowers, the air is thick with the sweet scent of blooming petals. A figure resembling Gabriel stands gracefully, cloaked in soft pastels, embodying tranquility. The golden light of dusk filters through the foliage, casting a warm glow that caresses the scene. The mood is peaceful and reflective, as whispers of faith linger like a gentle breeze, while the weeds symbolize resilience, thriving amidst beauty. The garden feels alive, a canvas of hope and serenity.\",\n",
      "        \"action_sequence\": \"Gabriel stands still, surrounded by wildflowers, as whispers of faith weave through the air like a gentle breeze.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 128.9,\n",
      "        \"text\": \"Hearts, turns and weeds, Gabriel is a hush\\nFaith not a weapon, it's a crutch\\nReal static, where the light will be\",\n",
      "        \"scene_description\": \"In a serene garden overrun with wildflowers, the air is thick with the sweet scent of blooming petals. A figure resembling Gabriel stands gracefully, cloaked in soft pastels, embodying tranquility. The golden light of dusk filters through the foliage, casting a warm glow that caresses the scene. The mood is peaceful and reflective, as whispers of faith linger like a gentle breeze, while the weeds symbolize resilience, thriving amidst beauty. The garden feels alive, a canvas of hope and serenity.\",\n",
      "        \"action_sequence\": \"Gabriel stands still, surrounded by wildflowers, as whispers of faith weave through the air like a gentle breeze.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 136.9,\n",
      "        \"text\": \"Hearts, turns and weeds, Gabriel is a hush\\nFaith not a weapon, it's a crutch\\nReal static, where the light will be\",\n",
      "        \"scene_description\": \"In a serene garden overrun with wildflowers, the air is thick with the sweet scent of blooming petals. A figure resembling Gabriel stands gracefully, cloaked in soft pastels, embodying tranquility. The golden light of dusk filters through the foliage, casting a warm glow that caresses the scene. The mood is peaceful and reflective, as whispers of faith linger like a gentle breeze, while the weeds symbolize resilience, thriving amidst beauty. The garden feels alive, a canvas of hope and serenity.\",\n",
      "        \"action_sequence\": \"Gabriel stands still, surrounded by wildflowers, as whispers of faith weave through the air like a gentle breeze.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 144.9,\n",
      "        \"text\": \"Hearts, turns and weeds, Gabriel is a hush\\nFaith not a weapon, it's a crutch\\nReal static, where the light will be\",\n",
      "        \"scene_description\": \"In a serene garden overrun with wildflowers, the air is thick with the sweet scent of blooming petals. A figure resembling Gabriel stands gracefully, cloaked in soft pastels, embodying tranquility. The golden light of dusk filters through the foliage, casting a warm glow that caresses the scene. The mood is peaceful and reflective, as whispers of faith linger like a gentle breeze, while the weeds symbolize resilience, thriving amidst beauty. The garden feels alive, a canvas of hope and serenity.\",\n",
      "        \"action_sequence\": \"Gabriel stands still, surrounded by wildflowers, as whispers of faith weave through the air like a gentle breeze.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 156.08,\n",
      "        \"text\": \"We're the static, we're the light\\nWe're the static, we're the static, we're the static\\nWe're the static, we're the static\",\n",
      "        \"scene_description\": \"In a vast, dark expanse, sparks of light scatter like stars across the cosmos. The static hum resonates, creating a harmonious symphony of energy. Figures emerge from the shadows, illuminated by glowing static, their faces reflecting determination and unity. The lighting shifts from deep shadows to vibrant bursts, embodying the essence of life and connection. The mood is empowering, capturing the triumph of spirit amidst the void, reminiscent of an epic graphic novel.\",\n",
      "        \"action_sequence\": \"Figures emerge from the darkness, united in glowing static, as sparks of light burst and resonate around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 164.08,\n",
      "        \"text\": \"We're the static, we're the light\\nWe're the static, we're the static, we're the static\\nWe're the static, we're the static\",\n",
      "        \"scene_description\": \"In a vast, dark expanse, sparks of light scatter like stars across the cosmos. The static hum resonates, creating a harmonious symphony of energy. Figures emerge from the shadows, illuminated by glowing static, their faces reflecting determination and unity. The lighting shifts from deep shadows to vibrant bursts, embodying the essence of life and connection. The mood is empowering, capturing the triumph of spirit amidst the void, reminiscent of an epic graphic novel.\",\n",
      "        \"action_sequence\": \"Figures emerge from the darkness, united in glowing static, as sparks of light burst and resonate around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 172.08,\n",
      "        \"text\": \"We're the static, we're the light\\nWe're the static, we're the static, we're the static\\nWe're the static, we're the static\",\n",
      "        \"scene_description\": \"In a vast, dark expanse, sparks of light scatter like stars across the cosmos. The static hum resonates, creating a harmonious symphony of energy. Figures emerge from the shadows, illuminated by glowing static, their faces reflecting determination and unity. The lighting shifts from deep shadows to vibrant bursts, embodying the essence of life and connection. The mood is empowering, capturing the triumph of spirit amidst the void, reminiscent of an epic graphic novel.\",\n",
      "        \"action_sequence\": \"Figures emerge from the darkness, united in glowing static, as sparks of light burst and resonate around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 180.08,\n",
      "        \"text\": \"We're the static, we're the light\\nWe're the static, we're the static, we're the static\\nWe're the static, we're the static\",\n",
      "        \"scene_description\": \"In a vast, dark expanse, sparks of light scatter like stars across the cosmos. The static hum resonates, creating a harmonious symphony of energy. Figures emerge from the shadows, illuminated by glowing static, their faces reflecting determination and unity. The lighting shifts from deep shadows to vibrant bursts, embodying the essence of life and connection. The mood is empowering, capturing the triumph of spirit amidst the void, reminiscent of an epic graphic novel.\",\n",
      "        \"action_sequence\": \"Figures emerge from the darkness, united in glowing static, as sparks of light burst and resonate around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 188.08,\n",
      "        \"text\": \"We're the static, we're the light\\nWe're the static, we're the static, we're the static\\nWe're the static, we're the static\",\n",
      "        \"scene_description\": \"In a vast, dark expanse, sparks of light scatter like stars across the cosmos. The static hum resonates, creating a harmonious symphony of energy. Figures emerge from the shadows, illuminated by glowing static, their faces reflecting determination and unity. The lighting shifts from deep shadows to vibrant bursts, embodying the essence of life and connection. The mood is empowering, capturing the triumph of spirit amidst the void, reminiscent of an epic graphic novel.\",\n",
      "        \"action_sequence\": \"Figures emerge from the darkness, united in glowing static, as sparks of light burst and resonate around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 196.08,\n",
      "        \"text\": \"We're the static, we're the light\\nWe're the static, we're the static, we're the static\\nWe're the static, we're the static\",\n",
      "        \"scene_description\": \"In a vast, dark expanse, sparks of light scatter like stars across the cosmos. The static hum resonates, creating a harmonious symphony of energy. Figures emerge from the shadows, illuminated by glowing static, their faces reflecting determination and unity. The lighting shifts from deep shadows to vibrant bursts, embodying the essence of life and connection. The mood is empowering, capturing the triumph of spirit amidst the void, reminiscent of an epic graphic novel.\",\n",
      "        \"action_sequence\": \"Figures emerge from the darkness, united in glowing static, as sparks of light burst and resonate around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 204.08,\n",
      "        \"text\": \"We're the static, we're the light\\nWe're the static, we're the static, we're the static\\nWe're the static, we're the static\",\n",
      "        \"scene_description\": \"In a vast, dark expanse, sparks of light scatter like stars across the cosmos. The static hum resonates, creating a harmonious symphony of energy. Figures emerge from the shadows, illuminated by glowing static, their faces reflecting determination and unity. The lighting shifts from deep shadows to vibrant bursts, embodying the essence of life and connection. The mood is empowering, capturing the triumph of spirit amidst the void, reminiscent of an epic graphic novel.\",\n",
      "        \"action_sequence\": \"Figures emerge from the darkness, united in glowing static, as sparks of light burst and resonate around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 212.08,\n",
      "        \"text\": \"We're the static, we're the light\\nWe're the static, we're the static, we're the static\\nWe're the static, we're the static\",\n",
      "        \"scene_description\": \"In a vast, dark expanse, sparks of light scatter like stars across the cosmos. The static hum resonates, creating a harmonious symphony of energy. Figures emerge from the shadows, illuminated by glowing static, their faces reflecting determination and unity. The lighting shifts from deep shadows to vibrant bursts, embodying the essence of life and connection. The mood is empowering, capturing the triumph of spirit amidst the void, reminiscent of an epic graphic novel.\",\n",
      "        \"action_sequence\": \"Figures emerge from the darkness, united in glowing static, as sparks of light burst and resonate around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 220.08,\n",
      "        \"text\": \"We're the static, we're the light\\nWe're the static, we're the static, we're the static\\nWe're the static, we're the static\",\n",
      "        \"scene_description\": \"In a vast, dark expanse, sparks of light scatter like stars across the cosmos. The static hum resonates, creating a harmonious symphony of energy. Figures emerge from the shadows, illuminated by glowing static, their faces reflecting determination and unity. The lighting shifts from deep shadows to vibrant bursts, embodying the essence of life and connection. The mood is empowering, capturing the triumph of spirit amidst the void, reminiscent of an epic graphic novel.\",\n",
      "        \"action_sequence\": \"Figures emerge from the darkness, united in glowing static, as sparks of light burst and resonate around them.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 228.08,\n",
      "        \"text\": \"We're the static, we're the light\\nWe're the static, we're the static, we're the static\\nWe're the static, we're the static\",\n",
      "        \"scene_description\": \"In a vast, dark expanse, sparks of light scatter like stars across the cosmos. The static hum resonates, creating a harmonious symphony of energy. Figures emerge from the shadows, illuminated by glowing static, their faces reflecting determination and unity. The lighting shifts from deep shadows to vibrant bursts, embodying the essence of life and connection. The mood is empowering, capturing the triumph of spirit amidst the void, reminiscent of an epic graphic novel.\",\n",
      "        \"action_sequence\": \"Figures emerge from the darkness, united in glowing static, as sparks of light burst and resonate around them.\"\n",
      "    }\n",
      "]\n",
      "last_end_value: 246.06 timestamp: 20250208_224909\n",
      "Load Flux pipeline and generate images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b2a2aea09c42009c408bde707fe62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c72e85d39b4f75b0a5eff8b7793d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fbdcf93c3c450daf992ff8868fe8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (112 > 77). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7685c9025884698bcc78eee77776083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e79f19d370441389c68ea98af70681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e08979d13e49b99fe4ce033e0fc4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571bd7c8e99a43228c2f020778f68229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4222decd91422182a88c775280e3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b0be4e26e94115871081498e442177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5dd3d88c104d498106e8e91f6da3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4133e003184f9db764cf98777b259d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8449e5cb9f41e08e563eb7a0b6c30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24535b1711294387b9ae44a3317fe2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d3aafd3c794f08934d18ec50e80ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf4d79637e04d658ac28f45dc00a24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55412535bb44894a5e52aae3c7aa40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca83ccb32ce4d25932656ec39754a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c18b5aa2fc4df1aabf25d6baec63d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39692485d1894ee596b503ff7550099d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e24dda1ec84528908d649992448fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264fcb80b52e4db6af265cbf0a5f9d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346296459840419780fcae1a9e64cdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0be7ecf2922486ab5380df1e7d31c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea30dc157ae4115902e101f3d6f9094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3552c82e194b43a0bda6f62d9b444abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed4d2385983477280c5516ee7024357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c383712ea44460aa34745061006c814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe916961629a4f02b44b1ef1c2091fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee346b35fca644b787ac8af010f107ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Video Pipeline\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c52585c9ec4c1691f9f5eb67a552dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cba4acd554445f9a81b70fe1a5ba4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c98e43ecc141d4a7120679153dc225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbb7ae881f1496599db08a1a69665fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9685b096d1345aaaa77ffcff9007c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 2 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3849b236d0c34ba69c59e3797ca0e5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 3 has 2 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e0413de39c49ff841b08d5e610bcba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e92c76065f4427caa81e6a41d92c87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 4 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1334cafb48e048b587d2ceac16dedae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 5 has 3 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71aefc839414582b03bbcd84e6e4bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb6f2bc23d84f68a1683955ff7f620e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb347f81bf94b52bec2f0f712b6e036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 6 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c0e4104d654e50854262e70da31a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 7 has 2 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f748acce9d3e46739c59cfff1714ed98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc782df1223c4c56b5a8a0fec2ce98e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 8 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a233229479c34e1e891a54ebfc8609cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 9 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b43fe9fc274453e867511cefec57692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 10 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0129e973c3a44fb8aba1579e81f66eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 11 has 2 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fc62f458de494cb503b3d6dd4996bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6198caf6b4465c993843b29a3ab5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 12 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21132f28e474fdaaec0567b95055cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 13 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8b47114de04538a39b231a0e8d202c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 14 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146ce15b61a444e29a8e3e431b3e44cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 15 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bc18a444cd4e3fb3478588ede82004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 16 has 2 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7163501a53c54233832d6e04564c22a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4474af7db240b0b22ca4462cb70b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 17 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c228357c986454bb9b5dd6dd798bbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 18 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e1d439524d47e5926803c9fd066ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 19 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb26131aca464e83aecea563b6ac4dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 20 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e7726a2a5c43bc9f664eb8cb67335d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 21 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e626967f1104d25b7e63d0d72367a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 22 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac498dab9f64e5fa776df3ee55d5fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 23 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e9d614733047a393cca714beb44bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 24 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a70e910e7364c07a1eb0b2da2dfb178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 25 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d56091936be46c492bfc1acfdb4b412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Scene 26 has 3 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed202f1b3e44341a99cf1b0ab75fc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1178077a24c4270aac093d6b69653dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a914b0a4df455e9b7cc073ca9f3f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250208_224909/temp_image.jpg'.\n",
      "Processing audio file: /mnt/d/Share/Audio/Lightlines.mp3\n",
      "Create unique directories for images and videos\n",
      "Transcribe audio using Whisper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silwa/anaconda3/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate video summary using OpenAI\n",
      "Create scenes based on lyrics\n",
      "revised scenes\n",
      "25 scenes:\n",
      "[\n",
      "    {\n",
      "        \"start\": 0,\n",
      "        \"text\": \"Angels on shapes, they're the cracks in the rust\\nWe drew swords on shadows in that warehouse of lies\\nSaw baby wings trembling, just demons in disguise\\nYou think heaven's a battlefield, nah, it's a sight\\nA thread through the needle of a blood-red sky\",\n",
      "        \"scene_description\": \"In a shadowy, derelict warehouse, rusted beams stretch overhead like skeletal fingers. The air is thick with an electric tension, and flickering shadows play tricks on the walls. Cloaked figures, their attire a mix of ancient armor and tattered fabric, wield swords that shimmer with an otherworldly light. Ethereal angelic forms pulse in and out of view, their delicate wings casting ghostly reflections against the backdrop of a blood-red sky seeping through shattered windows. The atmosphere is charged with anticipation, a haunting blend of beauty and dread.\",\n",
      "        \"action_sequence\": \"Figures move gracefully, swords gliding through the air as shadows swirl around them, creating a mesmerizing dance of light and darkness.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 8,\n",
      "        \"text\": \"Angels on shapes, they're the cracks in the rust\\nWe drew swords on shadows in that warehouse of lies\\nSaw baby wings trembling, just demons in disguise\\nYou think heaven's a battlefield, nah, it's a sight\\nA thread through the needle of a blood-red sky\",\n",
      "        \"scene_description\": \"The abandoned warehouse looms, its rusted beams casting long shadows. A palpable tension fills the air, thick with whispers of secrets. Cloaked figures stand poised, their swords glinting faintly in the dim light. Ethereal shapes flicker, revealing glimpses of angelic wings intertwined with darker forms. The haunting glow of a blood-red sky seeps through broken panes, illuminating the scene with an eerie beauty, evoking a sense of foreboding and wonder.\",\n",
      "        \"action_sequence\": \"Figures engage in a slow, fluid motion, swords tracing arcs in the air as shadows weave around them, creating an ethereal tableau.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 16,\n",
      "        \"text\": \"Angels on shapes, they're the cracks in the rust\\nWe drew swords on shadows in that warehouse of lies\\nSaw baby wings trembling, just demons in disguise\\nYou think heaven's a battlefield, nah, it's a sight\\nA thread through the needle of a blood-red sky\",\n",
      "        \"scene_description\": \"Within the warehouse, rusted metal beams stretch like ancient trees. The atmosphere is thick with anticipation, shadows flickering like restless spirits. Figures in tattered cloaks wield swords that shimmer with a ghostly light. Ethereal angelic shapes pulse softly, their wings hinting at hidden darkness. The scene is bathed in the haunting glow of a blood-red sky, creating a surreal contrast between light and shadow, beauty and despair.\",\n",
      "        \"action_sequence\": \"Figures move in a slow, deliberate dance, swords gliding through the air as shadows swirl, creating a haunting visual symphony.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 30,\n",
      "        \"text\": \"We're all ghosts, keeping score in a silent war\\nAngels bleach the rage from the devil's core\\nAngels don't fall, they dissolve in a trust\\nLight lines, not halos in the dusk\",\n",
      "        \"scene_description\": \"In a spectral realm, translucent figures float amidst a twilight haze. Wisps of shimmering light weave through them, creating delicate patterns that resemble constellations. The atmosphere is serene yet haunting, as angels dissolve into soft glows, merging with the dusky backdrop. The colors swirl in muted tones, embodying the struggle between light and darkness, evoking a sense of peace amidst the chaos.\",\n",
      "        \"action_sequence\": \"Ghostly figures glide gracefully, merging and shifting like smoke, embodying the harmony of trust in a silent struggle.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 38,\n",
      "        \"text\": \"We're all ghosts, keeping score in a silent war\\nAngels bleach the rage from the devil's core\\nAngels don't fall, they dissolve in a trust\\nLight lines, not halos in the dusk\",\n",
      "        \"scene_description\": \"A dreamlike landscape unfolds, where ethereal figures float through a soft twilight. Wisps of light dance around them, creating a celestial tapestry. The mood is both tranquil and haunting, as angels dissolve into gentle glows, their forms blending seamlessly with the dusk. The background swirls with muted colors, symbolizing the eternal battle between light and darkness, offering a glimpse of serenity amidst turmoil.\",\n",
      "        \"action_sequence\": \"Figures drift slowly, merging and flowing like mist, embodying the essence of trust and unity in a silent conflict.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 54.88,\n",
      "        \"text\": \"Lucifer's the pill that makes the pain fade slower\\nAngels hum, you're enough where the static goes over\\nAngels hum, you're enough where the static goes over\\nAngels hum, you're enough where the static goes over\",\n",
      "        \"scene_description\": \"In a surreal dreamscape, swirling clouds of static envelop the scene, pulsating with an ethereal hum. Angels float serenely, their expressions calm and inviting, extending their hands toward the viewer. The colors are soft and muted, creating a tranquil ambiance that contrasts with the chaos beyond. Light glimmers off their wings, casting gentle reflections that dance across the static, creating a mesmerizing visual symphony.\",\n",
      "        \"action_sequence\": \"Angels sway gently, their harmonizing hums resonating through the static, enveloping the scene in a soothing embrace.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 62.88,\n",
      "        \"text\": \"Lucifer's the pill that makes the pain fade slower\\nAngels hum, you're enough where the static goes over\\nAngels hum, you're enough where the static goes over\\nAngels hum, you're enough where the static goes over\",\n",
      "        \"scene_description\": \"A dreamlike atmosphere unfolds, where clouds of static swirl gently, pulsating with a soft hum. Angels float gracefully, their serene expressions radiating warmth as they reach out with inviting gestures. The muted colors create a calming backdrop, contrasting with the static's chaotic energy. Their wings catch the light, casting delicate reflections that ripple through the air, creating a tranquil visual melody.\",\n",
      "        \"action_sequence\": \"Angels move in slow, fluid motions, their hums resonating softly, wrapping the scene in a cocoon of peace.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 70.88,\n",
      "        \"text\": \"Lucifer's the pill that makes the pain fade slower\\nAngels hum, you're enough where the static goes over\\nAngels hum, you're enough where the static goes over\\nAngels hum, you're enough where the static goes over\",\n",
      "        \"scene_description\": \"In a surreal realm, static clouds swirl around, pulsating with an otherworldly hum. Angels float serenely, their expressions filled with compassion, extending their hands in a gesture of comfort. The soft, muted colors create a serene atmosphere, contrasting with the chaotic energy of the static. Light glimmers off their wings, casting gentle reflections that dance through the air, creating a captivating visual harmony.\",\n",
      "        \"action_sequence\": \"Angels sway gently, their soothing hums filling the air, enveloping the scene in a tranquil embrace.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 88.08,\n",
      "        \"text\": \"10% flesh, 90 divine, stumbling blind\\nKarma's a code of sec where gods realign\\nLimbo's just a disco from a shape and soles\\nDancing with the baggage that the daylight stole\",\n",
      "        \"scene_description\": \"In a vibrant limbo, pulsating lights flicker like stars in a nightclub atmosphere. Figures, part human and part divine, sway rhythmically, their silhouettes shifting in and out of focus. The air buzzes with electric energy, as colorful lights cast playful shadows on the walls. The mood is celebratory yet reflective, as these beings dance with their unseen burdens, remnants of daylight lingering like ghosts of the past.\",\n",
      "        \"action_sequence\": \"Figures move fluidly, intertwining in a dance that blends joy and sorrow, each step resonating with the weight of their past.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 96.08,\n",
      "        \"text\": \"10% flesh, 90 divine, stumbling blind\\nKarma's a code of sec where gods realign\\nLimbo's just a disco from a shape and soles\\nDancing with the baggage that the daylight stole\",\n",
      "        \"scene_description\": \"A lively limbo unfolds, filled with pulsating lights and flickering shadows, creating a vibrant nightclub ambiance. Figures, embodying both human and divine traits, sway in harmony, their silhouettes shifting like waves. The air crackles with energy, as colorful lights playfully dance across the walls. The atmosphere is a blend of celebration and introspection, as these beings navigate their unseen burdens, remnants of daylight echoing like whispers.\",\n",
      "        \"action_sequence\": \"Figures sway gracefully, their movements a blend of joy and reflection, each step echoing the weight of their journey.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 109.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, verdant landscape, the air is thick with humidity, saturating the colors with life. Warriors and sisters stand in a solemn circle, their attire a blend of ancient armor and flowing fabrics that ripple in the warm breeze. The atmosphere is charged with anticipation, as whispers of faith float like soft melodies, creating a sense of unity. The scene shifts between moments of stillness and gentle movement, reflecting the ebb and flow of life.\",\n",
      "        \"action_sequence\": \"Figures breathe in unison, embodying strength and serenity as they stand together, united in purpose.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 117.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a vibrant landscape, the air is thick with humidity, enhancing the colors' brilliance. Warriors and sisters form a circle, their attire a mix of ancient armor and flowing fabrics that dance in the warm breeze. The atmosphere is heavy with anticipation, as whispers of faith echo softly, creating an aura of unity and strength. The scene flows between stillness and gentle movement, mirroring the rhythm of life.\",\n",
      "        \"action_sequence\": \"Figures stand resolute, breathing together in a harmonious rhythm, embodying the essence of strength and unity.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 125.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, verdant setting, humidity thickens the air, making colors pop with vibrancy. Warriors and sisters stand poised in a circle, their attire blending ancient armor with flowing fabrics that flutter in the gentle breeze. The mood is charged with anticipation, as soft whispers of faith fill the air, creating a sense of unity. The scene ebbs and flows, capturing the essence of life\\u2019s rhythm.\",\n",
      "        \"action_sequence\": \"Figures breathe in harmony, embodying strength and serenity as they unite in purpose.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 133.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, vibrant landscape, the air is thick with humidity, enhancing the colors' intensity. Warriors and sisters stand in a circle, their attire a fusion of ancient armor and flowing fabrics that ripple in the warm breeze. The atmosphere is heavy with anticipation, as whispers of faith drift through the air, creating an aura of unity. The scene oscillates between stillness and gentle movement, reflecting life's ebb and flow.\",\n",
      "        \"action_sequence\": \"Figures breathe together, embodying strength and serenity in a sacred bond.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 141.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, verdant landscape, the humidity saturates the air, making colors vibrant and alive. Warriors and sisters stand poised in a circle, their attire a blend of ancient armor and flowing fabrics that flutter in the warm breeze. The atmosphere is thick with anticipation, as soft whispers of faith float through the air, creating a sense of unity. The scene shifts between moments of stillness and gentle movement, reflecting the ebb and flow of life.\",\n",
      "        \"action_sequence\": \"Figures breathe in unison, embodying strength and serenity as they unite in purpose.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 149.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, vibrant landscape, the air is thick with humidity, enhancing the colors' brilliance. Warriors and sisters form a circle, their attire a mix of ancient armor and flowing fabrics that dance in the warm breeze. The atmosphere is heavy with anticipation, as whispers of faith echo softly, creating an aura of unity and strength. The scene flows between stillness and gentle movement, mirroring the rhythm of life.\",\n",
      "        \"action_sequence\": \"Figures stand resolute, breathing together in a harmonious rhythm, embodying the essence of strength and unity.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 157.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, verdant setting, humidity thickens the air, making colors pop with vibrancy. Warriors and sisters stand poised in a circle, their attire blending ancient armor with flowing fabrics that flutter in the gentle breeze. The mood is charged with anticipation, as soft whispers of faith fill the air, creating a sense of unity. The scene ebbs and flows, capturing the essence of life\\u2019s rhythm.\",\n",
      "        \"action_sequence\": \"Figures breathe in harmony, embodying strength and serenity as they unite in purpose.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 165.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, vibrant landscape, the air is thick with humidity, enhancing the colors' intensity. Warriors and sisters stand in a circle, their attire a fusion of ancient armor and flowing fabrics that ripple in the warm breeze. The atmosphere is heavy with anticipation, as whispers of faith drift through the air, creating an aura of unity. The scene oscillates between stillness and gentle movement, reflecting life's ebb and flow.\",\n",
      "        \"action_sequence\": \"Figures breathe together, embodying strength and serenity in a sacred bond.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 173.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, verdant landscape, the humidity saturates the air, making colors vibrant and alive. Warriors and sisters stand poised in a circle, their attire a blend of ancient armor and flowing fabrics that flutter in the warm breeze. The atmosphere is thick with anticipation, as soft whispers of faith float through the air, creating a sense of unity. The scene shifts between moments of stillness and gentle movement, reflecting the ebb and flow of life.\",\n",
      "        \"action_sequence\": \"Figures breathe in unison, embodying strength and serenity as they unite in purpose.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 181.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, vibrant landscape, the air is thick with humidity, enhancing the colors' brilliance. Warriors and sisters form a circle, their attire a mix of ancient armor and flowing fabrics that dance in the warm breeze. The atmosphere is heavy with anticipation, as whispers of faith echo softly, creating an aura of unity and strength. The scene flows between stillness and gentle movement, mirroring the rhythm of life.\",\n",
      "        \"action_sequence\": \"Figures stand resolute, breathing together in a harmonious rhythm, embodying the essence of strength and unity.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 189.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, verdant setting, humidity thickens the air, making colors pop with vibrancy. Warriors and sisters stand poised in a circle, their attire blending ancient armor with flowing fabrics that flutter in the gentle breeze. The mood is charged with anticipation, as soft whispers of faith fill the air, creating a sense of unity. The scene ebbs and flows, capturing the essence of life\\u2019s rhythm.\",\n",
      "        \"action_sequence\": \"Figures breathe in harmony, embodying strength and serenity as they unite in purpose.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 197.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, vibrant landscape, the air is thick with humidity, enhancing the colors' intensity. Warriors and sisters stand in a circle, their attire a fusion of ancient armor and flowing fabrics that ripple in the warm breeze. The atmosphere is heavy with anticipation, as whispers of faith drift through the air, creating an aura of unity. The scene oscillates between stillness and gentle movement, reflecting life's ebb and flow.\",\n",
      "        \"action_sequence\": \"Figures breathe together, embodying strength and serenity in a sacred bond.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 205.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, verdant landscape, the humidity saturates the air, making colors vibrant and alive. Warriors and sisters stand poised in a circle, their attire a blend of ancient armor and flowing fabrics that flutter in the warm breeze. The atmosphere is thick with anticipation, as soft whispers of faith float through the air, creating a sense of unity. The scene shifts between moments of stillness and gentle movement, reflecting the ebb and flow of life.\",\n",
      "        \"action_sequence\": \"Figures breathe in unison, embodying strength and serenity as they unite in purpose.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 213.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, vibrant landscape, the air is thick with humidity, enhancing the colors' brilliance. Warriors and sisters form a circle, their attire a mix of ancient armor and flowing fabrics that dance in the warm breeze. The atmosphere is heavy with anticipation, as whispers of faith echo softly, creating an aura of unity and strength. The scene flows between stillness and gentle movement, mirroring the rhythm of life.\",\n",
      "        \"action_sequence\": \"Figures stand resolute, breathing together in a harmonious rhythm, embodying the essence of strength and unity.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 221.38,\n",
      "        \"text\": \"Hot times are weak, Gabrielle's a hush\\nFaith not a weapon, it's a crutch\\nIn thehus warriors and sisters\\nIn the humid and humid\\nSpring, fast, fast, slow\",\n",
      "        \"scene_description\": \"In a lush, verdant setting, humidity thickens the air, making colors pop with vibrancy. Warriors and sisters stand poised in a circle, their attire blending ancient armor with flowing fabrics that flutter in the gentle breeze. The mood is charged with anticipation, as soft whispers of faith fill the air, creating a sense of unity. The scene ebbs and flows, capturing the essence of life\\u2019s rhythm.\",\n",
      "        \"action_sequence\": \"Figures breathe in harmony, embodying strength and serenity as they unite in purpose.\"\n",
      "    }\n",
      "]\n",
      "last_end_value: 233.35999999999999 timestamp: 20250209_084711\n",
      "Load Flux pipeline and generate images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6fe60aad164c8c93b47280cfd40b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f0c51ffb7b4147898b22a361d7e35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6fa08183b94e6caf3fd67100fb56d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (113 > 77). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec6ce94abfb44a39aae2085db1b5905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ddb43ea1d74138bff0528d32f45ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a3fe7733de4729812dc4908a17eb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbc32e9feac4c4ba3d56630aeed0fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa4e5b6c179471f9c0b79b7a2c25073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ce9fae78aa45c4bc10bed7499b32db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b50f90dee343baa2be35cac2dd0c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74197c304b704f80ac84868b2762f797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ab2f1300514b6a94fccfd7e8f92597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13aa3ae6cf243fda70f6d43feb7f67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5e3704aff74dca8aaf8da1c87114a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e59f271c54343bc8921418896daca82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931490272ec644b3a1e5cc0f60c8975a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21423ec9b429409f944f4345cef39f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94861328eb0f4fc1b0fe1d0b0c4be3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e1239207a6449abc1a7c641a2a4981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0996d4a6754cf3b301059ea4e8b0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86e9c5320f94d79a67ec5ff589ee18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27aeb87e3c2a468096597ffff89d60fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe2b80852ec429e8accd3989b2ac894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3456ade8814ce1bafdbd6beaf5adf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8588cf76a9fe478caed6f57699c8c3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075855d9ee324954a295524e8d987b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc62aab5e87441f939c7f188f8982c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f4adcf05e14c26b1a827dc5352f16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Video Pipeline\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6f69015e40493aafde88cb563be5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b9823a792e441e8fa6b66b63123f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d42e4145ee46d0bfcb6f9753dbd9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8c5f2fdf3d4c6894f20f898735bc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cee74edd9924dee8b67d3f9a71808b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250209_084711/temp_image.jpg'.\n",
      "Scene 2 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71270928597248b0bf72592ea971b516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250209_084711/temp_image.jpg'.\n",
      "Scene 3 has 2 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb1e27365d24143b5d2891870846d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250209_084711/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff769e196664dd98880f538a99981d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250209_084711/temp_image.jpg'.\n",
      "Scene 4 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91cce9081584d92bd828880900d3e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250209_084711/temp_image.jpg'.\n",
      "Scene 5 has 3 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fc61611a444e7aaff9c534425199a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250209_084711/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807ceccc6f694c93a09456edddbf4666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250209_084711/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbb0368817044cbbd992baa148f7e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250209_084711/temp_image.jpg'.\n",
      "Scene 6 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3170972659d4a73af0bb3e862282976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/Lightlines_20250209_084711/temp_image.jpg'.\n",
      "Scene 7 has 1 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95591ca35cdc48ee938104b21eeaf58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run new systems\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m audio_file \u001b[38;5;129;01min\u001b[39;00m CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_files\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mcreate_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 579\u001b[0m, in \u001b[0;36mcreate_video\u001b[0;34m()\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_video\u001b[39m():\n\u001b[1;32m    578\u001b[0m     config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp \u001b[38;5;241m=\u001b[39m process_all_audios(audio_file, CONFIG)\n\u001b[0;32m--> 579\u001b[0m     \u001b[43mprocess_audio_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_images_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_videos_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_end_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 546\u001b[0m, in \u001b[0;36mprocess_audio_video\u001b[0;34m(config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp)\u001b[0m\n\u001b[1;32m    544\u001b[0m video_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(video_num)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m video_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(audio_videos_dir, video_name)\n\u001b[0;32m--> 546\u001b[0m \u001b[43mgenerate_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_pipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_output_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Pause for 1 second\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# After generating the video, extract the last frame to use as input for the next segment\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 171\u001b[0m, in \u001b[0;36mgenerate_video\u001b[0;34m(pipe_image, prompt, image_input, config, seed_value, video_filename, num_frames)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mGenerates and saves a video from the provided image and prompt.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSlow movements, slow camera. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prompt\n\u001b[0;32m--> 171\u001b[0m latents, seed \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipe_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m latents\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    182\u001b[0m batch_video_frames \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[4], line 149\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(pipe_image, prompt, image_input, config, num_inference_steps, guidance_scale, seed, num_frames)\u001b[0m\n\u001b[1;32m    146\u001b[0m image_input \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_input)\u001b[38;5;241m.\u001b[39mresize(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m720\u001b[39m, \u001b[38;5;241m480\u001b[39m))  \u001b[38;5;66;03m# Convert to PIL\u001b[39;00m\n\u001b[1;32m    147\u001b[0m image \u001b[38;5;241m=\u001b[39m load_image(image_input)\n\u001b[0;32m--> 149\u001b[0m video_pt \u001b[38;5;241m=\u001b[39m \u001b[43mpipe_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_videos_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_dynamic_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHEIGHT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWIDTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mframes\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m video_pt, seed\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/diffusers/pipelines/cogvideo/pipeline_cogvideox_image2video.py:836\u001b[0m, in \u001b[0;36mCogVideoXImageToVideoPipeline.__call__\u001b[0;34m(self, image, prompt, negative_prompt, height, width, num_frames, num_inference_steps, timesteps, guidance_scale, use_dynamic_cfg, num_videos_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, attention_kwargs, callback_on_step_end, callback_on_step_end_tensor_inputs, max_sequence_length)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_dynamic_cfg:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guidance_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m guidance_scale \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m--> 836\u001b[0m         (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mcos(math\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m ((num_inference_steps \u001b[38;5;241m-\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m num_inference_steps) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5.0\u001b[39m)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    837\u001b[0m     )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance:\n\u001b[1;32m    839\u001b[0m     noise_pred_uncond, noise_pred_text \u001b[38;5;241m=\u001b[39m noise_pred\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run new systems\n",
    "for audio_file in CONFIG[\"audio_files\"]:\n",
    "    create_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f01978-588d-4f2a-910a-68786df670de",
   "metadata": {},
   "source": [
    "### Run previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba626ac1-4c9d-487c-8939-afb86fde6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run saved config\n",
    "scenes_file_path = './images/Lightlines_20250208_215256/scenes.json'\n",
    "audio_images_dir = './images/Lightlines_20250208_215256'\n",
    "audio_videos_dir = './output/Lightlines_20250208_215256'\n",
    "timestamp = '20250208_215256'\n",
    "last_end_value = 220.0\n",
    "\n",
    "with open(scenes_file_path, \"r\") as scenes_file:\n",
    "    scenes = json.load(scenes_file)\n",
    "\n",
    "process_audio_video(CONFIG, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a0750-5e16-4713-8ac3-825cb8713762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee95d39-df0a-49c5-96ca-e5164d3a4785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
