{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f87d72-5305-4778-ba0c-31549333c8db",
   "metadata": {},
   "source": [
    "# Music Video Synthesis\n",
    "* Extract lyrics from song with timestamps\n",
    "* Compose scenes, include timestamps\n",
    "* Construct video text prompt for each scene\n",
    "* Build videos for each scene\n",
    "* Stitch together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b074-e32d-45f2-977f-c923878625e6",
   "metadata": {},
   "source": [
    "# We will use openai whipser for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45c210-fd2b-4381-9fd3-c1eb18feefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet --upgrade pip\n",
    "#!pip3 install torch==2.4.1 torchvision torchaudio optimum-quanto torchao xformers --index-url https://download.pytorch.org/whl/cu124\n",
    "#!pip install --quiet --upgrade openai-whisper openai\n",
    "# Ubuntu or Debian\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!pip install setuptools-rust\n",
    "#!pip install -U diffusers imageio imageio_ffmpeg opencv-python moviepy transformers huggingface-hub optimum pillow safetensors\n",
    "#!pip install git+https://github.com/xhinker/sd_embed.git@main\n",
    "#!pip install accelerate flash_attention numba -U\n",
    "#!pip install flash_attn --no-build-isolation\n",
    "#!pip install -r requirements.txt -U\n",
    "#!pip install numpy==1.26.4\n",
    "#!pip install git+https://github.com/zRzRzRzRzRzRzR/diffusers.git@cogvideox1.1-5b -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537e766-eab2-4757-b6f9-fbac4da44930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import diffusers\n",
    "import gc\n",
    "import imageio\n",
    "import imageio_ffmpeg\n",
    "import json\n",
    "import math\n",
    "import moviepy.editor as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import whisper\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from diffusers import AutoencoderKL, AutoencoderKLCogVideoX, AutoPipelineForText2Image, CogVideoXTransformer3DModel, CogVideoXPipeline, CogVideoXDPMScheduler\n",
    "from diffusers import CogVideoXTransformer3DModel, CogVideoXImageToVideoPipeline, FlowMatchEulerDiscreteScheduler\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from diffusers.utils import export_to_video, load_video, load_image\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from model4 import T5EncoderModel as m_T5EncoderModel, FluxTransformer2DModel\n",
    "from numba import cuda\n",
    "from openai import OpenAI\n",
    "from optimum.quanto import freeze, qfloat8, quantize, requantize\n",
    "from PIL import Image\n",
    "from safetensors.torch import load_file as load_safetensors, save_file as save_safetensors\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_flux1\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5TokenizerFast, T5EncoderModel as t_T5EncoderModel\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Define the paths where quantized weights will be saved\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "retry_limit = 3\n",
    "quantization = int8_weight_only\n",
    "\n",
    "WIDTH=1360\n",
    "HEIGHT=768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54ba32-2208-45c8-8ed2-5fcb1e79aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"openai_api_key\": \"\",\n",
    "    \"openai_model\": \"gpt-4o-mini\",\n",
    "    \"openai_model_large\": \"gpt-4o\",\n",
    "    \"hf_token\": \"\",\n",
    "    \"base_working_dir\": \"./images\",\n",
    "    \"base_video_dir\": \"./output\",\n",
    "    \"audio_files\": [\n",
    "        \"//mnt/d/audio/Coustang.mp3\",\n",
    "        \"//mnt/d/audio/Coustang.mp3\",\n",
    "        \"//mnt/d/audio/Coustang.mp3\",\n",
    "    ],\n",
    "    \"device\": device,\n",
    "    \"dtype\": dtype,\n",
    "    \"retry_limit\": retry_limit,\n",
    "    \"MAX_SEED\": MAX_SEED,\n",
    "}\n",
    "\n",
    "# Ensure base directories exist\n",
    "os.makedirs(CONFIG[\"base_working_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"base_video_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34281c1-e231-4472-9d9b-096fa23b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_memory(device):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    \n",
    "def get_openai_prompt_response(\n",
    "    prompt: str,\n",
    "    config: dict,\n",
    "    max_tokens: int = 6000,\n",
    "    temperature: float = 0.33,\n",
    "    openai_model: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response with retry logic.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Act as a helpful assistant, you are an expert editor.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=openai_model or config[\"openai_model\"],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < config[\"retry_limit\"]:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            return message_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == config[\"retry_limit\"]:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{config['retry_limit']})\")\n",
    "                time.sleep(1)  # Optional: wait before retrying\n",
    "\n",
    "\n",
    "def load_flux_pipe():\n",
    "    text_encoder_2: T5EncoderModel = m_T5EncoderModel.from_pretrained(\n",
    "        \"HighCWu/FLUX.1-dev-4bit\",\n",
    "        subfolder=\"text_encoder_2\",\n",
    "        torch_dtype=dtype,\n",
    "    )\n",
    "    \n",
    "    transformer: FluxTransformer2DModel = FluxTransformer2DModel.from_pretrained(\n",
    "        \"HighCWu/FLUX.1-dev-4bit\",\n",
    "        subfolder=\"transformer\",\n",
    "        torch_dtype=dtype,\n",
    "    )\n",
    "    \n",
    "    pipe: FluxPipeline = FluxPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-dev\",\n",
    "        text_encoder_2=text_encoder_2,\n",
    "        transformer=transformer,\n",
    "        torch_dtype=dtype,\n",
    "    )\n",
    "    #pipe.enable_model_cpu_offload() # with cpu offload, it cost 8.5GB vram\n",
    "    pipe.remove_all_hooks()\n",
    "    pipe = pipe.to('cuda') # without cpu offload, it cost 11GB vram\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def gen_flux_image(pipe, prompt, config: dict, height=1024, width=1024, guidance_scale=3.5, num_inference_steps=32, max_sequence_length=512, seed=-1):\n",
    "    \"\"\"\n",
    "    Generates an image based on the provided prompt using the Flux pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds, pooled_prompt_embeds = get_weighted_text_embeddings_flux1(\n",
    "            pipe        = pipe,\n",
    "            prompt    = prompt\n",
    "        )\n",
    "        \n",
    "        image = pipe(\n",
    "            prompt_embeds               = prompt_embeds,\n",
    "            pooled_prompt_embeds      = pooled_prompt_embeds,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            guidance_scale=guidance_scale,\n",
    "            output_type=\"pil\",\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "        ).images[0]\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "def load_video_pipeline():\n",
    "    \"\"\"\n",
    "    Loads and configures the video generation pipeline.\n",
    "    \"\"\"\n",
    "    text_encoder = t_T5EncoderModel.from_pretrained(\"THUDM/CogVideoX1.5-5B-I2V\", subfolder=\"text_encoder\", torch_dtype=dtype)\n",
    "    quantize_(text_encoder, quantization())\n",
    "\n",
    "    transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX1.5-5B-I2V\", subfolder=\"transformer\", torch_dtype=dtype)\n",
    "    quantize_(transformer, quantization())\n",
    "\n",
    "    vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX1.5-5B-I2V\", subfolder=\"vae\", torch_dtype=dtype)\n",
    "    quantize_(vae, quantization())\n",
    "\n",
    "    # Create pipeline and run inference\n",
    "    pipe = CogVideoXImageToVideoPipeline.from_pretrained(\n",
    "        \"THUDM/CogVideoX1.5-5B-I2V\",\n",
    "        text_encoder=text_encoder,\n",
    "        transformer=transformer,\n",
    "        vae=vae,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).to(device)\n",
    "    #pipe.enable_model_cpu_offload()\n",
    "    pipe.vae.enable_tiling()\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def infer(pipe_image, prompt: str, image_input: str, config: dict, num_inference_steps: int = 50, guidance_scale: float = 7.0, seed: int = -1, num_frames: int = 49):\n",
    "    \"\"\"\n",
    "    Generates video frames from an image and prompt using the video pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 255)\n",
    "\n",
    "    image_input = Image.open(image_input).resize(size=(720, 480))  # Convert to PIL\n",
    "    image = load_image(image_input)\n",
    "\n",
    "    video_pt = pipe_image(\n",
    "        image=image,\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        num_videos_per_prompt=1,\n",
    "        use_dynamic_cfg=True,\n",
    "        output_type=\"pt\",\n",
    "        guidance_scale=guidance_scale,\n",
    "        height=HEIGHT,\n",
    "        width=WIDTH,\n",
    "        generator=torch.Generator(device=\"cpu\").manual_seed(seed),\n",
    "        num_frames=num_frames,\n",
    "    ).frames\n",
    "\n",
    "    return video_pt, seed\n",
    "\n",
    "\n",
    "def generate_video(pipe_image, prompt, image_input, config: dict, seed_value: int = -1, video_filename: str = \"\", num_frames: int = 49):\n",
    "    \"\"\"\n",
    "    Generates and saves a video from the provided image and prompt.\n",
    "    \"\"\"\n",
    "    prompt = \"Slow movements, slow camera. \" + prompt\n",
    "    latents, seed = infer(\n",
    "        pipe_image,\n",
    "        prompt,\n",
    "        image_input,\n",
    "        config,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=6.0,\n",
    "        seed=seed_value,\n",
    "        num_frames=num_frames,\n",
    "    )\n",
    "    batch_size = latents.shape[0]\n",
    "    batch_video_frames = []\n",
    "    for batch_idx in range(batch_size):\n",
    "        pt_image = latents[batch_idx]\n",
    "        pt_image = torch.stack([pt_image[i] for i in range(pt_image.shape[0])])\n",
    "        image_np = VaeImageProcessor.pt_to_numpy(pt_image)\n",
    "        image_pil = VaeImageProcessor.numpy_to_pil(image_np)\n",
    "        batch_video_frames.append(image_pil)\n",
    "\n",
    "    video_path = save_video(\n",
    "        batch_video_frames[0],\n",
    "        fps=math.ceil((len(batch_video_frames[0]) - 1) / 6),\n",
    "        filename=video_filename\n",
    "    )\n",
    "    # After processing frames\n",
    "    del latents\n",
    "    del batch_video_frames\n",
    "    reset_memory(device)\n",
    "    return video_path\n",
    "\n",
    "\n",
    "def save_video(frames, fps: int, filename: str):\n",
    "    \"\"\"\n",
    "    Saves a list of frames as a video file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        writer = imageio.get_writer(temp_video_path, fps=fps)\n",
    "        for frame in frames:\n",
    "            writer.append_data(np.array(frame))\n",
    "        writer.close()\n",
    "\n",
    "    os.rename(temp_video_path, filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def convert_to_gif(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a video file to a GIF.\n",
    "    \"\"\"\n",
    "    clip = mp.VideoFileClip(video_path)\n",
    "    clip = clip.set_fps(8)\n",
    "    clip = clip.resize(height=240)\n",
    "    gif_path = video_path.replace(\".mp4\", \".gif\")\n",
    "    clip.write_gif(gif_path, fps=8)\n",
    "    return gif_path\n",
    "\n",
    "\n",
    "def resize_if_unfit(input_video: str) -> str:\n",
    "    \"\"\"\n",
    "    Resizes the video to the target dimensions if it does not match.\n",
    "    \"\"\"\n",
    "    width, height = get_video_dimensions(input_video)\n",
    "\n",
    "    if width == 720 and height == 480:\n",
    "        return input_video\n",
    "    else:\n",
    "        return center_crop_resize(input_video)\n",
    "\n",
    "\n",
    "def get_video_dimensions(input_video_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Retrieves the dimensions of the video.\n",
    "    \"\"\"\n",
    "    reader = imageio_ffmpeg.read_frames(input_video_path)\n",
    "    metadata = next(reader)\n",
    "    return metadata[\"size\"]\n",
    "\n",
    "\n",
    "def center_crop_resize(input_video_path: str, target_width: int = 720, target_height: int = 480) -> str:\n",
    "    \"\"\"\n",
    "    Resizes and center-crops the video to the target dimensions.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    width_factor = target_width / orig_width\n",
    "    height_factor = target_height / orig_height\n",
    "    resize_factor = max(width_factor, height_factor)\n",
    "\n",
    "    inter_width = int(orig_width * resize_factor)\n",
    "    inter_height = int(orig_height * resize_factor)\n",
    "\n",
    "    target_fps = 8\n",
    "    ideal_skip = max(0, math.ceil(orig_fps / target_fps) - 1)\n",
    "    skip = min(5, ideal_skip)  # Cap at 5\n",
    "\n",
    "    while (total_frames / (skip + 1)) < 49 and skip > 0:\n",
    "        skip -= 1\n",
    "\n",
    "    processed_frames = []\n",
    "    frame_count = 0\n",
    "    total_read = 0\n",
    "\n",
    "    while frame_count < 49 and total_read < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if total_read % (skip + 1) == 0:\n",
    "            resized = cv2.resize(frame, (inter_width, inter_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            start_x = (inter_width - target_width) // 2\n",
    "            start_y = (inter_height - target_height) // 2\n",
    "            cropped = resized[start_y:start_y + target_height, start_x:start_x + target_width]\n",
    "\n",
    "            processed_frames.append(cropped)\n",
    "            frame_count += 1\n",
    "\n",
    "        total_read += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(temp_video_path, fourcc, target_fps, (target_width, target_height))\n",
    "\n",
    "        for frame in processed_frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    return temp_video_path\n",
    "\n",
    "\n",
    "def extract_last_frame(video_filename: str, output_image_filename: str):\n",
    "    \"\"\"\n",
    "    Extracts the last frame from a video file and saves it as an image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = imageio.get_reader(video_filename, 'ffmpeg')\n",
    "        last_frame = None\n",
    "        for frame in reader:\n",
    "            last_frame = frame\n",
    "        reader.close()\n",
    "\n",
    "        if last_frame is not None:\n",
    "            imageio.imwrite(output_image_filename, last_frame)\n",
    "            print(f\"Last frame saved successfully as '{output_image_filename}'.\")\n",
    "        else:\n",
    "            print(\"The video contains no frames.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{video_filename}' was not found.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except RuntimeError as re:\n",
    "        print(f\"RuntimeError: {re}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def create_scenes(text: str, video_summary: str, config: dict):\n",
    "    \"\"\"\n",
    "    Creates scenes based on the extracted lyrics using OpenAI's API.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Create a json list of diverse, unique scenes (groupings of text), scene_description (200 words or less), and action_sequence (30 words or less) from the following text.  Scenes should be groups of lyrics with new scenes when the lyric context changes.  Text: {text}   \n",
    "The json list should have the start value for the first item in the scene and the text that is combined for all items in the same scene.  \n",
    "The scene_description should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes.  Use theme descriptions, such as graphic novel, photograph, oil painting, etc.\n",
    "The action_sequence should describe the action in the scene.  Scenes should be unique, creative, imaginative, and awe-inspiring to create an amazing video.  Create beautiful and mesmerizing scene descriptions that are creative, unique, artistic, and imaginative. Each scene must be unique, imaginative, and visually captivating, blending creativity with artistic flair. Use powerful, descriptive language to craft scenes that are awe-inspiring and leave the audience in wonder. These scenes should evoke a sense of beauty, grandeur, mystery, or anything emotional, drawing from both realistic and fantastical elements. Ensure the descriptions are immersive, emotionally resonant, and filled with unexpected twists that engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  Action can't have sudden movements or fast zooms, camera movement.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.66)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "def revise_scenes(scenes, config: dict):\n",
    "    \"\"\"\n",
    "    Revise scenes based on the extracted scenes.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Revise the JSON scenes to update the scene_description and action_sequence to engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  JSON scenes: {scenes}   \n",
    "The scene_description (200 words or less) should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes. Use theme descriptions, such as graphic novel, photograph, oil painting, etc.\n",
    "The action_sequence (30 words or less) should describe the action in the scene.  The goal is to create input to create a stunning, cinematic video experience.   Action can't have sudden movements or fast zooms, camera movement.\n",
    "Only update the scene_description and action_sequence.  Do not delete any items as having scenes with the given start times are important.  We do not want to have the same scene_description and action_sequence for the items with repeatitive input text.  Please change these to be creative and consistent with dynamic video sequences.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.33)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def process_audio_scenes(audio_file: str, config: dict):\n",
    "    # set maximum duration for an image basis, should be in intervals of video generation length\n",
    "    video_gen_length = 6\n",
    "    max_duration_seconds  = video_gen_length * 3\n",
    "    \"\"\"\n",
    "    Processes a single audio file through the entire workflow.\n",
    "    \"\"\"\n",
    "    # Create unique identifier based on audio file name\n",
    "    audio_basename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = f\"{audio_basename}_{timestamp}\"\n",
    "\n",
    "    # Create unique directories for images and videos\n",
    "    print(f\"Create unique directories for images and videos\")\n",
    "    audio_images_dir = os.path.join(config[\"base_working_dir\"], unique_id)\n",
    "    audio_videos_dir = os.path.join(config[\"base_video_dir\"], unique_id)\n",
    "    os.makedirs(audio_images_dir, exist_ok=True)\n",
    "    os.makedirs(audio_videos_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Transcribe audio using Whisper\n",
    "    print(f\"Transcribe audio using Whisper\")\n",
    "    model = whisper.load_model(\"turbo\")\n",
    "    result = model.transcribe(audio_file)\n",
    "\n",
    "    # Cleanup Whisper model memory\n",
    "    del model\n",
    "    reset_memory(device)\n",
    "\n",
    "    segments = result['segments']\n",
    "\n",
    "    # Extract list of start times and texts\n",
    "    segment_texts_and_start_times = [(segment['text'].strip(), segment['start']) for segment in segments]\n",
    "\n",
    "    # Combine texts\n",
    "    text = \"\"\n",
    "    for segment_text, start in segment_texts_and_start_times:\n",
    "        text += f\"Start: {start}, Text: {segment_text}\\n\"\n",
    "\n",
    "    last_end_value = segments[-1]['end']\n",
    "\n",
    "    # Path to scenes.json file\n",
    "    scenes_file_path = os.path.join(audio_images_dir, \"scenes.json\")\n",
    "\n",
    "    # Check if scenes.json exists\n",
    "    if os.path.exists(scenes_file_path):\n",
    "        print(f\"Scenes file already exists at {scenes_file_path}. Skipping scene generation.\")\n",
    "        with open(scenes_file_path, \"r\") as scenes_file:\n",
    "            scenes = json.load(scenes_file)\n",
    "        return scenes, audio_images_dir, audio_videos_dir, last_end_value\n",
    "\n",
    "    # Step 2: Generate video summary using OpenAI\n",
    "    print(f\"Generate video summary using OpenAI\")\n",
    "    video_summary_prompt = f'Create a short summary that describes a music video based on these lyrics: {text}'\n",
    "    video_summary = get_openai_prompt_response(video_summary_prompt, config, openai_model=config[\"openai_model\"])\n",
    "\n",
    "    # Step 3: Create scenes based on lyrics\n",
    "    print(f\"Create scenes based on lyrics\")\n",
    "    try:\n",
    "        scenes = create_scenes(text, video_summary, config)\n",
    "    except:\n",
    "        try:\n",
    "            scenes = create_scenes(text, video_summary, config)\n",
    "        except:\n",
    "            try:\n",
    "                scenes = create_scenes(text, video_summary, config)\n",
    "            except: \n",
    "                return \"\", audio_images_dir, audio_videos_dir, last_end_value\n",
    "            \n",
    "    # we don't want scenes longer than 18 seconds\n",
    "    new_scenes = []\n",
    "    for i in range(len(scenes)):\n",
    "        scene = scenes[i]\n",
    "        if i == 0:\n",
    "            start_time = 0\n",
    "        else:\n",
    "            start_time = scene['start']\n",
    "        # Determine the end time\n",
    "        if i < len(scenes) - 1:\n",
    "            end_time = scenes[i + 1]['start']\n",
    "        else:\n",
    "            end_time = last_end_value\n",
    "        duration = end_time - start_time\n",
    "        # Split the scene if duration exceeds 18 seconds\n",
    "        while duration > 18:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "            start_time += max_duration_seconds\n",
    "            duration = end_time - start_time\n",
    "        # Append the remaining part of the scene\n",
    "        if duration > 0:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "    # Replace the original scenes with the new list\n",
    "    scenes = new_scenes\n",
    "    # improve the scenes with a revision\n",
    "    try:\n",
    "        scenes_revised = revise_scenes(scenes, config)\n",
    "        scenes = scenes_revised\n",
    "        print(f'revised scenes')\n",
    "    except:\n",
    "        try:\n",
    "            scenes_revised = revise_scenes(scenes, config)\n",
    "            scenes = scenes_revised\n",
    "            print(f'revised scenes')\n",
    "        except:\n",
    "            print('cannot revise scenes')\n",
    "            \n",
    "    \n",
    "    # Save the scenes to scenes.json\n",
    "    with open(scenes_file_path, \"w\") as scenes_file:\n",
    "        json.dump(scenes, scenes_file)\n",
    "        \n",
    "    return scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def process_audio_images(config: dict, scenes, audio_images_dir):\n",
    "    # Step 4: Load Flux pipeline and generate images\n",
    "    print(f\"Load Flux pipeline and generate images\")\n",
    "    flux_pipe = load_flux_pipe()\n",
    "    height = HEIGHT\n",
    "    width = WIDTH\n",
    "    guidance_scale = 3.9\n",
    "    num_inference_steps = 48\n",
    "    max_sequence_length = 512\n",
    "    seed = -1\n",
    "\n",
    "    # Generate images for each scene\n",
    "    image_num = 1\n",
    "    for scene in scenes:\n",
    "        image_prompt = scene['scene_description']\n",
    "        image = gen_flux_image(flux_pipe, image_prompt, config, height, width, guidance_scale, num_inference_steps, max_sequence_length, seed)\n",
    "        filename = f\"image_{str(image_num).zfill(2)}.jpg\"\n",
    "        image_path = os.path.join(audio_images_dir, filename)\n",
    "        image.save(image_path, dpi=(300, 300))\n",
    "        image_num += 1\n",
    "\n",
    "    # Move the pipeline back to CPU and delete it\n",
    "    flux_pipe.to('cpu')\n",
    "    del flux_pipe\n",
    "    reset_memory(device)\n",
    "    return\n",
    "\n",
    "def process_audio_video(config: dict, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp):\n",
    "    # Step 6: Load Video Pipeline\n",
    "    print(f\"Load Video Pipeline\")\n",
    "    video_pipe = load_video_pipeline()\n",
    "\n",
    "    # Temporary image path\n",
    "    temp_image = os.path.join(audio_images_dir, \"temp_image.jpg\")\n",
    "    video_num = 1\n",
    "\n",
    "    # Step 7: Generate video sequences\n",
    "    for i, scene in enumerate(scenes):\n",
    "        prompt = scene[\"action_sequence\"]\n",
    "\n",
    "        # Use the initial image for each scene\n",
    "        image_input = os.path.join(audio_images_dir, f\"image_{str(i+1).zfill(2)}.jpg\")\n",
    "\n",
    "        # Calculate duration to keep the video in 6-second increments\n",
    "        if i + 1 < len(scenes):\n",
    "            next_start_time = scenes[i + 1][\"start\"]\n",
    "        else:\n",
    "            next_start_time = last_end_value  # Use the final ending time for the last scene\n",
    "\n",
    "        if i == 0:\n",
    "            duration = next_start_time\n",
    "        else:\n",
    "            duration = next_start_time - scene[\"start\"]\n",
    "        num_video_segments = int((duration + 3) // 6)\n",
    "\n",
    "        print(f\"Scene {i+1} has {num_video_segments} segments\")\n",
    "        for j in range(num_video_segments):\n",
    "            video_name = f\"video_{str(video_num).zfill(2)}_{str(i+1)}_{str(j+1).zfill(2)}_{timestamp}.mp4\"\n",
    "            video_output_path = os.path.join(audio_videos_dir, video_name)\n",
    "            generate_video(video_pipe, prompt, image_input, config, seed_value=-1, video_filename=video_output_path)\n",
    "            time.sleep(1)  # Pause for 1 second\n",
    "\n",
    "            # After generating the video, extract the last frame to use as input for the next segment\n",
    "            extract_last_frame(video_output_path, temp_image)\n",
    "\n",
    "            # Use the last frame as input for the next video segment in the same scene\n",
    "            image_input = temp_image\n",
    "\n",
    "            video_num += 1  # Increment video number for the next segment\n",
    "\n",
    "    # Move the pipeline back to CPU before deleting\n",
    "    video_pipe.to('cpu')\n",
    "    del video_pipe\n",
    "    reset_memory(device)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def process_all_audios(audio_file, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a list of audio files through the workflow.\n",
    "    \"\"\"\n",
    "    print(f\"Processing audio file: {audio_file}\")\n",
    "    scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_audio_scenes(audio_file, config)\n",
    "    print(f'{len(scenes)} scenes:\\n{json.dumps(scenes, indent=4)}')\n",
    "    print(f'last_end_value: {last_end_value} timestamp: {timestamp}')\n",
    "    # Create starting images for scenes\n",
    "    process_audio_images(config, scenes, audio_images_dir)\n",
    "    return config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def create_video():\n",
    "    config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_all_audios(audio_file, CONFIG)\n",
    "    process_audio_video(config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp)\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efd2eb-8304-4027-a8c3-eaafe04ade14",
   "metadata": {},
   "source": [
    "### Run new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0cc96-e609-4cd2-9c67-dcd63562988a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run new systems\n",
    "for audio_file in CONFIG[\"audio_files\"]:\n",
    "    create_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f01978-588d-4f2a-910a-68786df670de",
   "metadata": {},
   "source": [
    "### Run previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba626ac1-4c9d-487c-8939-afb86fde6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run saved config\n",
    "scenes_file_path = './images/AlphabetJoy_20241117_152536/scenes.json'\n",
    "audio_images_dir = './images/AlphabetJoy_20241117_152536'\n",
    "audio_videos_dir = './output/AlphabetJoy_20241117_152536'\n",
    "timestamp = '20241117_152536'\n",
    "last_end_value = 199.4\n",
    "\n",
    "with open(scenes_file_path, \"r\") as scenes_file:\n",
    "    scenes = json.load(scenes_file)\n",
    "\n",
    "process_audio_video(CONFIG, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a0750-5e16-4713-8ac3-825cb8713762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee95d39-df0a-49c5-96ca-e5164d3a4785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
