{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f87d72-5305-4778-ba0c-31549333c8db",
   "metadata": {},
   "source": [
    "# Music Video Synthesis\n",
    "* Extract lyrics from song with timestamps\n",
    "* Compose scenes, include timestamps\n",
    "* Generate images for each scene\n",
    "* A human should evalute photos and scenes, creating a curated one with the desired characteristics\n",
    "* Construct video text prompt for each scene\n",
    "* Build videos for each scene, use referall link to sign up: https://www.segmind.com/invite/773118b7-41f4-4154-87f4-49326d973ec3\n",
    "* Stitch together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b074-e32d-45f2-977f-c923878625e6",
   "metadata": {},
   "source": [
    "# We will use openai whipser for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45c210-fd2b-4381-9fd3-c1eb18feefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet --upgrade pip\n",
    "#!pip3 install torch==2.4 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "#!pip install --quiet --upgrade openai-whisper openai\n",
    "# Ubuntu or Debian\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!pip install setuptools-rust\n",
    "#!pip install -U diffusers imageio imageio_ffmpeg opencv-python moviepy transformers huggingface-hub optimum pillow safetensors optimum-quanto accelerate\n",
    "#!pip install --upgrade optimum-quanto torchao --extra-index-url https://download.pytorch.org/whl/cu124 # full options are cpu/cu118/cu121/cu124\n",
    "#!pip install git+https://github.com/xhinker/sd_embed.git@main\n",
    "#!pip install accelerate flash_attention numba -U\n",
    "#!pip install flash_attn --no-build-isolation\n",
    "#!pip install -r requirements.txt -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8537e766-eab2-4757-b6f9-fbac4da44930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 21:14:37.988686: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-19 21:14:37.997861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737342878.009141   22277 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737342878.012542   22277 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-19 21:14:38.024871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import base64\n",
    "import cv2\n",
    "import diffusers\n",
    "import gc\n",
    "import imageio\n",
    "import imageio_ffmpeg\n",
    "import json\n",
    "import math\n",
    "import moviepy as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import psutil\n",
    "import random\n",
    "import requests\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import whisper\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from diffusers import AutoencoderKL, AutoPipelineForText2Image\n",
    "from diffusers import FlowMatchEulerDiscreteScheduler\n",
    "from diffusers import EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, DPMSolverMultistepScheduler, PNDMScheduler, DDIMScheduler\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel\n",
    "from diffusers.utils import export_to_video, load_video, load_image\n",
    "from hyvideo.utils.file_utils import save_videos_grid\n",
    "from hyvideo.config import parse_args\n",
    "from hyvideo.inference import HunyuanVideoSampler\n",
    "from hyvideo.constants import NEGATIVE_PROMPT\n",
    "from mmgp import offload, profile_type\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from mmgp import offload, profile_type\n",
    "from numba import cuda\n",
    "from openai import OpenAI\n",
    "from optimum.quanto import freeze, qfloat8, quantize, requantize\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from safetensors.torch import load_file as load_safetensors, save_file as save_safetensors\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_flux1\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5TokenizerFast, T5EncoderModel\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Define the paths where quantized weights will be saved\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "retry_limit = 3\n",
    "quantization = int8_weight_only\n",
    "\n",
    "WIDTH = 848\n",
    "HEIGHT = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f54ba32-2208-45c8-8ed2-5fcb1e79aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"openai_api_key\": \"\",\n",
    "    \"openai_model\": \"gpt-4o-mini\",\n",
    "    \"openai_model_large\": \"gpt-4o\",\n",
    "    \"hf_token\": \"\",\n",
    "    \"base_working_dir\": \"./images\",\n",
    "    \"base_video_dir\": \"./output\",\n",
    "    \"audio_files\": [\n",
    "        \"/mnt/d/Share/Audio/DreamingTogetherness.mp3\",\n",
    "        \"/mnt/d/Share/Audio/DreamingTogetherness.mp3\",\n",
    "        \"/mnt/d/Share/Audio/DreamingTogetherness.mp3\",\n",
    "        # Add more audio file paths here\n",
    "    ],\n",
    "    \"device\": device,\n",
    "    \"dtype\": dtype,\n",
    "    \"retry_limit\": retry_limit,\n",
    "    \"MAX_SEED\": MAX_SEED,\n",
    "    \"segmind_key\": \"\"\n",
    "}\n",
    "\n",
    "# Ensure base directories exist\n",
    "os.makedirs(CONFIG[\"base_working_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"base_video_dir\"], exist_ok=True)\n",
    "\n",
    "api_key = CONFIG[\"segmind_key\"]\n",
    "url = \"https://api.segmind.com/v1/hunyuan-video\"\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    quantize_transformer=False,\n",
    "    lora_weight=[],\n",
    "    lora_multiplier=[],\n",
    "    profile=-1,\n",
    "    verbose=1,\n",
    "    server_port=0,\n",
    "    server_name='',\n",
    "    open_browser=False,\n",
    "    model='HYVideo-T/2-cfgdistill',\n",
    "    latent_channels=16,\n",
    "    precision='bf16',\n",
    "    rope_theta=256,\n",
    "    vae='884-16c-hy',\n",
    "    vae_precision='fp16',\n",
    "    vae_tiling=True,\n",
    "    text_encoder='llm',\n",
    "    text_encoder_precision='fp16',\n",
    "    text_states_dim=4096,\n",
    "    text_len=256,\n",
    "    tokenizer='llm',\n",
    "    prompt_template='dit-llm-encode',\n",
    "    prompt_template_video='dit-llm-encode-video',\n",
    "    hidden_state_skip_layer=2,\n",
    "    apply_final_norm=False,\n",
    "    text_encoder_2='clipL',\n",
    "    text_encoder_precision_2='fp16',\n",
    "    text_states_dim_2=768,\n",
    "    tokenizer_2='clipL',\n",
    "    text_len_2=77,\n",
    "    denoise_type='flow',\n",
    "    flow_shift=7.0,\n",
    "    flow_reverse=True,\n",
    "    flow_solver='euler',\n",
    "    use_linear_quadratic_schedule=False,\n",
    "    linear_schedule_end=25,\n",
    "    model_base='ckpts',\n",
    "    dit_weight='ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt',\n",
    "    model_resolution='540p',\n",
    "    load_key='module',\n",
    "    use_cpu_offload=False,\n",
    "    batch_size=1,\n",
    "    infer_steps=50,\n",
    "    disable_autocast=False,\n",
    "    save_path='./results',\n",
    "    save_path_suffix='',\n",
    "    name_suffix='',\n",
    "    num_videos=1,\n",
    "    video_size=(720, 1280),\n",
    "    video_length=129,\n",
    "    prompt=None,\n",
    "    seed_type='auto',\n",
    "    seed=None,\n",
    "    neg_prompt=None,\n",
    "    cfg_scale=1.0,\n",
    "    embedded_cfg_scale=6.0,\n",
    "    reproduce=False,\n",
    "    ulysses_degree=1,\n",
    "    ring_degree=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c34281c1-e231-4472-9d9b-096fa23b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplerArgs:\n",
    "    \"\"\"\n",
    "    Minimal container for sampler-related settings.\n",
    "    Extend this if you need additional fields that HunyuanVideoSampler\n",
    "    or your pipeline expects.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Set to True if you want flow reversal in the pipeline\n",
    "        self.flow_reverse = True    \n",
    "def reset_memory(device):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "def get_openai_prompt_response(\n",
    "    prompt: str,\n",
    "    config: dict,\n",
    "    max_tokens: int = 6000,\n",
    "    temperature: float = 0.33,\n",
    "    openai_model: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response with retry logic.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Act as a helpful assistant, you are an expert editor.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=openai_model or config[\"openai_model\"],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < config[\"retry_limit\"]:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            return message_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == config[\"retry_limit\"]:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{config['retry_limit']})\")\n",
    "                time.sleep(1)  # Optional: wait before retrying\n",
    "\n",
    "\n",
    "def load_flux_pipe():\n",
    "    bfl_repo = \"black-forest-labs/FLUX.1-dev\"\n",
    "    revision = \"refs/pr/3\"\n",
    "    adapter_id = \"alimama-creative/FLUX.1-Turbo-Alpha\"\n",
    "\n",
    "    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(bfl_repo, subfolder=\"scheduler\", revision=revision)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "    text_encoder_2 = T5EncoderModel.from_pretrained(bfl_repo, subfolder=\"text_encoder_2\", torch_dtype=dtype, revision=revision)\n",
    "    tokenizer_2 = T5TokenizerFast.from_pretrained(bfl_repo, subfolder=\"tokenizer_2\", torch_dtype=dtype, revision=revision)\n",
    "    vae = AutoencoderKL.from_pretrained(bfl_repo, subfolder=\"vae\", torch_dtype=dtype, revision=revision)\n",
    "    transformer = FluxTransformer2DModel.from_pretrained(bfl_repo, subfolder=\"transformer\", torch_dtype=dtype, revision=revision)\n",
    "    \n",
    "    quantize_(transformer, quantization())\n",
    "    quantize_(text_encoder_2, quantization())\n",
    "    pipe = FluxPipeline(\n",
    "        scheduler=scheduler,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        text_encoder_2=text_encoder_2,\n",
    "        tokenizer_2=tokenizer_2,\n",
    "        vae=vae,\n",
    "        transformer=transformer,\n",
    "    )\n",
    "\n",
    "    pipe = pipe.to('cuda')\n",
    "    pipe.load_lora_weights(adapter_id)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def gen_flux_image(pipe, prompt, config: dict, height=1024, width=1024, guidance_scale=3.5, num_inference_steps=8, max_sequence_length=512, seed=-1):\n",
    "    \"\"\"\n",
    "    Generates an image based on the provided prompt using the Flux pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds, pooled_prompt_embeds = get_weighted_text_embeddings_flux1(\n",
    "            pipe        = pipe,\n",
    "            prompt    = prompt\n",
    "        )\n",
    "        \n",
    "        image = pipe(\n",
    "            prompt_embeds               = prompt_embeds,\n",
    "            pooled_prompt_embeds      = pooled_prompt_embeds,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            guidance_scale=guidance_scale,\n",
    "            output_type=\"pil\",\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "        ).images[0]\n",
    "\n",
    "        # Delete variables\n",
    "        del prompt_embeds\n",
    "        del pooled_prompt_embeds\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "def image_file_to_base64(image_path):\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image_data = f.read()\n",
    "    return base64.b64encode(image_data).decode('utf-8')\n",
    "\n",
    "# Use this function to fetch an image from a URL and convert it to base64\n",
    "def image_url_to_base64(image_url):\n",
    "    response = requests.get(image_url)\n",
    "    image_data = response.content\n",
    "    return base64.b64encode(image_data).decode('utf-8')\n",
    "    \n",
    "def load_hunyuan_video_sampler(\n",
    "    server_config_filename: str = \"gradio_config.json\",\n",
    "    forced_profile_no: int = -1,\n",
    "    verbose_level: int = 1,\n",
    "    quantize_transformer: bool = True,\n",
    "    lora_weight: list = None,\n",
    "    lora_multiplier: list = None,\n",
    "    device: str = \"cpu\",\n",
    ") -> HunyuanVideoSampler:\n",
    "    \"\"\"\n",
    "    Loads the HunyuanVideo pipeline according to settings in `server_config_filename`.\n",
    "    If `forced_profile_no` is >= 0, that overrides the 'profile' field in the server config.\n",
    "    This version does NOT parse any command-line arguments.\n",
    "    \"\"\"\n",
    "    if lora_weight is None:\n",
    "        lora_weight = []\n",
    "    if lora_multiplier is None:\n",
    "        lora_multiplier = []\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Read or create server_config\n",
    "    # -----------------------------------------------------------------------\n",
    "    if not Path(server_config_filename).is_file():\n",
    "        # Default config if none present\n",
    "        server_config = {\n",
    "            \"attention_mode\": \"sage\",\n",
    "            \"transformer_filename\": \"ckpts/hunyuan-video-t2v-720p/transformers/hunyuan_video_720_quanto_int8.safetensors\",\n",
    "            \"text_encoder_filename\": \"ckpts/text_encoder/llava-llama-3-8b-v1_1_quanto_int8.safetensors\",\n",
    "            \"compile\": \"\",\n",
    "            \"profile\": profile_type.HighRAM_LowVRAM,\n",
    "        }\n",
    "        with open(server_config_filename, \"w\", encoding=\"utf-8\") as writer:\n",
    "            writer.write(json.dumps(server_config))\n",
    "    else:\n",
    "        with open(server_config_filename, \"r\", encoding=\"utf-8\") as reader:\n",
    "            text = reader.read()\n",
    "        server_config = json.loads(text)\n",
    "\n",
    "    # Pull out config\n",
    "    transformer_filename = server_config[\"transformer_filename\"]\n",
    "    text_encoder_filename = server_config[\"text_encoder_filename\"]\n",
    "    attention_mode = server_config[\"attention_mode\"]\n",
    "    profile = forced_profile_no if forced_profile_no >= 0 else server_config[\"profile\"]\n",
    "    compile_mode = server_config.get(\"compile\", \"\")\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Download any missing models from HF or any other source (if needed)\n",
    "    # -----------------------------------------------------------------------\n",
    "    def download_models(transformer_filename, text_encoder_filename):\n",
    "        \"\"\"\n",
    "        Stub: Implement your huggingface_hub logic here if needed.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    download_models(transformer_filename, text_encoder_filename)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Optional: tweak VAE config, etc.\n",
    "    # -----------------------------------------------------------------------\n",
    "    offload.default_verboseLevel = verbose_level\n",
    "\n",
    "    vae_config_path = \"./ckpts/hunyuan-video-t2v-720p/vae/config.json\"\n",
    "    if os.path.isfile(vae_config_path):\n",
    "        with open(vae_config_path, \"r\", encoding=\"utf-8\") as reader:\n",
    "            vae_config = json.loads(reader.read())\n",
    "        # Example: reduce time window used by the VAE for temporal splitting\n",
    "        if vae_config.get(\"sample_tsize\", 64) == 64:\n",
    "            vae_config[\"sample_tsize\"] = 32\n",
    "        with open(vae_config_path, \"w\", encoding=\"utf-8\") as writer:\n",
    "            writer.write(json.dumps(vae_config))\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Decide how to pin memory, partial pin, etc. \n",
    "    # -----------------------------------------------------------------------\n",
    "    if profile == 5:\n",
    "        pinToMemory = False\n",
    "        partialPinning = False\n",
    "    else:\n",
    "        pinToMemory = True\n",
    "        physical_memory = psutil.virtual_memory().total\n",
    "        # E.g. partial pin if <= 32 GB of RAM\n",
    "        partialPinning = physical_memory <= (2**30) * 32\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Load the pipeline\n",
    "    # -----------------------------------------------------------------------\n",
    "    hunyuan_video_sampler = HunyuanVideoSampler.from_pretrained(\n",
    "        transformer_filename,\n",
    "        text_encoder_filename,\n",
    "        attention_mode=attention_mode,\n",
    "        pinToMemory=pinToMemory,\n",
    "        partialPinning=partialPinning,\n",
    "        args=args,      # passes our simple SamplerArgs object\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    pipe = hunyuan_video_sampler.pipeline\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Optionally load LoRAs\n",
    "    # -----------------------------------------------------------------------\n",
    "    if len(lora_weight) > 0:\n",
    "        offload.load_loras_into_model(pipe.transformer, lora_weight, lora_multiplier)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Profile, compile, or quantize\n",
    "    # -----------------------------------------------------------------------\n",
    "    offload.profile(\n",
    "        pipe,\n",
    "        profile_no=profile,\n",
    "        compile=compile_mode,\n",
    "        quantizeTransformer=quantize_transformer,\n",
    "    )\n",
    "\n",
    "    return hunyuan_video_sampler\n",
    "\n",
    "\n",
    "def generate_video(\n",
    "    hunyuan_video_sampler,\n",
    "    height=HEIGHT,\n",
    "    width=WIDTH,\n",
    "    video_length=121,\n",
    "    infer_steps=50,\n",
    "    prompt=\"A cat walks on the grass, realistic style.\",\n",
    "    negative_prompt=\"Aerial view, overexposed, low quality, deformation\",\n",
    "    flow_shift=7.0,\n",
    "    filename=\"./output.mp4\",\n",
    "    seed=42,\n",
    "    cfg_scale=7.5,\n",
    "    batch_size=1,\n",
    "    embedded_cfg_scale=1.0,\n",
    "):\n",
    "\n",
    "       # TeaCache\n",
    "    trans = hunyuan_video_sampler.pipeline.transformer.__class__\n",
    "    trans.enable_teacache = False\n",
    "    if trans.enable_teacache:\n",
    "        trans.num_steps = num_inference_steps\n",
    "        trans.cnt = 0\n",
    "        trans.rel_l1_thresh = 0.15 # 0.1 for 1.6x speedup, 0.15 for 2.1x speedup\n",
    "        trans.accumulated_rel_l1_distance = 0\n",
    "        trans.previous_modulated_input = None\n",
    "        trans.previous_residual = None\n",
    "        \n",
    "    \"\"\"\n",
    "    Generates and saves a video using the provided sampler, based on the specified parameters.\n",
    "    The result is written to 'filename'.\n",
    "    \"\"\"\n",
    "    outputs = hunyuan_video_sampler.predict(\n",
    "        prompt=prompt,\n",
    "        height=480,\n",
    "        width=848,\n",
    "        video_length=121,\n",
    "        seed=seed,\n",
    "        negative_prompt=negative_prompt,\n",
    "        infer_steps=50,\n",
    "        guidance_scale=1.0,\n",
    "        num_videos_per_prompt=1,\n",
    "        flow_shift=7.0,\n",
    "        batch_size=batch_size,\n",
    "        embedded_guidance_scale=6.0,\n",
    "    )\n",
    "\n",
    "    samples = outputs[\"samples\"]\n",
    "    # Assuming one video per prompt:\n",
    "    for i, sample in enumerate(samples):\n",
    "        # shape is (C, T, H, W)\n",
    "        sample = sample.unsqueeze(0)  # (1, C, T, H, W)\n",
    "        save_videos_grid(sample, filename, fps=24)\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "def unload_hunyuan_video_sampler(hunyuan_video_sampler):\n",
    "    \"\"\"\n",
    "    Frees the memory used by the pipeline.\n",
    "    In a normal Python script, deleting references and calling torch.cuda.empty_cache()\n",
    "    is usually enough.\n",
    "    \"\"\"\n",
    "    del hunyuan_video_sampler\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def create_scenes(text: str, video_summary: str, config: dict):\n",
    "    \"\"\"\n",
    "    Creates scenes based on the extracted lyrics using OpenAI's API.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Create a json list of diverse, unique scenes (groupings of text), scene_description (200 words or less), and action_sequence (30 words or less) from the following text.  Scenes should be groups of lyrics with new scenes when the lyric context changes.  Text: {text}   \n",
    "The json list should have the start value for the first item in the scene and the text that is combined for all items in the same scene.  \n",
    "The scene_description should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes.\n",
    "The action_sequence should describe the action in the scene.  Scenes should be unique, creative, imaginative, and awe-inspiring to create an amazing video.  Create beautiful and mesmerizing scene descriptions that are creative, unique, artistic, and imaginative. Each scene must be unique, imaginative, and visually captivating, blending creativity with artistic flair. Use powerful, descriptive language to craft scenes that are awe-inspiring and leave the audience in wonder. These scenes should evoke a sense of beauty, grandeur, mystery, or anything emotional, drawing from both realistic and fantastical elements. Ensure the descriptions are immersive, emotionally resonant, and filled with unexpected twists that engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.66)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "def revise_scenes(scenes, config: dict):\n",
    "    \"\"\"\n",
    "    Revise scenes based on the extracted scenes.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Revise the JSON scenes to update the scene_description and action_sequence to engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  JSON scenes: {scenes}   \n",
    "The scene_description (200 words or less) should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes.\n",
    "The action_sequence (30 words or less) should describe the action in the scene.  The goal is to create input to create a stunning, cinematic video experience.\n",
    "Only update the scene_description and action_sequence.  Do not delete any items as having scenes with the given start times are important.  We do not want to have the same scene_description and action_sequence for the items with repeatitive input text.  Please change these to be creative and consistent with dynamic video sequences.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.33)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def process_audio_scenes(audio_file: str, config: dict):\n",
    "    # set maximum duration for an image basis, should be in intervals of video generation length\n",
    "    video_gen_length = 5\n",
    "    max_duration_seconds  = video_gen_length * 3\n",
    "    \"\"\"\n",
    "    Processes a single audio file through the entire workflow.\n",
    "    \"\"\"\n",
    "    # Create unique identifier based on audio file name\n",
    "    audio_basename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = f\"{audio_basename}_{timestamp}\"\n",
    "\n",
    "    # Create unique directories for images and videos\n",
    "    print(f\"Create unique directories for images and videos\")\n",
    "    audio_images_dir = os.path.join(config[\"base_working_dir\"], unique_id)\n",
    "    audio_videos_dir = os.path.join(config[\"base_video_dir\"], unique_id)\n",
    "    os.makedirs(audio_images_dir, exist_ok=True)\n",
    "    os.makedirs(audio_videos_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Transcribe audio using Whisper\n",
    "    print(f\"Transcribe audio using Whisper\")\n",
    "    model = whisper.load_model(\"turbo\")\n",
    "    result = model.transcribe(audio_file)\n",
    "\n",
    "    # Cleanup Whisper model memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    segments = result['segments']\n",
    "\n",
    "    # Extract list of start times and texts\n",
    "    segment_texts_and_start_times = [(segment['text'].strip(), segment['start']) for segment in segments]\n",
    "\n",
    "    # Combine texts\n",
    "    text = \"\"\n",
    "    for segment_text, start in segment_texts_and_start_times:\n",
    "        text += f\"Start: {start}, Text: {segment_text}\\n\"\n",
    "\n",
    "    last_end_value = segments[-1]['end']\n",
    "\n",
    "    # Path to scenes.json file\n",
    "    scenes_file_path = os.path.join(audio_images_dir, \"scenes.json\")\n",
    "\n",
    "    # Check if scenes.json exists\n",
    "    if os.path.exists(scenes_file_path):\n",
    "        print(f\"Scenes file already exists at {scenes_file_path}. Skipping scene generation.\")\n",
    "        with open(scenes_file_path, \"r\") as scenes_file:\n",
    "            scenes = json.load(scenes_file)\n",
    "        return scenes, audio_images_dir, audio_videos_dir, last_end_value\n",
    "\n",
    "    # Step 2: Generate video summary using OpenAI\n",
    "    print(f\"Generate video summary using OpenAI\")\n",
    "    video_summary_prompt = f'Create a short summary that describes a music video based on these lyrics: {text}'\n",
    "    video_summary = get_openai_prompt_response(video_summary_prompt, config, openai_model=config[\"openai_model\"])\n",
    "\n",
    "    # Step 3: Create scenes based on lyrics\n",
    "    print(f\"Create scenes based on lyrics\")\n",
    "    try:\n",
    "        scenes = create_scenes(text, video_summary, config)\n",
    "    except:\n",
    "        try:\n",
    "            scenes = create_scenes(text, video_summary, config)\n",
    "        except:\n",
    "            try:\n",
    "                scenes = create_scenes(text, video_summary, config)\n",
    "            except: \n",
    "                return \"\", audio_images_dir, audio_videos_dir, last_end_value\n",
    "            \n",
    "    # we don't want scenes longer than 18 seconds\n",
    "    new_scenes = []\n",
    "    for i in range(len(scenes)):\n",
    "        scene = scenes[i]\n",
    "        if i == 0:\n",
    "            start_time = 0\n",
    "        else:\n",
    "            start_time = scene['start']\n",
    "        # Determine the end time\n",
    "        if i < len(scenes) - 1:\n",
    "            end_time = scenes[i + 1]['start']\n",
    "        else:\n",
    "            end_time = last_end_value\n",
    "        duration = end_time - start_time\n",
    "        # Split the scene if duration exceeds max_duration_seconds seconds\n",
    "        while duration > max_duration_seconds:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "            start_time += max_duration_seconds\n",
    "            duration = end_time - start_time\n",
    "        # Append the remaining part of the scene\n",
    "        if duration > 0:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "    # Replace the original scenes with the new list\n",
    "    scenes = new_scenes\n",
    "    # improve the scenes with a revision\n",
    "    try:\n",
    "        scenes_revised = revise_scenes(scenes, config)\n",
    "        scenes = scenes_revised\n",
    "        print(f'revised scenes')\n",
    "    except:\n",
    "        try:\n",
    "            scenes_revised = revise_scenes(scenes, config)\n",
    "            scenes = scenes_revised\n",
    "            print(f'revised scenes')\n",
    "        except:\n",
    "            print('cannot revise scenes')\n",
    "            \n",
    "    \n",
    "    # Save the scenes to scenes.json\n",
    "    with open(scenes_file_path, \"w\") as scenes_file:\n",
    "        json.dump(scenes, scenes_file)\n",
    "        \n",
    "    return scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def process_audio_images(config: dict, scenes, audio_images_dir):\n",
    "    # Step 4: Load Flux pipeline and generate images\n",
    "    print(f\"Load Flux pipeline and generate images\")\n",
    "    flux_pipe = load_flux_pipe()\n",
    "    height = HEIGHT\n",
    "    width = WIDTH\n",
    "    guidance_scale = 3.9\n",
    "    num_inference_steps = 8\n",
    "    max_sequence_length = 512\n",
    "    seed = -1\n",
    "\n",
    "    try:\n",
    "        # Generate images for each scene\n",
    "        image_num = 1\n",
    "        for scene in scenes:\n",
    "            image_prompt = scene['scene_description']\n",
    "            image = gen_flux_image(flux_pipe, image_prompt, config, height, width, guidance_scale, num_inference_steps, max_sequence_length, seed)\n",
    "            filename = f\"image_{str(image_num).zfill(2)}.jpg\"\n",
    "            image_path = os.path.join(audio_images_dir, filename)\n",
    "            image.save(image_path, dpi=(300, 300))\n",
    "            del image\n",
    "            torch.cuda.empty_cache()\n",
    "            image_num += 1\n",
    "    finally:\n",
    "        # Move the pipeline back to CPU and delete it\n",
    "        flux_pipe.to('cpu')\n",
    "        del flux_pipe\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return\n",
    "\n",
    "def process_audio_video(config: dict, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp, skip_first):\n",
    "    video_num = 1\n",
    "    negative_prompt = \"Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion\"\n",
    "   \n",
    "    sampler = load_hunyuan_video_sampler(\n",
    "        server_config_filename=\"gradio_config.json\",\n",
    "        forced_profile_no=-1,\n",
    "        verbose_level=1,\n",
    "        quantize_transformer=True,\n",
    "        lora_weight=[],\n",
    "        lora_multiplier=[],\n",
    "        device=\"cuda\",  # or \"cpu\"\n",
    "    )\n",
    "    # Step 7: Generate video sequences\n",
    "    for i, scene in enumerate(scenes):\n",
    "        prompt = scene[\"scene_description\"] + \" \" + scene[\"action_sequence\"]\n",
    "\n",
    "        # Calculate duration to keep the video in 6-second increments\n",
    "        if i + 1 < len(scenes):\n",
    "            next_start_time = scenes[i + 1][\"start\"]\n",
    "        else:\n",
    "            next_start_time = last_end_value  # Use the final ending time for the last scene\n",
    "\n",
    "        if i == 0:\n",
    "            duration = next_start_time\n",
    "        else:\n",
    "            duration = next_start_time - scene[\"start\"]\n",
    "        num_video_segments = int((duration + 2) // 5)\n",
    "\n",
    "        print(f\"Scene {i+1} has {num_video_segments} segments\")\n",
    "        for j in range(num_video_segments):\n",
    "            video_name = f\"video_{str(video_num).zfill(2)}_{str(j+1).zfill(2)}_{timestamp}.mp4\"\n",
    "            video_output_path = os.path.join(audio_videos_dir, video_name)\n",
    "            if video_num > skip_first:\n",
    "                seed = random.randint(0, MAX_SEED)\n",
    "                generate_video(hunyuan_video_sampler=sampler, height=HEIGHT, width=WIDTH, video_length=121, infer_steps=50,\n",
    "                    prompt=prompt, negative_prompt=negative_prompt, flow_shift=7.0, filename=video_output_path,\n",
    "                    seed=seed, cfg_scale=7.5, batch_size=1, embedded_cfg_scale=1.0)\n",
    "                \n",
    "                time.sleep(1)  # Pause for 1 second\n",
    "\n",
    "            video_num += 1  # Increment video number for the next segment\n",
    "    \n",
    "    free_hunyuan_video_model(hunyuan_video_sampler)\n",
    "    return\n",
    "\n",
    "\n",
    "def process_all_audios(audio_file, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a list of audio files through the workflow.\n",
    "    \"\"\"\n",
    "    print(f\"Processing audio file: {audio_file}\")\n",
    "    scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_audio_scenes(audio_file, config)\n",
    "    print(f'{len(scenes)} scenes:\\n{json.dumps(scenes, indent=4)}')\n",
    "    # Create starting images for scenes\n",
    "    process_audio_images(config, scenes, audio_images_dir)\n",
    "    return config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def create_video(images_only):\n",
    "    config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_all_audios(audio_file, CONFIG)\n",
    "    if not images_only:\n",
    "        process_audio_video(config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp)\n",
    "    print(f'audio_images_dir: {audio_images_dir}')\n",
    "    print(f'audio_videos_dir: {audio_videos_dir}')\n",
    "    print(f'last_end_value: {last_end_value}')\n",
    "    print(f'timestamp: {timestamp}')\n",
    "    \n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0cc96-e609-4cd2-9c67-dcd63562988a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run and curate images for scenes\n",
    "human_in_loop = True\n",
    "for audio_file in CONFIG[\"audio_files\"]:\n",
    "    create_video(human_in_loop)\n",
    "\n",
    "reset_memory(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb4c19-a206-4882-9e44-a8b0e169033d",
   "metadata": {},
   "source": [
    "## Video is expensive, only process after curating scenes and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97267f-5524-4701-a96a-c509a40fe4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-19 21:14:42.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.inference\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mGot text-to-video model root path: ckpts/hunyuan-video-t2v-720p/transformers/hunyuan_video_720_quanto_int8.safetensors\u001b[0m\n",
      "\u001b[32m2025-01-19 21:14:42.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.inference\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m187\u001b[0m - \u001b[1mBuilding model...\u001b[0m\n",
      "\u001b[32m2025-01-19 21:14:42.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.inference\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mLoading torch model ckpts/hunyuan-video-t2v-720p/transformers/hunyuan_video_720_quanto_int8.safetensors...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinning data of 'ckpts/hunyuan-video-t2v-720p/transformers/hunyuan_video_720_quanto_int8.safetensors' to reserved RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-19 21:14:49.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.vae\u001b[0m:\u001b[36mload_vae\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLoading 3D VAE model (884-16c-hy) from: ./ckpts/hunyuan-video-t2v-720p/vae\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole model was pinned to reserved RAM: 54 large blocks spread across 12580.24 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-19 21:14:51.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.vae\u001b[0m:\u001b[36mload_vae\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mVAE to dtype: torch.float16\u001b[0m\n",
      "\u001b[32m2025-01-19 21:14:51.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.text_encoder\u001b[0m:\u001b[36mload_tokenizer\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mLoading tokenizer (llm) from: ./ckpts/text_encoder\u001b[0m\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "\u001b[32m2025-01-19 21:14:51.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.text_encoder\u001b[0m:\u001b[36mload_text_encoder\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mLoading text encoder model (clipL) from: ./ckpts/text_encoder_2\u001b[0m\n",
      "\u001b[32m2025-01-19 21:14:51.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.text_encoder\u001b[0m:\u001b[36mload_text_encoder\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mText encoder to dtype: torch.float16\u001b[0m\n",
      "\u001b[32m2025-01-19 21:14:52.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.text_encoder\u001b[0m:\u001b[36mload_tokenizer\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mLoading tokenizer (clipL) from: ./ckpts/text_encoder_2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m************ Memory Management for the GPU Poor (mmgp 3.1) by DeepBeepMeep ************\u001b[0m\u001b[0m\n",
      "You have chosen a profile that requires at least 48 GB of RAM and 12 GB of VRAM. Some RAM is consumed to reduce VRAM consumption.\n",
      "Model 'transformer' is already quantized to format 'qint8'\n",
      "Pinning data of 'vae' to reserved RAM\n",
      "The whole model was pinned to reserved RAM: 2 large blocks spread across 470.12 MB\n",
      "Pinning data of 'text_encoder' to reserved RAM\n",
      "The whole model was pinned to reserved RAM: 34 large blocks spread across 7661.63 MB\n",
      "Model 'transformer' already pinned to reserved memory\n",
      "Pinning data of 'text_encoder_2' to reserved RAM\n",
      "The whole model was pinned to reserved RAM: 1 large blocks spread across 234.72 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-19 21:14:57.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.inference\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mInput (height, width, video_length) = (480, 848, 121)\u001b[0m\n",
      "\u001b[32m2025-01-19 21:14:57.580\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mhyvideo.inference\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m647\u001b[0m - \u001b[34m\u001b[1m\n",
      "                        height: 480\n",
      "                         width: 848\n",
      "                  video_length: 121\n",
      "                        prompt: ['Under a vast, twinkling sky, the desert comes alive with shimmering sands. The Southern Cross glows softly, illuminating a circle of individuals in flowing, translucent garments that catch the breeze. Their faces, aglow with wonder, reflect the celestial beauty above. The air is cool and crisp, filled with the gentle rustle of leaves and the distant call of nocturnal creatures, creating a serene atmosphere of unity and peace. The group raises their arms towards the sky, swaying gently, lost in the beauty of the cosmos.']\n",
      "                    neg_prompt: ['Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion']\n",
      "                          seed: 2091345338\n",
      "                   infer_steps: 50\n",
      "         num_videos_per_prompt: 1\n",
      "                guidance_scale: 1.0\n",
      "                      n_tokens: 49290\n",
      "                    flow_shift: 7.0\n",
      "       embedded_guidance_scale: 6.0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1 has 3 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6ef99dadbf4f2b90751b9d52941a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-19 21:31:44.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.inference\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m681\u001b[0m - \u001b[1mSuccess, time: 1006.8779830932617\u001b[0m\n",
      "\u001b[32m2025-01-19 21:31:45.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.inference\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mInput (height, width, video_length) = (480, 848, 121)\u001b[0m\n",
      "\u001b[32m2025-01-19 21:31:45.962\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mhyvideo.inference\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m647\u001b[0m - \u001b[34m\u001b[1m\n",
      "                        height: 480\n",
      "                         width: 848\n",
      "                  video_length: 121\n",
      "                        prompt: ['Under a vast, twinkling sky, the desert comes alive with shimmering sands. The Southern Cross glows softly, illuminating a circle of individuals in flowing, translucent garments that catch the breeze. Their faces, aglow with wonder, reflect the celestial beauty above. The air is cool and crisp, filled with the gentle rustle of leaves and the distant call of nocturnal creatures, creating a serene atmosphere of unity and peace. The group raises their arms towards the sky, swaying gently, lost in the beauty of the cosmos.']\n",
      "                    neg_prompt: ['Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion']\n",
      "                          seed: 1752803209\n",
      "                   infer_steps: 50\n",
      "         num_videos_per_prompt: 1\n",
      "                guidance_scale: 1.0\n",
      "                      n_tokens: 49290\n",
      "                    flow_shift: 7.0\n",
      "       embedded_guidance_scale: 6.0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2950d5d4eaf48cfa809b0f1bd05ea4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-19 21:48:33.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.inference\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m681\u001b[0m - \u001b[1mSuccess, time: 1007.43781208992\u001b[0m\n",
      "\u001b[32m2025-01-19 21:48:34.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhyvideo.inference\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mInput (height, width, video_length) = (480, 848, 121)\u001b[0m\n",
      "\u001b[32m2025-01-19 21:48:34.860\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mhyvideo.inference\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m647\u001b[0m - \u001b[34m\u001b[1m\n",
      "                        height: 480\n",
      "                         width: 848\n",
      "                  video_length: 121\n",
      "                        prompt: ['Under a vast, twinkling sky, the desert comes alive with shimmering sands. The Southern Cross glows softly, illuminating a circle of individuals in flowing, translucent garments that catch the breeze. Their faces, aglow with wonder, reflect the celestial beauty above. The air is cool and crisp, filled with the gentle rustle of leaves and the distant call of nocturnal creatures, creating a serene atmosphere of unity and peace. The group raises their arms towards the sky, swaying gently, lost in the beauty of the cosmos.']\n",
      "                    neg_prompt: ['Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion']\n",
      "                          seed: 208603514\n",
      "                   infer_steps: 50\n",
      "         num_videos_per_prompt: 1\n",
      "                guidance_scale: 1.0\n",
      "                      n_tokens: 49290\n",
      "                    flow_shift: 7.0\n",
      "       embedded_guidance_scale: 6.0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d573e722074c4f81d4aae7c5972d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_in_loop = True\n",
    "skip_first = 0\n",
    "if human_in_loop:\n",
    "    scenes_file_path = './images/DreamingTogetherness_20250119_110422/scenes.json'\n",
    "    audio_images_dir = './images/DreamingTogetherness_20250119_110422'\n",
    "    audio_videos_dir = './output/DreamingTogetherness_20250119_110422'\n",
    "    timestamp = '20250119_110422'\n",
    "    last_end_value = 215.52\n",
    "    \n",
    "    with open(scenes_file_path, \"r\") as scenes_file:\n",
    "        scenes = json.load(scenes_file)\n",
    "    process_audio_video(CONFIG, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp, skip_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a0750-5e16-4713-8ac3-825cb8713762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
