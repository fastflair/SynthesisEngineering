{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f87d72-5305-4778-ba0c-31549333c8db",
   "metadata": {},
   "source": [
    "# Music Video Synthesis\n",
    "* Extract lyrics from song with timestamps\n",
    "* Compose scenes, include timestamps\n",
    "* Construct video text prompt for each scene\n",
    "* Build videos for each scene\n",
    "* Stitch together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b074-e32d-45f2-977f-c923878625e6",
   "metadata": {},
   "source": [
    "# We will use openai whipser for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45c210-fd2b-4381-9fd3-c1eb18feefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet --upgrade pip\n",
    "#!pip3 install torch==2.4.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "#!pip install --quiet --upgrade openai-whisper openai\n",
    "# Ubuntu or Debian\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!pip install setuptools-rust\n",
    "#!pip install -U diffusers imageio imageio_ffmpeg opencv-python moviepy transformers huggingface-hub optimum pillow safetensors optimum-quanto accelerate\n",
    "#!pip install --upgrade optimum-quanto torchao --extra-index-url https://download.pytorch.org/whl/cu124 # full options are cpu/cu118/cu121/cu124\n",
    "#!pip install git+https://github.com/xhinker/sd_embed.git@main\n",
    "#!pip install accelerate flash_attention numba -U\n",
    "#!pip install flash_attn --no-build-isolation\n",
    "#!pip install -r requirements.txt -U\n",
    "#!pip install protobuf==3.20.3\n",
    "#!pip install --upgrade google-api-core google-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537e766-eab2-4757-b6f9-fbac4da44930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import cv2\n",
    "import diffusers\n",
    "import gc\n",
    "import imageio\n",
    "import imageio_ffmpeg\n",
    "import json\n",
    "import math\n",
    "import moviepy.editor as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torchaudio\n",
    "import tqdm\n",
    "import whisper\n",
    "\n",
    "from cached_path import cached_path\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from diffusers import AutoencoderKL, AutoencoderKLCogVideoX, AutoPipelineForText2Image, CogVideoXTransformer3DModel, CogVideoXPipeline, CogVideoXDPMScheduler\n",
    "from diffusers import CogVideoXTransformer3DModel, CogVideoXImageToVideoPipeline, FlowMatchEulerDiscreteScheduler\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from diffusers.utils import export_to_video, load_video, load_image\n",
    "from einops import rearrange\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from model import CFM, DiT, MMDiT, UNetT\n",
    "from model.utils import (convert_char_to_pinyin, get_tokenizer,\n",
    "                         load_checkpoint, save_spectrogram)\n",
    "from model4 import T5EncoderModel as m_T5EncoderModel, FluxTransformer2DModel\n",
    "from numba import cuda\n",
    "from openai import OpenAI\n",
    "from optimum.quanto import freeze, qfloat8, quantize, requantize\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pydub import AudioSegment, silence\n",
    "from safetensors.torch import load_file as load_safetensors, save_file as save_safetensors\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_flux1\n",
    "from tempfile import NamedTemporaryFile\n",
    "from torchaudio import transforms\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "from transformers import pipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5TokenizerFast, T5EncoderModel as t_T5EncoderModel\n",
    "from vocos import Vocos\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Define the paths where quantized weights will be saved\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"CPU\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "retry_limit = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54ba32-2208-45c8-8ed2-5fcb1e79aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"openai_api_key\": \"\",\n",
    "    \"openai_model\": \"gpt-4o-mini\",\n",
    "    \"openai_model_large\": \"gpt-4o\",\n",
    "    \"hf_token\": \"\",\n",
    "    \"base_working_dir\": \"./images\",\n",
    "    \"base_video_dir\": \"./output\",\n",
    "    \"goals\": [\n",
    "        '''The goal of the animation is to craft a relatable, emotionally resonant story that inspires viewers to reflect on their lives and encourages a shift in their mindset. The movie should be fun and catchy while delivering a deeper message about self-acceptance and spiritual growth.''',\n",
    "        # Add more goals\n",
    "    ],\n",
    "    \"device\": device,\n",
    "    \"dtype\": dtype,\n",
    "    \"retry_limit\": retry_limit,\n",
    "    \"MAX_SEED\": MAX_SEED,\n",
    "}\n",
    "scenes_file_path = './output/scenes.json'\n",
    "\n",
    "# Ensure base directories exist\n",
    "os.makedirs(CONFIG[\"base_working_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"base_video_dir\"], exist_ok=True)\n",
    "\n",
    "# Configurations\n",
    "target_sample_rate = 24000\n",
    "n_mel_channels = 100\n",
    "hop_length = 256\n",
    "target_rms = 0.1\n",
    "nfe_step = 32\n",
    "cfg_strength = 2.0\n",
    "ode_method = \"euler\"\n",
    "sway_sampling_coef = -1.0\n",
    "speed = 1.0\n",
    "remove_silence_default = True\n",
    "\n",
    "target_sample_rate = 24000\n",
    "n_mel_channels = 100\n",
    "hop_length = 256\n",
    "target_rms = 0.1\n",
    "nfe_step = 32  # 16, 32\n",
    "cfg_strength = 2.0\n",
    "ode_method = \"euler\"\n",
    "sway_sampling_coef = -1.0\n",
    "speed = 1.0\n",
    "# fix_duration = 27  # None or float (duration in seconds)\n",
    "fix_duration = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34281c1-e231-4472-9d9b-096fa23b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_memory(device):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    \n",
    "def get_openai_prompt_response(\n",
    "    prompt: str,\n",
    "    config: dict,\n",
    "    max_tokens: int = 6000,\n",
    "    temperature: float = 0.33,\n",
    "    openai_model: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response with retry logic.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Act as a helpful assistant, you are an expert editor.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=openai_model or config[\"openai_model\"],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < config[\"retry_limit\"]:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            return message_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == config[\"retry_limit\"]:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{config['retry_limit']})\")\n",
    "                time.sleep(1)  # Optional: wait before retrying\n",
    "\n",
    "\n",
    "def load_flux_pipe():\n",
    "    text_encoder_2 = m_T5EncoderModel.from_pretrained(\n",
    "        \"HighCWu/FLUX.1-dev-4bit\",\n",
    "        subfolder=\"text_encoder_2\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        # hqq_4bit_compute_dtype=torch.float32,\n",
    "    )\n",
    "    \n",
    "    transformer = FluxTransformer2DModel.from_pretrained(\n",
    "        \"HighCWu/FLUX.1-dev-4bit\",\n",
    "        subfolder=\"transformer\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    pipe = FluxPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-dev\",\n",
    "        text_encoder_2=text_encoder_2,\n",
    "        transformer=transformer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    #pipe.enable_model_cpu_offload() # with cpu offload, it cost 8.5GB vram\n",
    "    pipe.remove_all_hooks()\n",
    "    pipe = pipe.to('cuda') # without cpu offload, it cost 11GB vram\n",
    "\n",
    "    del text_encoder_2\n",
    "    del transformer\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def gen_flux_image(pipe, prompt, config: dict, height=1024, width=1024, guidance_scale=3.5, num_inference_steps=4, max_sequence_length=512, seed=-1):\n",
    "    \"\"\"\n",
    "    Generates an image based on the provided prompt using the Flux pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds, pooled_prompt_embeds = get_weighted_text_embeddings_flux1(\n",
    "            pipe        = pipe,\n",
    "            prompt    = prompt\n",
    "        )\n",
    "        \n",
    "        image = pipe(\n",
    "            prompt_embeds               = prompt_embeds,\n",
    "            pooled_prompt_embeds      = pooled_prompt_embeds,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            guidance_scale=guidance_scale,\n",
    "            output_type=\"pil\",\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "        ).images[0]\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "def load_video_pipeline():\n",
    "    \"\"\"\n",
    "    Loads and configures the video generation pipeline.\n",
    "    \"\"\"\n",
    "    quantization = int8_weight_only\n",
    "\n",
    "    text_encoder = t_T5EncoderModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\n",
    "    quantize_(text_encoder, quantization())\n",
    "    \n",
    "    transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n",
    "    quantize_(transformer, quantization())\n",
    "    \n",
    "    i2v_transformer = CogVideoXTransformer3DModel.from_pretrained(\n",
    "        \"THUDM/CogVideoX-5b-I2V\", subfolder=\"transformer\", torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"vae\", torch_dtype=torch.bfloat16)\n",
    "    quantize_(vae, quantization())\n",
    "    \n",
    "    # Create pipeline and run inference\n",
    "    pipe = CogVideoXPipeline.from_pretrained(\n",
    "        \"THUDM/CogVideoX-5b\",\n",
    "        text_encoder=text_encoder,\n",
    "        transformer=transformer,\n",
    "        vae=vae,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    #pipe.enable_model_cpu_offload()\n",
    "    pipe.vae.enable_tiling()\n",
    "    \n",
    "    pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "    \n",
    "    i2v_vae=pipe.vae\n",
    "    i2v_scheduler=pipe.scheduler\n",
    "    i2v_tokenizer=pipe.tokenizer\n",
    "    i2v_text_encoder=pipe.text_encoder\n",
    "    \n",
    "    del pipe\n",
    "    reset_memory(device)\n",
    "    \n",
    "    # Load the pipeline once before the loop\n",
    "    pipe_image = CogVideoXImageToVideoPipeline.from_pretrained(\n",
    "        \"THUDM/CogVideoX-5b-I2V\",\n",
    "        transformer=i2v_transformer,\n",
    "        vae=i2v_vae,\n",
    "        scheduler=i2v_scheduler,\n",
    "        tokenizer=i2v_tokenizer,\n",
    "        text_encoder=i2v_text_encoder,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).to(device)\n",
    "\n",
    "    #pipe_image.transformer.to(memory_format=torch.channels_last)\n",
    "    #pipe_image.transformer = torch.compile(pipe_image.transformer, fullgraph=True)\n",
    "\n",
    "    return pipe_image\n",
    "\n",
    "\n",
    "def infer_vid(pipe_image, prompt: str, image_input: str, config: dict, num_inference_steps: int = 50, guidance_scale: float = 7.0, seed: int = -1, num_frames: int = 49):\n",
    "    \"\"\"\n",
    "    Generates video frames from an image and prompt using the video pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 255)\n",
    "\n",
    "    image_input = Image.open(image_input).resize(size=(720, 480))  # Convert to PIL\n",
    "    image = load_image(image_input)\n",
    "\n",
    "    video_pt = pipe_image(\n",
    "        image=image,\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        num_videos_per_prompt=1,\n",
    "        use_dynamic_cfg=True,\n",
    "        output_type=\"pt\",\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=torch.Generator(device=\"cpu\").manual_seed(seed),\n",
    "        num_frames=num_frames,\n",
    "    ).frames\n",
    "\n",
    "    return video_pt, seed\n",
    "\n",
    "\n",
    "def generate_video(pipe_image, prompt, image_input, config: dict, seed_value: int = -1, video_filename: str = \"\", num_frames: int = 49):\n",
    "    \"\"\"\n",
    "    Generates and saves a video from the provided image and prompt.\n",
    "    \"\"\"\n",
    "    latents, seed = infer_vid(\n",
    "        pipe_image,\n",
    "        prompt,\n",
    "        image_input,\n",
    "        config,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.0,\n",
    "        seed=seed_value,\n",
    "        num_frames=num_frames,\n",
    "    )\n",
    "    batch_size = latents.shape[0]\n",
    "    batch_video_frames = []\n",
    "    for batch_idx in range(batch_size):\n",
    "        pt_image = latents[batch_idx]\n",
    "        pt_image = torch.stack([pt_image[i] for i in range(pt_image.shape[0])])\n",
    "        image_np = VaeImageProcessor.pt_to_numpy(pt_image)\n",
    "        image_pil = VaeImageProcessor.numpy_to_pil(image_np)\n",
    "        batch_video_frames.append(image_pil)\n",
    "\n",
    "    video_path = save_video(\n",
    "        batch_video_frames[0],\n",
    "        fps=math.ceil((len(batch_video_frames[0]) - 1) / 6),\n",
    "        filename=video_filename\n",
    "    )\n",
    "    # After processing frames\n",
    "    del latents\n",
    "    del batch_video_frames\n",
    "    reset_memory(device)\n",
    "    return video_path\n",
    "\n",
    "\n",
    "def save_video(frames, fps: int, filename: str):\n",
    "    \"\"\"\n",
    "    Saves a list of frames as a video file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        writer = imageio.get_writer(temp_video_path, fps=fps)\n",
    "        for frame in frames:\n",
    "            writer.append_data(np.array(frame))\n",
    "        writer.close()\n",
    "\n",
    "    os.rename(temp_video_path, filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def convert_to_gif(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a video file to a GIF.\n",
    "    \"\"\"\n",
    "    clip = mp.VideoFileClip(video_path)\n",
    "    clip = clip.set_fps(8)\n",
    "    clip = clip.resize(height=240)\n",
    "    gif_path = video_path.replace(\".mp4\", \".gif\")\n",
    "    clip.write_gif(gif_path, fps=8)\n",
    "    return gif_path\n",
    "\n",
    "\n",
    "def resize_if_unfit(input_video: str) -> str:\n",
    "    \"\"\"\n",
    "    Resizes the video to the target dimensions if it does not match.\n",
    "    \"\"\"\n",
    "    width, height = get_video_dimensions(input_video)\n",
    "\n",
    "    if width == 720 and height == 480:\n",
    "        return input_video\n",
    "    else:\n",
    "        return center_crop_resize(input_video)\n",
    "\n",
    "\n",
    "def get_video_dimensions(input_video_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Retrieves the dimensions of the video.\n",
    "    \"\"\"\n",
    "    reader = imageio_ffmpeg.read_frames(input_video_path)\n",
    "    metadata = next(reader)\n",
    "    return metadata[\"size\"]\n",
    "\n",
    "\n",
    "def center_crop_resize(input_video_path: str, target_width: int = 720, target_height: int = 480) -> str:\n",
    "    \"\"\"\n",
    "    Resizes and center-crops the video to the target dimensions.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    width_factor = target_width / orig_width\n",
    "    height_factor = target_height / orig_height\n",
    "    resize_factor = max(width_factor, height_factor)\n",
    "\n",
    "    inter_width = int(orig_width * resize_factor)\n",
    "    inter_height = int(orig_height * resize_factor)\n",
    "\n",
    "    target_fps = 8\n",
    "    ideal_skip = max(0, math.ceil(orig_fps / target_fps) - 1)\n",
    "    skip = min(5, ideal_skip)  # Cap at 5\n",
    "\n",
    "    while (total_frames / (skip + 1)) < 49 and skip > 0:\n",
    "        skip -= 1\n",
    "\n",
    "    processed_frames = []\n",
    "    frame_count = 0\n",
    "    total_read = 0\n",
    "\n",
    "    while frame_count < 49 and total_read < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if total_read % (skip + 1) == 0:\n",
    "            resized = cv2.resize(frame, (inter_width, inter_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            start_x = (inter_width - target_width) // 2\n",
    "            start_y = (inter_height - target_height) // 2\n",
    "            cropped = resized[start_y:start_y + target_height, start_x:start_x + target_width]\n",
    "\n",
    "            processed_frames.append(cropped)\n",
    "            frame_count += 1\n",
    "\n",
    "        total_read += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(temp_video_path, fourcc, target_fps, (target_width, target_height))\n",
    "\n",
    "        for frame in processed_frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    return temp_video_path\n",
    "\n",
    "\n",
    "def extract_last_frame(video_filename: str, output_image_filename: str):\n",
    "    \"\"\"\n",
    "    Extracts the last frame from a video file and saves it as an image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = imageio.get_reader(video_filename, 'ffmpeg')\n",
    "        last_frame = None\n",
    "        for frame in reader:\n",
    "            last_frame = frame\n",
    "        reader.close()\n",
    "\n",
    "        if last_frame is not None:\n",
    "            imageio.imwrite(output_image_filename, last_frame)\n",
    "            print(f\"Last frame saved successfully as '{output_image_filename}'.\")\n",
    "        else:\n",
    "            print(\"The video contains no frames.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{video_filename}' was not found.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except RuntimeError as re:\n",
    "        print(f\"RuntimeError: {re}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def revise_scenes(scenes, config: dict):\n",
    "    \"\"\"\n",
    "    Revise scenes based on the extracted scenes.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Revise the JSON scenes to update the scene_description and action_sequence to engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  JSON scenes: {scenes}   \n",
    "The scene_description (200 words or less) should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes.\n",
    "The action_sequence (30 words or less) should describe the action in the scene.  The goal is to create input to create a stunning, cinematic video experience.\n",
    "Only update the scene_description and action_sequence.  Do not delete any items as having scenes with the given start times are important.  We do not want to have the same scene_description and action_sequence for the items with repeatitive input text.  Please change these to be creative and consistent with dynamic video sequences.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.33)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def process_audio_scenes(scenes, config: dict):\n",
    "    # Create unique identifier based on audio file name\n",
    "    audio_basename = 'video'\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = f\"{audio_basename}_{timestamp}\"\n",
    "\n",
    "    # Create unique directories for images and videos\n",
    "    print(f\"Create unique directories for images and videos\")\n",
    "    audio_images_dir = os.path.join(config[\"base_working_dir\"], unique_id)\n",
    "    audio_videos_dir = os.path.join(config[\"base_video_dir\"], unique_id)\n",
    "    os.makedirs(audio_images_dir, exist_ok=True)\n",
    "    os.makedirs(audio_videos_dir, exist_ok=True)\n",
    "\n",
    "    return scenes, audio_images_dir, audio_videos_dir\n",
    "\n",
    "def process_audio_images(config: dict, scenes, audio_images_dir):\n",
    "    # Step 4: Load Flux pipeline and generate images\n",
    "    print(f\"Load Flux pipeline and generate images\")\n",
    "    flux_pipe = load_flux_pipe()\n",
    "    height = 480\n",
    "    width = 720\n",
    "    guidance_scale = 3.9\n",
    "    num_inference_steps = 48\n",
    "    max_sequence_length = 512\n",
    "    seed = -1\n",
    "\n",
    "    # Generate images for each scene\n",
    "    image_num = 1\n",
    "    for scene in scenes:\n",
    "        image_prompt = scene['scene_description']\n",
    "        image = gen_flux_image(flux_pipe, image_prompt, config, height, width, guidance_scale, num_inference_steps, max_sequence_length, seed)\n",
    "        filename = f\"image_{str(image_num).zfill(2)}.jpg\"\n",
    "        image_path = os.path.join(audio_images_dir, filename)\n",
    "        image.save(image_path, dpi=(300, 300))\n",
    "        image_num += 1\n",
    "\n",
    "    # Move the pipeline back to CPU and delete it\n",
    "    flux_pipe.to('cpu')\n",
    "    del flux_pipe\n",
    "    reset_memory(device)\n",
    "    return\n",
    "\n",
    "def process_audio_video(config: dict, scenes, audio_images_dir, audio_videos_dir):\n",
    "    # Step 6: Load Video Pipeline\n",
    "    print(f\"Load Video Pipeline\")\n",
    "    video_pipe = load_video_pipeline()\n",
    "\n",
    "    # Temporary image path\n",
    "    temp_image = os.path.join(audio_images_dir, \"temp_image.jpg\")\n",
    "    video_num = 1\n",
    "\n",
    "    # Step 7: Generate video sequences\n",
    "    for i, scene in enumerate(scenes):\n",
    "        prompt = scene[\"action_sequence\"]\n",
    "\n",
    "        # Use the initial image for each scene\n",
    "        image_input = os.path.join(audio_images_dir, f\"image_{str(i+1).zfill(2)}.jpg\")\n",
    "\n",
    "        num_video_segments = 2\n",
    "\n",
    "        print(f\"Scene {i+1} has {num_video_segments} segments\")\n",
    "        for j in range(num_video_segments):\n",
    "            video_name = f\"video_{str(video_num).zfill(2)}_{str(j+1).zfill(2)}.mp4\"\n",
    "            video_output_path = os.path.join(audio_videos_dir, video_name)\n",
    "            generate_video(video_pipe, prompt, image_input, config, seed_value=-1, video_filename=video_output_path)\n",
    "            time.sleep(1)  # Pause for 1 second\n",
    "\n",
    "            # After generating the video, extract the last frame to use as input for the next segment\n",
    "            extract_last_frame(video_output_path, temp_image)\n",
    "\n",
    "            # Use the last frame as input for the next video segment in the same scene\n",
    "            image_input = temp_image\n",
    "\n",
    "            video_num += 1  # Increment video number for the next segment\n",
    "\n",
    "    # Move the pipeline back to CPU before deleting\n",
    "    video_pipe.to('cpu')\n",
    "    del video_pipe\n",
    "    reset_memory(device)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def process_all_audios(scenes, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a list of audio files through the workflow.\n",
    "    \"\"\"\n",
    "    scenes, audio_images_dir, audio_videos_dir = process_audio_scenes(scenes, config)\n",
    "    # Create starting images for scenes\n",
    "    process_audio_images(config, scenes, audio_images_dir)\n",
    "    return config, scenes, audio_images_dir, audio_videos_dir\n",
    "\n",
    "def create_scenes(config, goal):    \n",
    "    \"\"\"\n",
    "    Creates scenes based on the extracted lyrics using OpenAI's API.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''\n",
    "I am creating Python code to generate scenes for a short animation.\n",
    "\n",
    "{goal}\n",
    "\n",
    "Please generate a series of scenes in JSON format, where each scene is a **complete and independent description** with no knowledge of the previous or future scenes. Each scene should contain the following fields:\n",
    "    \"character\": The character speaking in the scene (narrator, country, town, etc.)\n",
    "    \"emotion\": The emotion of the character (angry, calm, disgust, fearful, happy, neutral, sad, surprised)\n",
    "    \"text\": The narration or dialogue associated with the scene.\n",
    "    \"scene_description\": A detailed description of the scene, using evocative language to engage the audience emotionally.\n",
    "    \"action_sequence\": A description of the actions taking place in the scene.\n",
    "\n",
    "Each scene should contribute to the overall goal of the animation but be written in a way that it stands alone.\n",
    "\n",
    "Here's the format to use:    [ {{\n",
    "     \"character\": \"The character goes here...\"\n",
    "     \"emotion\": \"The character emotion goes here...\",\n",
    "     \"text\": \"Narration or dialogue goes here...\",\n",
    "     \"scene_description\": \"Detailed description of the scene...\",\n",
    "     \"action_sequence\": \"Description of the actions in the scene...\"\n",
    "    }},\n",
    "    {{\n",
    "     \"character\": \"The character goes here...\"\n",
    "     \"emotion\": \"The character emotion goes here...\",\n",
    "     \"text\": \"Next narration or dialogue...\",\n",
    "     \"scene_description\": \"Detailed description of the next scene...\",\n",
    "     \"action_sequence\": \"Description of the actions in the next scene...\"\n",
    "    }}\n",
    "    // Additional scenes...\n",
    "    ]\n",
    "    Return only the json, less jargon.\n",
    "    '''\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.66)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "\n",
    "    # Save the scenes to scenes.json\n",
    "    with open(scenes_file_path, \"w\") as scenes_file:\n",
    "        json.dump(scenes, scenes_file)\n",
    "        \n",
    "    return scenes\n",
    "\n",
    "def get_first_file_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Returns the full file path to the first file found in the specified directory.\n",
    "    If the directory does not contain any files, returns None.\n",
    "    \n",
    "    Parameters:\n",
    "        directory_path (str): The path to the directory to search.\n",
    "        \n",
    "    Returns:\n",
    "        str or None: The full path to the first file found, or None if no files are found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Iterate over the entries in the directory\n",
    "        for entry in os.listdir(directory_path):\n",
    "            full_path = os.path.join(directory_path, entry)\n",
    "            # Check if the entry is a file (not a directory)\n",
    "            if os.path.isfile(full_path):\n",
    "                return full_path  # Return the full path of the first file found\n",
    "        # No files found in the directory\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{directory_path}' does not exist.\")\n",
    "        return None\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied for directory '{directory_path}'.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_model(model_cls, model_cfg, ckpt_path, file_vocab):\n",
    "    if file_vocab == \"\":\n",
    "        file_vocab = \"Emilia_ZH_EN\"\n",
    "        tokenizer = \"pinyin\"\n",
    "    else:\n",
    "        tokenizer = \"custom\"\n",
    "\n",
    "    print(\"\\nvocab : \", file_vocab, tokenizer)\n",
    "    print(\"tokenizer : \", tokenizer)\n",
    "    print(\"model : \", ckpt_path, \"\\n\")\n",
    "\n",
    "    vocab_char_map, vocab_size = get_tokenizer(file_vocab, tokenizer)\n",
    "    model = CFM(\n",
    "        transformer=model_cls(\n",
    "            **model_cfg, text_num_embeds=vocab_size, mel_dim=n_mel_channels\n",
    "        ),\n",
    "        mel_spec_kwargs=dict(\n",
    "            target_sample_rate=target_sample_rate,\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            hop_length=hop_length,\n",
    "        ),\n",
    "        odeint_kwargs=dict(\n",
    "            method=ode_method,\n",
    "        ),\n",
    "        vocab_char_map=vocab_char_map,\n",
    "    ).to(device)\n",
    "\n",
    "    model = load_checkpoint(model, ckpt_path, device, use_ema=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def chunk_text(text, max_chars=135):\n",
    "    \"\"\"\n",
    "    Splits the input text into chunks, each with a maximum number of characters.\n",
    "    Args:\n",
    "        text (str): The text to be split.\n",
    "        max_chars (int): The maximum number of characters per chunk.\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    # Split the text into sentences based on punctuation followed by whitespace\n",
    "    sentences = re.split(r'(?<=[;:,.!?])\\s+|(?<=[；：，。！？])', text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk.encode('utf-8')) + len(sentence.encode('utf-8')) <= max_chars:\n",
    "            current_chunk += sentence + \" \" if sentence and len(sentence[-1].encode('utf-8')) == 1 else sentence\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \" if sentence and len(sentence[-1].encode('utf-8')) == 1 else sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def infer_batch(ref_audio, ref_text, gen_text_batches, ema_model, remove_silence, vocos, cross_fade_duration=0.15):\n",
    "    audio, sr = ref_audio\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "\n",
    "    rms = torch.sqrt(torch.mean(torch.square(audio)))\n",
    "    if rms < target_rms:\n",
    "        audio = audio * target_rms / rms\n",
    "    if sr != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sample_rate)\n",
    "        audio = resampler(audio)\n",
    "    audio = audio.to(device)\n",
    "\n",
    "    generated_waves = []\n",
    "    spectrograms = []\n",
    "\n",
    "    if len(ref_text[-1].encode('utf-8')) == 1:\n",
    "        ref_text = ref_text + \" \"\n",
    "\n",
    "    # Check if gen_text_batches is empty\n",
    "    if len(gen_text_batches) == 0:\n",
    "        raise ValueError(\"gen_text_batches is empty, cannot proceed with inference.\")\n",
    "\n",
    "    for i, gen_text in enumerate(tqdm.tqdm(gen_text_batches)):\n",
    "        # Prepare the text\n",
    "        text_list = [ref_text + gen_text]\n",
    "        final_text_list = convert_char_to_pinyin(text_list)\n",
    "\n",
    "        # Calculate duration\n",
    "        ref_audio_len = audio.shape[-1] // hop_length\n",
    "        zh_pause_punc = r\"。，、；：？！\"\n",
    "        ref_text_len = len(ref_text.encode('utf-8')) + 3 * len(re.findall(zh_pause_punc, ref_text))\n",
    "        gen_text_len = len(gen_text.encode('utf-8')) + 3 * len(re.findall(zh_pause_punc, gen_text))\n",
    "        duration = ref_audio_len + int(ref_audio_len / ref_text_len * gen_text_len / speed)\n",
    "\n",
    "        # inference\n",
    "        with torch.inference_mode():\n",
    "            try:\n",
    "                generated, _ = ema_model.sample(\n",
    "                    cond=audio,\n",
    "                    text=final_text_list,\n",
    "                    duration=duration,\n",
    "                    steps=nfe_step,\n",
    "                    cfg_strength=cfg_strength,\n",
    "                    sway_sampling_coef=sway_sampling_coef,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error during model inference: {e}\")\n",
    "                continue  # Skip to the next batch\n",
    "\n",
    "        # Ensure `generated` is not empty\n",
    "        if generated is None or generated.shape[0] == 0:\n",
    "            print(f\"Generated is empty for batch {i}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        generated = generated[:, ref_audio_len:, :]\n",
    "        generated_mel_spec = rearrange(generated, \"1 n d -> 1 d n\")\n",
    "        generated_wave = vocos.decode(generated_mel_spec.cpu())\n",
    "        if rms < target_rms:\n",
    "            generated_wave = generated_wave * rms / target_rms\n",
    "\n",
    "        # wav -> numpy\n",
    "        generated_wave = generated_wave.squeeze().cpu().numpy()\n",
    "        \n",
    "        generated_waves.append(generated_wave)\n",
    "        spectrograms.append(generated_mel_spec[0].cpu().numpy())\n",
    "\n",
    "    # If no waves were generated, raise an error\n",
    "    if len(generated_waves) == 0:\n",
    "        raise ValueError(\"No waves were generated, something went wrong during inference.\")\n",
    "\n",
    "    # Combine all generated waves with cross-fading\n",
    "    if cross_fade_duration <= 0:\n",
    "        # Simply concatenate\n",
    "        final_wave = np.concatenate(generated_waves)\n",
    "    else:\n",
    "        print(len(generated_waves))\n",
    "        final_wave = generated_waves[0]\n",
    "        for i in range(1, len(generated_waves)):\n",
    "            prev_wave = final_wave\n",
    "            next_wave = generated_waves[i]\n",
    "\n",
    "            # Calculate cross-fade samples, ensuring it does not exceed wave lengths\n",
    "            cross_fade_samples = int(cross_fade_duration * target_sample_rate)\n",
    "            cross_fade_samples = min(cross_fade_samples, len(prev_wave), len(next_wave))\n",
    "\n",
    "            if cross_fade_samples <= 0:\n",
    "                # No overlap possible, concatenate\n",
    "                final_wave = np.concatenate([prev_wave, next_wave])\n",
    "                continue\n",
    "\n",
    "            # Overlapping parts\n",
    "            prev_overlap = prev_wave[-cross_fade_samples:]\n",
    "            next_overlap = next_wave[:cross_fade_samples]\n",
    "\n",
    "            # Fade out and fade in\n",
    "            fade_out = np.linspace(1, 0, cross_fade_samples)\n",
    "            fade_in = np.linspace(0, 1, cross_fade_samples)\n",
    "\n",
    "            # Cross-faded overlap\n",
    "            cross_faded_overlap = prev_overlap * fade_out + next_overlap * fade_in\n",
    "\n",
    "            # Combine\n",
    "            new_wave = np.concatenate([\n",
    "                prev_wave[:-cross_fade_samples],\n",
    "                cross_faded_overlap,\n",
    "                next_wave[cross_fade_samples:]\n",
    "            ])\n",
    "\n",
    "            final_wave = new_wave\n",
    "\n",
    "    # Create a combined spectrogram\n",
    "    combined_spectrogram = np.concatenate(spectrograms, axis=1)\n",
    "\n",
    "    return final_wave, combined_spectrogram\n",
    "\n",
    "\n",
    "def process_voice(ref_audio_orig, ref_text):\n",
    "    print(\"Converting audio...\")\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as f:\n",
    "        aseg = AudioSegment.from_file(ref_audio_orig)\n",
    "\n",
    "        non_silent_segs = silence.split_on_silence(aseg, min_silence_len=1000, silence_thresh=-50, keep_silence=1000)\n",
    "        non_silent_wave = AudioSegment.silent(duration=0)\n",
    "        for non_silent_seg in non_silent_segs:\n",
    "            non_silent_wave += non_silent_seg\n",
    "        aseg = non_silent_wave\n",
    "\n",
    "        audio_duration = len(aseg)\n",
    "        if audio_duration > 15000:\n",
    "            print(\"Audio is over 15s, clipping to only first 15s.\")\n",
    "            aseg = aseg[:15000]\n",
    "        aseg.export(f.name, format=\"wav\")\n",
    "        ref_audio = f.name\n",
    "\n",
    "    if not ref_text.strip():\n",
    "        print(\"No reference text provided, transcribing reference audio...\")\n",
    "        #pipe = pipeline(\n",
    "        #    \"automatic-speech-recognition\",\n",
    "        #    model=\"openai/whisper-large-v3-turbo\",\n",
    "        #    torch_dtype=torch.float16,\n",
    "        #    device=device,\n",
    "        #)\n",
    "        pipe = get_whisper_pipe()\n",
    "        ref_text = pipe(\n",
    "            ref_audio,\n",
    "            chunk_length_s=30,\n",
    "            batch_size=128,\n",
    "            generate_kwargs={\"task\": \"transcribe\"},\n",
    "            return_timestamps=False,\n",
    "        )[\"text\"].strip()\n",
    "        print(\"Finished transcription\")\n",
    "    else:\n",
    "        print(\"Using custom reference text...\")\n",
    "    return ref_audio, ref_text    \n",
    "\n",
    "def infer(ref_audio, ref_text, gen_text, remove_silence, ema_model, vocos, cross_fade_duration=0.15):\n",
    "    print(gen_text)\n",
    "    # Add the functionality to ensure it ends with \". \"\n",
    "    if not ref_text.endswith(\". \") and not ref_text.endswith(\"。\"):\n",
    "        if ref_text.endswith(\".\"):\n",
    "            ref_text += \" \"\n",
    "        else:\n",
    "            ref_text += \". \"\n",
    "\n",
    "    # Split the input text into batches\n",
    "    audio, sr = torchaudio.load(ref_audio)\n",
    "    max_chars = int(len(ref_text.encode('utf-8')) / (audio.shape[-1] / sr) * (25 - audio.shape[-1] / sr))\n",
    "    gen_text_batches = chunk_text(gen_text, max_chars=max_chars)\n",
    "    print('ref_text', ref_text)\n",
    "    for i, gen_text in enumerate(gen_text_batches):\n",
    "        print(f'gen_text {i}', gen_text)\n",
    "    \n",
    "    print(f\"Generating audio in {len(gen_text_batches)} batches...\")\n",
    "    return infer_batch((audio, sr), ref_text, gen_text_batches, ema_model, remove_silence, vocos, cross_fade_duration)\n",
    "    \n",
    "\n",
    "def process(ref_audio, ref_text, text_gen, remove_silence, ema_model, config, vocos, wave_path):\n",
    "    main_voice = {\"ref_audio\": ref_audio, \"ref_text\": ref_text}\n",
    "    if \"voices\" not in config:\n",
    "        voices = {\"main\": main_voice}\n",
    "    else:\n",
    "        voices = config[\"voices\"]\n",
    "        voices[\"main\"] = main_voice\n",
    "    for voice in voices:\n",
    "        voices[voice]['ref_audio'], voices[voice]['ref_text'] = process_voice(voices[voice]['ref_audio'], voices[voice]['ref_text'])\n",
    "\n",
    "    generated_audio_segments = []\n",
    "    reg1 = r'(?=\\[\\w+\\])'\n",
    "    chunks = re.split(reg1, text_gen)\n",
    "    reg2 = r'\\[(\\w+)\\]'\n",
    "    for text in chunks:\n",
    "        match = re.match(reg2, text)\n",
    "        if not match or voice not in voices:\n",
    "            voice = \"main\"\n",
    "        else:\n",
    "            voice = match[1]\n",
    "        text = re.sub(reg2, \"\", text)\n",
    "        gen_text = text.strip()\n",
    "        ref_audio = voices[voice]['ref_audio']\n",
    "        ref_text = voices[voice]['ref_text']\n",
    "        print(f\"Voice: {voice}\")\n",
    "        audio, spectragram = infer(ref_audio, ref_text, gen_text, remove_silence, ema_model, vocos)\n",
    "        generated_audio_segments.append(audio)\n",
    "        # Append one second of silence after each audio segment\n",
    "        one_second_silence = np.zeros(int(target_sample_rate * 2.0))\n",
    "        generated_audio_segments.append(one_second_silence)\n",
    "\n",
    "    wave_path = f'{audio_videos_dir}/dialog.wav'\n",
    "    if generated_audio_segments:\n",
    "        final_wave = np.concatenate(generated_audio_segments)\n",
    "        with open(wave_path, \"wb\") as f:\n",
    "            sf.write(f.name, final_wave, target_sample_rate)\n",
    "            # Remove silence\n",
    "            if remove_silence:\n",
    "                aseg = AudioSegment.from_file(f.name)\n",
    "                non_silent_segs = silence.split_on_silence(\n",
    "                    aseg, min_silence_len=1000, silence_thresh=-50, keep_silence=500\n",
    "                )\n",
    "                non_silent_wave = AudioSegment.silent(duration=0)\n",
    "                for non_silent_seg in non_silent_segs:\n",
    "                    non_silent_wave += non_silent_seg\n",
    "                aseg = non_silent_wave\n",
    "                aseg.export(f.name, format=\"wav\")\n",
    "            print(f.name)\n",
    "\n",
    "def synthesize_audio_from_scenes(scenes, audio_videos_dir):\n",
    "    text_to_synthesize = \"\"\n",
    "    scene_count = 0\n",
    "    for scene in scenes:\n",
    "        if scene_count > 0:\n",
    "            text_to_synthesize = text_to_synthesize + f\"[{scene['character'].lower()}] {scene['text']} \"\n",
    "        else:\n",
    "            text_to_synthesize = text_to_synthesize + f\"{scene['text']}\"\n",
    "        scene_count = scene_count + 1\n",
    "\n",
    "    # Initialize the template\n",
    "    config = {\n",
    "        \"model\": \"F5-TTS\",\n",
    "        \"ref_audio\": \"/mnt/d/data/audio/intro.mp3\",\n",
    "        \"ref_text\": \"\",\n",
    "        \"gen_text\": \"\",\n",
    "        \"remove_silence\": True,  # Corrected 'true' to 'True'\n",
    "        \"output_dir\": \"samples\",\n",
    "        \"voices\": {}\n",
    "    }\n",
    "    \n",
    "    # Create voices entries from unique characters\n",
    "    for character in unique_characters_list:\n",
    "        directory = f'/mnt/d/data/audio/characters/{character.lower()}'\n",
    "        \n",
    "        # Check if the directory exists\n",
    "        if not os.path.isdir(directory):\n",
    "            print(f\"Directory does not exist for character '{character}'. Picking a random existing directory.\")\n",
    "            directory = get_random_existing_directory('/mnt/d/data/audio/characters/')\n",
    "            if not directory:\n",
    "                print(f\"No existing directories found. Skipping character {character.lower()}.\")\n",
    "                continue  # Skip to the next character if no directories are found\n",
    "    \n",
    "        first_audio_file = get_first_file_in_directory(directory)\n",
    "        \n",
    "        if first_audio_file:\n",
    "            config['voices'][character.lower()] = {\n",
    "                \"ref_audio\": first_audio_file,\n",
    "                \"ref_text\": \"\"\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No audio file found in directory: {directory} for character: '{character}'\")\n",
    "    \n",
    "    # Convert the config dictionary to a JSON string\n",
    "    config_json = json.dumps(config, indent=4)\n",
    "\n",
    "    # Convert the string to a dictionary\n",
    "    config = json.loads(config_json)\n",
    "    \n",
    "    ref_audio = config[\"ref_audio\"]\n",
    "    ref_text = config[\"ref_text\"]\n",
    "    gen_text = config[\"gen_text\"]\n",
    "    #gen_text = '''A Town Mouse and a Country Mouse were acquaintances, and the Country Mouse one day invited his friend to come and see him at his home in the fields. The Town Mouse came, and they sat down to a dinner of barleycorns and roots, the latter of which had a distinctly earthy flavour. The fare was not much to the taste of the guest, and presently he broke out with [town] “My poor dear friend, you live here no better than the ants. Now, you should just see how I fare! My larder is a regular horn of plenty. You must come and stay with me, and I promise you you shall live on the fat of the land.” [main] So when he returned to town he took the Country Mouse with him, and showed him into a larder containing flour and oatmeal and figs and honey and dates. The Country Mouse had never seen anything like it, and sat down to enjoy the luxuries his friend provided: but before they had well begun, the door of the larder opened and someone came in. The two Mice scampered off and hid themselves in a narrow and exceedingly uncomfortable hole. Presently, when all was quiet, they ventured out again; but someone else came in, and off they scuttled again. This was too much for the visitor. [country] “Goodbye,” [main] said he, [country] “I’m off. You live in the lap of luxury, I can see, but you are surrounded by dangers; whereas at home I can enjoy my simple dinner of roots and corn in peace.\"'''\n",
    "    gen_text = text_to_synthesize\n",
    "    output_dir = config[\"output_dir\"]\n",
    "    model = config[\"model\"]\n",
    "    ckpt_file = \"\"\n",
    "    vocab_file = \"\"\n",
    "    remove_silence = config[\"remove_silence\"]\n",
    "    wave_path = Path(output_dir)/\"out.wav\"\n",
    "    spectrogram_path = Path(output_dir)/\"out.png\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the vocos model once\n",
    "    vocos = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\")\n",
    "\n",
    "    # Load models outside of functions to prevent multiple loading\n",
    "    F5TTS_model_cfg = dict(\n",
    "        dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4\n",
    "    )\n",
    "    E2TTS_model_cfg = dict(dim=1024, depth=24, heads=16, ff_mult=4)\n",
    "    \n",
    "    # Load the TTS model once\n",
    "    if model == \"F5-TTS\":\n",
    "        if ckpt_file == \"\":\n",
    "            repo_name = \"F5-TTS\"\n",
    "            exp_name = \"F5TTS_Base\"\n",
    "            ckpt_step = 1200000\n",
    "            ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\n",
    "    \n",
    "        ema_model = load_model(DiT, F5TTS_model_cfg, ckpt_file, vocab_file)\n",
    "    elif model == \"E2-TTS\":\n",
    "        if ckpt_file == \"\":\n",
    "            repo_name = \"E2-TTS\"\n",
    "            exp_name = \"E2TTS_Base\"\n",
    "            ckpt_step = 1200000\n",
    "            ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\n",
    "    \n",
    "        ema_model = load_model(UNetT, E2TTS_model_cfg, ckpt_file, vocab_file)\n",
    "\n",
    "    # Now call process with the loaded ema_model\n",
    "    process(ref_audio, ref_text, gen_text, remove_silence, ema_model, config, vocos, wave_path)\n",
    "    del vocos\n",
    "    del ema_model\n",
    "    reset_memory(device)\n",
    "\n",
    "    return wave_path\n",
    "\n",
    "def get_random_existing_directory(base_directory):\n",
    "    \"\"\"\n",
    "    Returns a random existing directory path within the base_directory.\n",
    "    \"\"\"\n",
    "    # Get a list of all directories within base_directory\n",
    "    dir_list = [\n",
    "        os.path.join(base_directory, d)\n",
    "        for d in os.listdir(base_directory)\n",
    "        if os.path.isdir(os.path.join(base_directory, d))\n",
    "    ]\n",
    "    if not dir_list:\n",
    "        return None  # Handle the case where no directories are found\n",
    "    return random.choice(dir_list)\n",
    "\n",
    "def get_whisper_pipe():\n",
    "    model_id = \"openai/whisper-large-v3-turbo\"  # Updated model ID if necessary\n",
    "\n",
    "    # Create the pipeline directly using model_id\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model_id,\n",
    "        chunk_length_s=30,\n",
    "        batch_size=16,  # Adjust based on your hardware capabilities\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac58b8a-a5ca-436a-aad3-8d8c15da3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_scene = False\n",
    "number_of_generations = 1\n",
    "for goal in CONFIG[\"goals\"]:\n",
    "    for i in range(number_of_generations):\n",
    "        if load_scene:\n",
    "            with open(scenes_file_path, \"r\") as scenes_file:\n",
    "                scenes = json.load(scenes_file)\n",
    "        else:\n",
    "            scenes = create_scenes(CONFIG, goal)\n",
    "        unique_characters = set([entry['character'] for entry in scenes])\n",
    "        unique_characters_list = list(unique_characters)\n",
    "        print(f'unique characters: {unique_characters_list}')\n",
    "        # Extract unique characters with their emotions\n",
    "        #unique_characters_with_emotions = {(entry['character'], entry['emotion']) for entry in scenes}\n",
    "        #unique_characters_with_emotions_list = list(unique_characters_with_emotions)\n",
    "        # Synthesize starting images for scenes\n",
    "        config, scenes, audio_images_dir, audio_videos_dir = process_all_audios(scenes, CONFIG)\n",
    "        # save scene in output directory\n",
    "        with open(f'{audio_videos_dir}/scenes.json', \"w\") as scenes_file:\n",
    "            json.dump(scenes, scenes_file)\n",
    "        # Synthesize dialog for scenes\n",
    "        synthesize_audio_from_scenes(scenes, audio_videos_dir)\n",
    "        # Synthesize video for scenes\n",
    "        process_audio_video(config, scenes, audio_images_dir, audio_videos_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866bc27b-b1a6-45dd-907d-5572cca64a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
