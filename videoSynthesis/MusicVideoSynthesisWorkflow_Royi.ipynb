{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f87d72-5305-4778-ba0c-31549333c8db",
   "metadata": {},
   "source": [
    "# Music Video Synthesis\n",
    "* Extract lyrics from song with timestamps\n",
    "* Compose scenes, include timestamps\n",
    "* Construct video text prompt for each scene\n",
    "* Build videos for each scene\n",
    "* Stitch together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b074-e32d-45f2-977f-c923878625e6",
   "metadata": {},
   "source": [
    "# We will use openai whipser for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45c210-fd2b-4381-9fd3-c1eb18feefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt install ffmpeg\n",
    "#!pip install --quiet --upgrade pip\n",
    "#!pip3 install torch torchvision torchaudio optimum-quanto torchao xformers\n",
    "#!pip install --quiet --upgrade openai-whisper openai\n",
    "# Ubuntu or Debian\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!pip install setuptools-rust\n",
    "#!pip install -U diffusers imageio imageio_ffmpeg opencv-python moviepy transformers huggingface-hub optimum pillow safetensors\n",
    "#!pip install git+https://github.com/xhinker/sd_embed.git@main\n",
    "#!pip install accelerate flash_attention numba -U\n",
    "#!pip install flash_attn --no-build-isolation\n",
    "#!pip install -r requirements.txt -U\n",
    "#!pip install numpy==1.26.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537e766-eab2-4757-b6f9-fbac4da44930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import diffusers\n",
    "import gc\n",
    "import imageio\n",
    "import imageio_ffmpeg\n",
    "import json\n",
    "import math\n",
    "import moviepy as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import whisper\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from diffusers import AutoencoderKL, AutoPipelineForText2Image\n",
    "from diffusers import FlowMatchEulerDiscreteScheduler\n",
    "from diffusers import EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, DPMSolverMultistepScheduler, PNDMScheduler, DDIMScheduler\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel\n",
    "from diffusers.utils import export_to_video, load_video, load_image\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from numba import cuda\n",
    "from omegaconf import OmegaConf\n",
    "from openai import OpenAI\n",
    "from optimum.quanto import freeze, qfloat8, quantize, requantize\n",
    "from PIL import Image\n",
    "from ruyi.data.bucket_sampler import ASPECT_RATIO_512, get_closest_ratio\n",
    "from ruyi.models.autoencoder_magvit import AutoencoderKLMagvit\n",
    "from ruyi.models.transformer3d import HunyuanTransformer3DModel\n",
    "from ruyi.pipeline.pipeline_ruyi_inpaint import RuyiInpaintPipeline\n",
    "from ruyi.utils.lora_utils import merge_lora, unmerge_lora\n",
    "from ruyi.utils.utils import get_image_to_video_latent, save_videos_grid\n",
    "from safetensors.torch import load_file as load_safetensors, save_file as save_safetensors\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_flux1\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5TokenizerFast, T5EncoderModel\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Define the paths where quantized weights will be saved\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "retry_limit = 3\n",
    "quantization = int8_weight_only\n",
    "\n",
    "WIDTH=1920\n",
    "HEIGHT=1080\n",
    "\n",
    "# Video settings\n",
    "video_length        = 120       # The max video length is 120 frames (24 frames per second)\n",
    "base_resolution     = 512       # # The pixels in the generated video are approximately 512 x 512. Values in the range of [384, 896] typically produce good video quality.\n",
    "video_size          = None      # Override base_resolution. Format: [height, width], e.g., [384, 672]\n",
    "# Control settings\n",
    "aspect_ratio        = \"16:9\"    # Do not change, currently \"16:9\" works better\n",
    "motion              = \"auto\"    # Motion control, choose in [\"1\", \"2\", \"3\", \"4\", \"auto\"]\n",
    "camera_direction    = \"auto\"    # Camera control, choose in [\"static\", \"left\", \"right\", \"up\", \"down\", \"auto\"]\n",
    "# Sampler settings\n",
    "steps               = 25\n",
    "cfg                 = 7.0\n",
    "scheduler_name      = \"DDIM\"    # Choose in [\"Euler\", \"Euler A\", \"DPM++\", \"PNDM\",\"DDIM\"]\n",
    "\n",
    "# GPU memory settings\n",
    "low_gpu_memory_mode = False     # Low gpu memory mode\n",
    "gpu_offload_steps   = 5         # Choose in [0, 10, 7, 5, 1], the latter number requires less GPU memory but longer time\n",
    "\n",
    "# Model settings\n",
    "config_path         = \"config/default.yaml\"\n",
    "model_name          = \"Ruyi-Mini-7B\"\n",
    "model_type          = \"Inpaint\"\n",
    "model_path          = f\"models/{model_name}\"    # (Down)load mode in this path\n",
    "auto_download       = True                      # Automatically download the model if the pipeline creation fails\n",
    "auto_update         = True                      # If auto_download is enabled, check for updates and update the model if necessary\n",
    "\n",
    "# LoRA settings\n",
    "lora_path           = None\n",
    "lora_weight         = 1.0\n",
    "\n",
    "# Prepare LoRA config\n",
    "loras = {\n",
    "    'models': [lora_path] if lora_path is not None else [],\n",
    "    'weights': [lora_weight] if lora_path is not None else [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54ba32-2208-45c8-8ed2-5fcb1e79aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"openai_api_key\": \"\",\n",
    "    \"openai_model\": \"gpt-4o-mini\",\n",
    "    \"openai_model_large\": \"gpt-4o\",\n",
    "    \"hf_token\": \"\",\n",
    "    \"base_working_dir\": \"./images\",\n",
    "    \"base_video_dir\": \"./output\",\n",
    "    \"audio_files\": [\n",
    "        \"/mnt/d/Share/Audio/Bells.mp3\",\n",
    "        \"/mnt/d/Share/Audio/Bells.mp3\",\n",
    "    ],\n",
    "    \"device\": device,\n",
    "    \"dtype\": dtype,\n",
    "    \"retry_limit\": retry_limit,\n",
    "    \"MAX_SEED\": MAX_SEED,\n",
    "}\n",
    "\n",
    "# Ensure base directories exist\n",
    "os.makedirs(CONFIG[\"base_working_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"base_video_dir\"], exist_ok=True)\n",
    "\n",
    "# Load config\n",
    "config_royi = OmegaConf.load(config_path)\n",
    "\n",
    "# Check for update\n",
    "repo_id = f\"IamCreateAI/{model_name}\"\n",
    "if auto_download and auto_update:\n",
    "    print(f\"Checking for {model_name} updates ...\")\n",
    "\n",
    "    # Download the model\n",
    "    snapshot_download(repo_id=repo_id, local_dir=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34281c1-e231-4472-9d9b-096fa23b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_memory(device):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    \n",
    "def get_openai_prompt_response(\n",
    "    prompt: str,\n",
    "    config: dict,\n",
    "    max_tokens: int = 6000,\n",
    "    temperature: float = 0.33,\n",
    "    openai_model: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response with retry logic.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Act as a helpful assistant, you are an expert editor.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=openai_model or config[\"openai_model\"],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < config[\"retry_limit\"]:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            return message_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == config[\"retry_limit\"]:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{config['retry_limit']})\")\n",
    "                time.sleep(1)  # Optional: wait before retrying\n",
    "\n",
    "\n",
    "def load_flux_pipe():\n",
    "    bfl_repo = \"black-forest-labs/FLUX.1-dev\"\n",
    "    revision = \"refs/pr/3\"\n",
    "    adapter_id = \"alimama-creative/FLUX.1-Turbo-Alpha\"\n",
    "\n",
    "    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(bfl_repo, subfolder=\"scheduler\", revision=revision)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "    text_encoder_2 = T5EncoderModel.from_pretrained(bfl_repo, subfolder=\"text_encoder_2\", torch_dtype=dtype, revision=revision)\n",
    "    tokenizer_2 = T5TokenizerFast.from_pretrained(bfl_repo, subfolder=\"tokenizer_2\", torch_dtype=dtype, revision=revision)\n",
    "    vae = AutoencoderKL.from_pretrained(bfl_repo, subfolder=\"vae\", torch_dtype=dtype, revision=revision)\n",
    "    transformer = FluxTransformer2DModel.from_pretrained(bfl_repo, subfolder=\"transformer\", torch_dtype=dtype, revision=revision)\n",
    "    \n",
    "    quantize_(transformer, quantization())\n",
    "    quantize_(text_encoder_2, quantization())\n",
    "    pipe = FluxPipeline(\n",
    "        scheduler=scheduler,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        text_encoder_2=text_encoder_2,\n",
    "        tokenizer_2=tokenizer_2,\n",
    "        vae=vae,\n",
    "        transformer=transformer,\n",
    "    )\n",
    "\n",
    "    pipe = pipe.to('cuda')\n",
    "    pipe.load_lora_weights(adapter_id)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def gen_flux_image(pipe, prompt, config: dict, height=1080, width=1920, guidance_scale=3.5, num_inference_steps=8, max_sequence_length=512, seed=-1):\n",
    "    \"\"\"\n",
    "    Generates an image based on the provided prompt using the Flux pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds, pooled_prompt_embeds = get_weighted_text_embeddings_flux1(\n",
    "            pipe        = pipe,\n",
    "            prompt    = prompt\n",
    "        )\n",
    "        \n",
    "        image = pipe(\n",
    "            prompt_embeds               = prompt_embeds,\n",
    "            pooled_prompt_embeds      = pooled_prompt_embeds,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            guidance_scale=guidance_scale,\n",
    "            output_type=\"pil\",\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "        ).images[0]\n",
    "\n",
    "        return image\n",
    "\n",
    "def get_control_embeddings(pipeline, aspect_ratio, motion, camera_direction):\n",
    "    # Default keys\n",
    "    p_default_key = \"p.default\"\n",
    "    n_default_key = \"n.default\"\n",
    "\n",
    "    # Load embeddings\n",
    "    if motion == \"auto\":\n",
    "        motion = \"0\"\n",
    "    p_key = f\"p.{aspect_ratio.replace(':', 'x')}movie{motion}{camera_direction}\"\n",
    "    embeddings = pipeline.embeddings\n",
    "\n",
    "    # Get embeddings\n",
    "    positive_embeds = embeddings.get(f\"{p_key}.emb1\", embeddings[f\"{p_default_key}.emb1\"])\n",
    "    positive_attention_mask = embeddings.get(f\"{p_key}.mask1\", embeddings[f\"{p_default_key}.mask1\"])\n",
    "    positive_embeds_2 = embeddings.get(f\"{p_key}.emb2\", embeddings[f\"{p_default_key}.emb2\"])\n",
    "    positive_attention_mask_2 = embeddings.get(f\"{p_key}.mask2\", embeddings[f\"{p_default_key}.mask2\"])\n",
    "\n",
    "    negative_embeds = embeddings[f\"{n_default_key}.emb1\"]\n",
    "    negative_attention_mask = embeddings[f\"{n_default_key}.mask1\"]\n",
    "    negative_embeds_2 = embeddings[f\"{n_default_key}.emb2\"]\n",
    "    negative_attention_mask_2 = embeddings[f\"{n_default_key}.mask2\"]\n",
    "\n",
    "    return {\n",
    "        \"positive_embeds\": positive_embeds,\n",
    "        \"positive_attention_mask\": positive_attention_mask,\n",
    "        \"positive_embeds_2\": positive_embeds_2,\n",
    "        \"positive_attention_mask_2\": positive_attention_mask_2,\n",
    "\n",
    "        \"negative_embeds\": negative_embeds,\n",
    "        \"negative_attention_mask\": negative_attention_mask,\n",
    "        \"negative_embeds_2\": negative_embeds_2,\n",
    "        \"negative_attention_mask_2\": negative_attention_mask_2,\n",
    "    }\n",
    "    \n",
    "def load_video_pipeline(model_path, weight_dtype, config):\n",
    "    try:\n",
    "        # Get Vae\n",
    "        vae = AutoencoderKLMagvit.from_pretrained(\n",
    "            model_path, \n",
    "            subfolder=\"vae\"\n",
    "        ).to(weight_dtype)\n",
    "        print(\"Vae loaded ...\")\n",
    "\n",
    "        # Get Transformer\n",
    "        transformer_additional_kwargs = OmegaConf.to_container(config['transformer_additional_kwargs'])\n",
    "        transformer = HunyuanTransformer3DModel.from_pretrained_2d(\n",
    "            model_path, \n",
    "            subfolder=\"transformer\",\n",
    "            transformer_additional_kwargs=transformer_additional_kwargs\n",
    "        ).to(weight_dtype)\n",
    "        print(\"Transformer loaded ...\")\n",
    "\n",
    "        # Load Clip\n",
    "        clip_image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "            model_path, subfolder=\"image_encoder\"\n",
    "        ).to(weight_dtype)\n",
    "        clip_image_processor = CLIPImageProcessor.from_pretrained(\n",
    "            model_path, subfolder=\"image_encoder\"\n",
    "        )\n",
    "\n",
    "        # Load sampler and create pipeline\n",
    "        Choosen_Scheduler = DDIMScheduler\n",
    "        scheduler = Choosen_Scheduler.from_pretrained(\n",
    "            model_path, \n",
    "            subfolder=\"scheduler\"\n",
    "        )\n",
    "        pipeline = RuyiInpaintPipeline.from_pretrained(\n",
    "            model_path,\n",
    "            vae=vae,\n",
    "            transformer=transformer,\n",
    "            scheduler=scheduler,\n",
    "            torch_dtype=weight_dtype,\n",
    "            clip_image_encoder=clip_image_encoder,\n",
    "            clip_image_processor=clip_image_processor,\n",
    "        )\n",
    "\n",
    "        # Load embeddings\n",
    "        embeddings = load_safetensors(os.path.join(model_path, \"embeddings.safetensors\"))\n",
    "        pipeline.embeddings = embeddings\n",
    "        print(\"Pipeline loaded ...\")\n",
    "\n",
    "        # Setup GPU memory mode\n",
    "        if low_gpu_memory_mode:\n",
    "            pipeline.enable_sequential_cpu_offload()\n",
    "        else:\n",
    "            pipeline.enable_model_cpu_offload()\n",
    "\n",
    "        # Set hidden states offload steps\n",
    "        pipeline.transformer.hidden_cache_size = gpu_offload_steps\n",
    "        \n",
    "        # Load Sampler\n",
    "        if scheduler_name == \"DPM++\":\n",
    "            noise_scheduler = DPMSolverMultistepScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "        elif scheduler_name == \"Euler\":\n",
    "            noise_scheduler = EulerDiscreteScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "        elif scheduler_name == \"Euler A\":\n",
    "            noise_scheduler = EulerAncestralDiscreteScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "        elif scheduler_name == \"PNDM\":\n",
    "            noise_scheduler = PNDMScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "        elif scheduler_name == \"DDIM\":\n",
    "            noise_scheduler = DDIMScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "        pipeline.scheduler = noise_scheduler\n",
    "        return pipeline\n",
    "    except Exception as e:\n",
    "        print(\"[Ruyi] Setup pipeline failed:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_video(video_pipe, embeddings, start_img, end_img, config, seed: int = -1, video_filename: str = \"test.mp4\", video_length: int = 120):\n",
    "    # Set random seed\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "    generator= torch.Generator(device).manual_seed(seed)\n",
    "\n",
    "    # Count most suitable height and width\n",
    "    if video_size is None:\n",
    "        aspect_ratio_sample_size = {key : [x / 512 * base_resolution for x in ASPECT_RATIO_512[key]] for key in ASPECT_RATIO_512.keys()}\n",
    "        original_width, original_height = start_img[0].size if type(start_img) is list else Image.open(start_img).size\n",
    "        closest_size, closest_ratio = get_closest_ratio(original_height, original_width, ratios=aspect_ratio_sample_size)\n",
    "        height, width = [int(x / 16) * 16 for x in closest_size]\n",
    "    else:\n",
    "        height, width = video_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        video_length = int(video_length // video_pipe.vae.mini_batch_encoder * video_pipe.vae.mini_batch_encoder) if video_length != 1 else 1\n",
    "        input_video, input_video_mask, clip_image = get_image_to_video_latent(start_img, end_img, video_length=video_length, sample_size=(height, width))\n",
    "    \n",
    "        for _lora_path, _lora_weight in zip(loras.get(\"models\", []), loras.get(\"weights\", [])):\n",
    "            video_pipe = merge_lora(video_pipe, _lora_path, _lora_weight)\n",
    "        \n",
    "        sample = video_pipe(\n",
    "            prompt_embeds = embeddings[\"positive_embeds\"],\n",
    "            prompt_attention_mask = embeddings[\"positive_attention_mask\"],\n",
    "            prompt_embeds_2 = embeddings[\"positive_embeds_2\"],\n",
    "            prompt_attention_mask_2 = embeddings[\"positive_attention_mask_2\"],\n",
    "    \n",
    "            negative_prompt_embeds = embeddings[\"negative_embeds\"],\n",
    "            negative_prompt_attention_mask = embeddings[\"negative_attention_mask\"],\n",
    "            negative_prompt_embeds_2 = embeddings[\"negative_embeds_2\"],\n",
    "            negative_prompt_attention_mask_2 = embeddings[\"negative_attention_mask_2\"],\n",
    "    \n",
    "            video_length = video_length,\n",
    "            height      = height,\n",
    "            width       = width,\n",
    "            generator   = generator,\n",
    "            guidance_scale = cfg,\n",
    "            num_inference_steps = steps,\n",
    "    \n",
    "            video        = input_video,\n",
    "            mask_video   = input_video_mask,\n",
    "            clip_image   = clip_image, \n",
    "        ).videos\n",
    "    \n",
    "        for _lora_path, _lora_weight in zip(loras.get(\"models\", []), loras.get(\"weights\", [])):\n",
    "            video_pipe = unmerge_lora(video_pipe, _lora_path, _lora_weight)\n",
    "\n",
    "    save_videos_grid(sample, video_filename, fps=24)\n",
    "\n",
    "def save_video(frames, fps: int, filename: str):\n",
    "    \"\"\"\n",
    "    Saves a list of frames as a video file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        writer = imageio.get_writer(temp_video_path, fps=fps)\n",
    "        for frame in frames:\n",
    "            writer.append_data(np.array(frame))\n",
    "        writer.close()\n",
    "\n",
    "    os.rename(temp_video_path, filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def convert_to_gif(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a video file to a GIF.\n",
    "    \"\"\"\n",
    "    clip = mp.VideoFileClip(video_path)\n",
    "    clip = clip.set_fps(8)\n",
    "    clip = clip.resize(height=240)\n",
    "    gif_path = video_path.replace(\".mp4\", \".gif\")\n",
    "    clip.write_gif(gif_path, fps=8)\n",
    "    return gif_path\n",
    "\n",
    "\n",
    "def resize_if_unfit(input_video: str) -> str:\n",
    "    \"\"\"\n",
    "    Resizes the video to the target dimensions if it does not match.\n",
    "    \"\"\"\n",
    "    width, height = get_video_dimensions(input_video)\n",
    "\n",
    "    if width == 720 and height == 480:\n",
    "        return input_video\n",
    "    else:\n",
    "        return center_crop_resize(input_video)\n",
    "\n",
    "\n",
    "def get_video_dimensions(input_video_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Retrieves the dimensions of the video.\n",
    "    \"\"\"\n",
    "    reader = imageio_ffmpeg.read_frames(input_video_path)\n",
    "    metadata = next(reader)\n",
    "    return metadata[\"size\"]\n",
    "\n",
    "\n",
    "def center_crop_resize(input_video_path: str, target_width: int = 720, target_height: int = 480) -> str:\n",
    "    \"\"\"\n",
    "    Resizes and center-crops the video to the target dimensions.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    width_factor = target_width / orig_width\n",
    "    height_factor = target_height / orig_height\n",
    "    resize_factor = max(width_factor, height_factor)\n",
    "\n",
    "    inter_width = int(orig_width * resize_factor)\n",
    "    inter_height = int(orig_height * resize_factor)\n",
    "\n",
    "    target_fps = 8\n",
    "    ideal_skip = max(0, math.ceil(orig_fps / target_fps) - 1)\n",
    "    skip = min(5, ideal_skip)  # Cap at 5\n",
    "\n",
    "    while (total_frames / (skip + 1)) < 49 and skip > 0:\n",
    "        skip -= 1\n",
    "\n",
    "    processed_frames = []\n",
    "    frame_count = 0\n",
    "    total_read = 0\n",
    "\n",
    "    while frame_count < 49 and total_read < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if total_read % (skip + 1) == 0:\n",
    "            resized = cv2.resize(frame, (inter_width, inter_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            start_x = (inter_width - target_width) // 2\n",
    "            start_y = (inter_height - target_height) // 2\n",
    "            cropped = resized[start_y:start_y + target_height, start_x:start_x + target_width]\n",
    "\n",
    "            processed_frames.append(cropped)\n",
    "            frame_count += 1\n",
    "\n",
    "        total_read += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(temp_video_path, fourcc, target_fps, (target_width, target_height))\n",
    "\n",
    "        for frame in processed_frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    return temp_video_path\n",
    "\n",
    "\n",
    "def extract_last_frame(video_filename: str, output_image_filename: str):\n",
    "    \"\"\"\n",
    "    Extracts the last frame from a video file and saves it as an image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = imageio.get_reader(video_filename, 'ffmpeg')\n",
    "        last_frame = None\n",
    "        for frame in reader:\n",
    "            last_frame = frame\n",
    "        reader.close()\n",
    "\n",
    "        if last_frame is not None:\n",
    "            imageio.imwrite(output_image_filename, last_frame)\n",
    "            print(f\"Last frame saved successfully as '{output_image_filename}'.\")\n",
    "        else:\n",
    "            print(\"The video contains no frames.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{video_filename}' was not found.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except RuntimeError as re:\n",
    "        print(f\"RuntimeError: {re}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def create_scenes(text: str, video_summary: str, config: dict):\n",
    "    \"\"\"\n",
    "    Creates scenes based on the extracted lyrics using OpenAI's API.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Create a json list of diverse, unique scenes (groupings of text), scene_description (200 words or less), and action_sequence (30 words or less) from the following text.  Scenes should be groups of lyrics with new scenes when the lyric context changes.  Text: {text}   \n",
    "The json list should have the start value for the first item in the scene and the text that is combined for all items in the same scene.  \n",
    "The scene_description should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes.  Use theme descriptions, such as graphic novel, photograph, oil painting, etc.\n",
    "The action_sequence should describe the action in the scene.  Scenes should be unique, creative, imaginative, and awe-inspiring to create an amazing video.  Create beautiful and mesmerizing scene descriptions that are creative, unique, artistic, and imaginative. Each scene must be unique, imaginative, and visually captivating, blending creativity with artistic flair. Use powerful, descriptive language to craft scenes that are awe-inspiring and leave the audience in wonder. These scenes should evoke a sense of beauty, grandeur, mystery, or anything emotional, drawing from both realistic and fantastical elements. Ensure the descriptions are immersive, emotionally resonant, and filled with unexpected twists that engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  Action can't have sudden movements or fast zooms, camera movement.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.66)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "def revise_scenes(scenes, config: dict):\n",
    "    \"\"\"\n",
    "    Revise scenes based on the extracted scenes.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Revise the JSON scenes to update the scene_description and action_sequence to engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  JSON scenes: {scenes}   \n",
    "The scene_description (200 words or less) should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes. Use theme descriptions, such as graphic novel, photograph, oil painting, etc.\n",
    "The action_sequence (30 words or less) should describe the action in the scene.  The goal is to create input to create a stunning, cinematic video experience.   Action can't have sudden movements or fast zooms, camera movement.\n",
    "Only update the scene_description and action_sequence.  Do not delete any items as having scenes with the given start times are important.  We do not want to have the same scene_description and action_sequence for the items with repeatitive input text.  Please change these to be creative and consistent with dynamic video sequences.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.33)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def process_audio_scenes(audio_file: str, config: dict):\n",
    "    # set maximum duration for an image basis, should be in intervals of video generation length\n",
    "    video_gen_length = 5\n",
    "    max_duration_seconds  = video_gen_length * 2\n",
    "    \"\"\"\n",
    "    Processes a single audio file through the entire workflow.\n",
    "    \"\"\"\n",
    "    # Create unique identifier based on audio file name\n",
    "    audio_basename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = f\"{audio_basename}_{timestamp}\"\n",
    "\n",
    "    # Create unique directories for images and videos\n",
    "    print(f\"Create unique directories for images and videos\")\n",
    "    audio_images_dir = os.path.join(config[\"base_working_dir\"], unique_id)\n",
    "    audio_videos_dir = os.path.join(config[\"base_video_dir\"], unique_id)\n",
    "    os.makedirs(audio_images_dir, exist_ok=True)\n",
    "    os.makedirs(audio_videos_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Transcribe audio using Whisper\n",
    "    print(f\"Transcribe audio using Whisper\")\n",
    "    model = whisper.load_model(\"turbo\")\n",
    "    result = model.transcribe(audio_file)\n",
    "\n",
    "    # Cleanup Whisper model memory\n",
    "    del model\n",
    "    reset_memory(device)\n",
    "\n",
    "    segments = result['segments']\n",
    "\n",
    "    # Extract list of start times and texts\n",
    "    segment_texts_and_start_times = [(segment['text'].strip(), segment['start']) for segment in segments]\n",
    "\n",
    "    # Combine texts\n",
    "    text = \"\"\n",
    "    for segment_text, start in segment_texts_and_start_times:\n",
    "        text += f\"Start: {start}, Text: {segment_text}\\n\"\n",
    "\n",
    "    last_end_value = segments[-1]['end']\n",
    "\n",
    "    # Path to scenes.json file\n",
    "    scenes_file_path = os.path.join(audio_images_dir, \"scenes.json\")\n",
    "\n",
    "    # Check if scenes.json exists\n",
    "    if os.path.exists(scenes_file_path):\n",
    "        print(f\"Scenes file already exists at {scenes_file_path}. Skipping scene generation.\")\n",
    "        with open(scenes_file_path, \"r\") as scenes_file:\n",
    "            scenes = json.load(scenes_file)\n",
    "        return scenes, audio_images_dir, audio_videos_dir, last_end_value\n",
    "\n",
    "    # Step 2: Generate video summary using OpenAI\n",
    "    print(f\"Generate video summary using OpenAI\")\n",
    "    video_summary_prompt = f'Create a short summary that describes a music video based on these lyrics: {text}'\n",
    "    video_summary = get_openai_prompt_response(video_summary_prompt, config, openai_model=config[\"openai_model\"])\n",
    "\n",
    "    # Step 3: Create scenes based on lyrics\n",
    "    print(f\"Create scenes based on lyrics\")\n",
    "    try:\n",
    "        scenes = create_scenes(text, video_summary, config)\n",
    "    except:\n",
    "        try:\n",
    "            scenes = create_scenes(text, video_summary, config)\n",
    "        except:\n",
    "            try:\n",
    "                scenes = create_scenes(text, video_summary, config)\n",
    "            except: \n",
    "                return \"\", audio_images_dir, audio_videos_dir, last_end_value\n",
    "            \n",
    "    # we don't want scenes longer than 18 seconds\n",
    "    new_scenes = []\n",
    "    for i in range(len(scenes)):\n",
    "        scene = scenes[i]\n",
    "        if i == 0:\n",
    "            start_time = 0\n",
    "        else:\n",
    "            start_time = scene['start']\n",
    "        # Determine the end time\n",
    "        if i < len(scenes) - 1:\n",
    "            end_time = scenes[i + 1]['start']\n",
    "        else:\n",
    "            end_time = last_end_value\n",
    "        duration = end_time - start_time\n",
    "        # Split the scene if duration exceeds 18 seconds\n",
    "        while duration > 18:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "            start_time += max_duration_seconds\n",
    "            duration = end_time - start_time\n",
    "        # Append the remaining part of the scene\n",
    "        if duration > 0:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "    # Replace the original scenes with the new list\n",
    "    scenes = new_scenes\n",
    "    # improve the scenes with a revision\n",
    "    try:\n",
    "        scenes_revised = revise_scenes(scenes, config)\n",
    "        scenes = scenes_revised\n",
    "        print(f'revised scenes')\n",
    "    except:\n",
    "        try:\n",
    "            scenes_revised = revise_scenes(scenes, config)\n",
    "            scenes = scenes_revised\n",
    "            print(f'revised scenes')\n",
    "        except:\n",
    "            print('cannot revise scenes')\n",
    "            \n",
    "    \n",
    "    # Save the scenes to scenes.json\n",
    "    with open(scenes_file_path, \"w\") as scenes_file:\n",
    "        json.dump(scenes, scenes_file)\n",
    "        \n",
    "    return scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def process_audio_images(config: dict, scenes, audio_images_dir):\n",
    "    # Step 4: Load Flux pipeline and generate images\n",
    "    print(f\"Load Flux pipeline and generate images\")\n",
    "    flux_pipe = load_flux_pipe()\n",
    "    height = HEIGHT\n",
    "    width = WIDTH\n",
    "    guidance_scale = 3.9\n",
    "    num_inference_steps = 8\n",
    "    max_sequence_length = 512\n",
    "    seed = -1\n",
    "\n",
    "    # Generate images for each scene\n",
    "    image_num = 1\n",
    "    for scene in scenes:\n",
    "        image_prompt = scene['scene_description']\n",
    "        image = gen_flux_image(flux_pipe, image_prompt, config, height, width, guidance_scale, num_inference_steps, max_sequence_length, seed)\n",
    "        filename = f\"image_{str(image_num).zfill(2)}.jpg\"\n",
    "        image_path = os.path.join(audio_images_dir, filename)\n",
    "        image.save(image_path, dpi=(300, 300))\n",
    "        image_num += 1\n",
    "\n",
    "    # Move the pipeline back to CPU and delete it\n",
    "    flux_pipe.to('cpu')\n",
    "    del flux_pipe\n",
    "    reset_memory(device)\n",
    "    return\n",
    "\n",
    "def process_audio_video(config: dict, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp, video_length):\n",
    "    # Step 6: Load Video Pipeline\n",
    "    print(f\"Load Video Pipeline\")\n",
    "    video_pipe = load_video_pipeline(model_path, dtype, config_royi)\n",
    "\n",
    "    # Load control embeddings\n",
    "    embeddings = get_control_embeddings(video_pipe, aspect_ratio, motion, camera_direction)\n",
    "\n",
    "    # Temporary image path\n",
    "    temp_image = os.path.join(audio_images_dir, \"temp_image.jpg\")\n",
    "    video_num = 1\n",
    "\n",
    "    # Step 7: Generate video sequences\n",
    "    for i, scene in enumerate(scenes):\n",
    "        prompt = scene[\"action_sequence\"]\n",
    "\n",
    "        # Use the initial image for each scene\n",
    "        image_input = os.path.join(audio_images_dir, f\"image_{str(i+1).zfill(2)}.jpg\")\n",
    "\n",
    "        # Calculate duration to keep the video in 6-second increments\n",
    "        if i + 1 < len(scenes):\n",
    "            next_start_time = scenes[i + 1][\"start\"]\n",
    "        else:\n",
    "            next_start_time = last_end_value  # Use the final ending time for the last scene\n",
    "\n",
    "        if i == 0:\n",
    "            duration = next_start_time\n",
    "        else:\n",
    "            duration = next_start_time - scene[\"start\"]\n",
    "        num_video_segments = int((duration + 3) // 6)\n",
    "\n",
    "        print(f\"Scene {i+1} has {num_video_segments} segments\")\n",
    "        for j in range(num_video_segments):\n",
    "            video_name = f\"video_{str(video_num).zfill(2)}_{str(i+1)}_{str(j+1).zfill(2)}_{timestamp}.mp4\"\n",
    "            video_output_path = os.path.join(audio_videos_dir, video_name)\n",
    "             # Load images\n",
    "            start_img = [Image.open(image_input).convert(\"RGB\")]\n",
    "            end_img   = None\n",
    "\n",
    "            generate_video(video_pipe, embeddings, start_img, end_img, config, seed=-1, video_filename=video_output_path, video_length=video_length)\n",
    "            time.sleep(1)  # Pause for 1 second\n",
    "\n",
    "            # After generating the video, extract the last frame to use as input for the next segment\n",
    "            extract_last_frame(video_output_path, temp_image)\n",
    "\n",
    "            # Use the last frame as input for the next video segment in the same scene\n",
    "            image_input = temp_image\n",
    "\n",
    "            video_num += 1  # Increment video number for the next segment\n",
    "\n",
    "    # Move the pipeline back to CPU before deleting\n",
    "    video_pipe.to('cpu')\n",
    "    del video_pipe\n",
    "    reset_memory(device)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def process_all_audios(audio_file, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a list of audio files through the workflow.\n",
    "    \"\"\"\n",
    "    print(f\"Processing audio file: {audio_file}\")\n",
    "    scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_audio_scenes(audio_file, config)\n",
    "    print(f'{len(scenes)} scenes:\\n{json.dumps(scenes, indent=4)}')\n",
    "    print(f'last_end_value: {last_end_value} timestamp: {timestamp}')\n",
    "    # Create starting images for scenes\n",
    "    process_audio_images(config, scenes, audio_images_dir)\n",
    "    return config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def create_video():\n",
    "    config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_all_audios(audio_file, CONFIG)\n",
    "    process_audio_video(config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp, video_length)\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efd2eb-8304-4027-a8c3-eaafe04ade14",
   "metadata": {},
   "source": [
    "### Run new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0cc96-e609-4cd2-9c67-dcd63562988a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run new systems\n",
    "for audio_file in CONFIG[\"audio_files\"]:\n",
    "    create_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f01978-588d-4f2a-910a-68786df670de",
   "metadata": {},
   "source": [
    "### Run previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba626ac1-4c9d-487c-8939-afb86fde6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run saved config\n",
    "scenes_file_path = './images/AlphabetJoy_20241117_152536/scenes.json'\n",
    "audio_images_dir = './images/AlphabetJoy_20241117_152536'\n",
    "audio_videos_dir = './output/AlphabetJoy_20241117_152536'\n",
    "timestamp = '20241117_152536'\n",
    "last_end_value = 199.4\n",
    "\n",
    "with open(scenes_file_path, \"r\") as scenes_file:\n",
    "    scenes = json.load(scenes_file)\n",
    "\n",
    "process_audio_video(CONFIG, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp, video_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a0750-5e16-4713-8ac3-825cb8713762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b3f0a-4844-4c20-9b27-f3f3794de269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
