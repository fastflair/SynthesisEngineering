{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f87d72-5305-4778-ba0c-31549333c8db",
   "metadata": {},
   "source": [
    "# Music Video Synthesis\n",
    "* Extract lyrics from song with timestamps\n",
    "* Compose scenes, include timestamps\n",
    "* Construct video text prompt for each scene\n",
    "* Build videos for each scene\n",
    "* Stitch together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b074-e32d-45f2-977f-c923878625e6",
   "metadata": {},
   "source": [
    "# We will use openai whipser for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b45c210-fd2b-4381-9fd3-c1eb18feefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt install ffmpeg\n",
    "#!pip install --quiet --upgrade pip\n",
    "#!pip3 install torch torchvision torchaudio optimum-quanto torchao xformers\n",
    "#!pip install --quiet --upgrade openai-whisper openai\n",
    "# Ubuntu or Debian\n",
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!pip install setuptools-rust\n",
    "#!pip install -U diffusers imageio imageio_ffmpeg opencv-python moviepy transformers huggingface-hub optimum pillow safetensors\n",
    "#!pip install git+https://github.com/xhinker/sd_embed.git@main\n",
    "#!pip install accelerate flash_attention numba -U\n",
    "#!pip install flash_attn --no-build-isolation\n",
    "#!pip install -r requirements.txt -U\n",
    "#!pip install numpy==1.26.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8537e766-eab2-4757-b6f9-fbac4da44930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 10:39:07.776886: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-27 10:39:07.785673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735317547.796093  596339 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735317547.799037  596339 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-27 10:39:07.809881: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import diffusers\n",
    "import gc\n",
    "import imageio\n",
    "import imageio_ffmpeg\n",
    "import json\n",
    "import math\n",
    "import moviepy as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import whisper\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from diffusers import AutoencoderKL, AutoPipelineForText2Image\n",
    "from diffusers import FlowMatchEulerDiscreteScheduler\n",
    "from diffusers import EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, DPMSolverMultistepScheduler, PNDMScheduler, DDIMScheduler\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel\n",
    "from diffusers.utils import export_to_video, load_video, load_image\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from numba import cuda\n",
    "from omegaconf import OmegaConf\n",
    "from openai import OpenAI\n",
    "from optimum.quanto import freeze, qfloat8, quantize, requantize\n",
    "from PIL import Image\n",
    "from ruyi.data.bucket_sampler import ASPECT_RATIO_512, get_closest_ratio\n",
    "from ruyi.models.autoencoder_magvit import AutoencoderKLMagvit\n",
    "from ruyi.models.transformer3d import HunyuanTransformer3DModel\n",
    "from ruyi.pipeline.pipeline_ruyi_inpaint import RuyiInpaintPipeline\n",
    "from ruyi.utils.lora_utils import merge_lora, unmerge_lora\n",
    "from ruyi.utils.utils import get_image_to_video_latent, save_videos_grid\n",
    "from safetensors.torch import load_file as load_safetensors, save_file as save_safetensors\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_flux1\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5TokenizerFast, T5EncoderModel\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Define the paths where quantized weights will be saved\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "retry_limit = 3\n",
    "quantization = int8_weight_only\n",
    "\n",
    "WIDTH=1360\n",
    "HEIGHT=768\n",
    "\n",
    "# Video settings\n",
    "video_length        = 120       # The max video length is 120 frames (24 frames per second)\n",
    "base_resolution     = 512       # # The pixels in the generated video are approximately 512 x 512. Values in the range of [384, 896] typically produce good video quality.\n",
    "video_size          = None      # Override base_resolution. Format: [height, width], e.g., [384, 672]\n",
    "# Control settings\n",
    "aspect_ratio        = \"16:9\"    # Do not change, currently \"16:9\" works better\n",
    "motion              = \"auto\"    # Motion control, choose in [\"1\", \"2\", \"3\", \"4\", \"auto\"]\n",
    "camera_direction    = \"auto\"    # Camera control, choose in [\"static\", \"left\", \"right\", \"up\", \"down\", \"auto\"]\n",
    "# Sampler settings\n",
    "steps               = 25\n",
    "cfg                 = 7.0\n",
    "scheduler_name      = \"DDIM\"    # Choose in [\"Euler\", \"Euler A\", \"DPM++\", \"PNDM\",\"DDIM\"]\n",
    "\n",
    "# GPU memory settings\n",
    "low_gpu_memory_mode = False     # Low gpu memory mode\n",
    "gpu_offload_steps   = 5         # Choose in [0, 10, 7, 5, 1], the latter number requires less GPU memory but longer time\n",
    "\n",
    "# Model settings\n",
    "config_path         = \"config/default.yaml\"\n",
    "model_name          = \"Ruyi-Mini-7B\"\n",
    "model_type          = \"Inpaint\"\n",
    "model_path          = f\"models/{model_name}\"    # (Down)load mode in this path\n",
    "auto_download       = True                      # Automatically download the model if the pipeline creation fails\n",
    "auto_update         = True                      # If auto_download is enabled, check for updates and update the model if necessary\n",
    "\n",
    "# LoRA settings\n",
    "lora_path           = None\n",
    "lora_weight         = 1.0\n",
    "\n",
    "# Prepare LoRA config\n",
    "loras = {\n",
    "    'models': [lora_path] if lora_path is not None else [],\n",
    "    'weights': [lora_weight] if lora_path is not None else [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f54ba32-2208-45c8-8ed2-5fcb1e79aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for Ruyi-Mini-7B updates ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fe63170f18432697890800e64c4309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"openai_api_key\": \"\",\n",
    "    \"openai_model\": \"gpt-4o-mini\",\n",
    "    \"openai_model_large\": \"gpt-4o\",\n",
    "    \"hf_token\": \"\",\n",
    "    \"base_working_dir\": \"./images\",\n",
    "    \"base_video_dir\": \"./output\",\n",
    "    \"audio_files\": [\n",
    "        \"/mnt/d/Share/Audio/ChristmasOrnaments.mp3\",\n",
    "        \"/mnt/d/Share/Audio/ChristmasOrnaments.mp3\",\n",
    "    ],\n",
    "    \"device\": device,\n",
    "    \"dtype\": dtype,\n",
    "    \"retry_limit\": retry_limit,\n",
    "    \"MAX_SEED\": MAX_SEED,\n",
    "}\n",
    "\n",
    "# Ensure base directories exist\n",
    "os.makedirs(CONFIG[\"base_working_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"base_video_dir\"], exist_ok=True)\n",
    "\n",
    "# Load config\n",
    "config_royi = OmegaConf.load(config_path)\n",
    "\n",
    "# Check for update\n",
    "repo_id = f\"IamCreateAI/{model_name}\"\n",
    "if auto_download and auto_update:\n",
    "    print(f\"Checking for {model_name} updates ...\")\n",
    "\n",
    "    # Download the model\n",
    "    snapshot_download(repo_id=repo_id, local_dir=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34281c1-e231-4472-9d9b-096fa23b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_memory(device):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    \n",
    "def get_openai_prompt_response(\n",
    "    prompt: str,\n",
    "    config: dict,\n",
    "    max_tokens: int = 6000,\n",
    "    temperature: float = 0.33,\n",
    "    openai_model: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends a prompt to OpenAI's API and retrieves the response with retry logic.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Act as a helpful assistant, you are an expert editor.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=openai_model or config[\"openai_model\"],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    while retry_count < config[\"retry_limit\"]:\n",
    "        try:\n",
    "            message_content = response.choices[0].message.content\n",
    "            return message_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            if retry_count == config[\"retry_limit\"]:\n",
    "                print(\"Retry limit reached. Moving to the next iteration.\")\n",
    "                return \"\"\n",
    "            else:\n",
    "                print(f\"Retrying... (Attempt {retry_count}/{config['retry_limit']})\")\n",
    "                time.sleep(1)  # Optional: wait before retrying\n",
    "\n",
    "\n",
    "def load_flux_pipe():\n",
    "    bfl_repo = \"black-forest-labs/FLUX.1-dev\"\n",
    "    revision = \"refs/pr/3\"\n",
    "    adapter_id = \"alimama-creative/FLUX.1-Turbo-Alpha\"\n",
    "\n",
    "    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(bfl_repo, subfolder=\"scheduler\", revision=revision)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=dtype)\n",
    "    text_encoder_2 = T5EncoderModel.from_pretrained(bfl_repo, subfolder=\"text_encoder_2\", torch_dtype=dtype, revision=revision)\n",
    "    tokenizer_2 = T5TokenizerFast.from_pretrained(bfl_repo, subfolder=\"tokenizer_2\", torch_dtype=dtype, revision=revision)\n",
    "    vae = AutoencoderKL.from_pretrained(bfl_repo, subfolder=\"vae\", torch_dtype=dtype, revision=revision)\n",
    "    transformer = FluxTransformer2DModel.from_pretrained(bfl_repo, subfolder=\"transformer\", torch_dtype=dtype, revision=revision)\n",
    "    \n",
    "    quantize_(transformer, quantization())\n",
    "    quantize_(text_encoder_2, quantization())\n",
    "    pipe = FluxPipeline(\n",
    "        scheduler=scheduler,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        text_encoder_2=text_encoder_2,\n",
    "        tokenizer_2=tokenizer_2,\n",
    "        vae=vae,\n",
    "        transformer=transformer,\n",
    "    )\n",
    "\n",
    "    pipe = pipe.to('cuda')\n",
    "    pipe.load_lora_weights(adapter_id)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def gen_flux_image(pipe, prompt, config: dict, height=1080, width=1920, guidance_scale=3.5, num_inference_steps=8, max_sequence_length=512, seed=-1):\n",
    "    \"\"\"\n",
    "    Generates an image based on the provided prompt using the Flux pipeline.\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        prompt_embeds, pooled_prompt_embeds = get_weighted_text_embeddings_flux1(\n",
    "            pipe        = pipe,\n",
    "            prompt    = prompt\n",
    "        )\n",
    "        \n",
    "        image = pipe(\n",
    "            prompt_embeds               = prompt_embeds,\n",
    "            pooled_prompt_embeds      = pooled_prompt_embeds,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            guidance_scale=guidance_scale,\n",
    "            output_type=\"pil\",\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            generator=torch.Generator(\"cpu\").manual_seed(seed)\n",
    "        ).images[0]\n",
    "\n",
    "        return image\n",
    "\n",
    "def get_control_embeddings(pipeline, aspect_ratio, motion, camera_direction):\n",
    "    # Default keys\n",
    "    p_default_key = \"p.default\"\n",
    "    n_default_key = \"n.default\"\n",
    "\n",
    "    # Load embeddings\n",
    "    if motion == \"auto\":\n",
    "        motion = \"0\"\n",
    "    p_key = f\"p.{aspect_ratio.replace(':', 'x')}movie{motion}{camera_direction}\"\n",
    "    embeddings = pipeline.embeddings\n",
    "\n",
    "    # Get embeddings\n",
    "    positive_embeds = embeddings.get(f\"{p_key}.emb1\", embeddings[f\"{p_default_key}.emb1\"])\n",
    "    positive_attention_mask = embeddings.get(f\"{p_key}.mask1\", embeddings[f\"{p_default_key}.mask1\"])\n",
    "    positive_embeds_2 = embeddings.get(f\"{p_key}.emb2\", embeddings[f\"{p_default_key}.emb2\"])\n",
    "    positive_attention_mask_2 = embeddings.get(f\"{p_key}.mask2\", embeddings[f\"{p_default_key}.mask2\"])\n",
    "\n",
    "    negative_embeds = embeddings[f\"{n_default_key}.emb1\"]\n",
    "    negative_attention_mask = embeddings[f\"{n_default_key}.mask1\"]\n",
    "    negative_embeds_2 = embeddings[f\"{n_default_key}.emb2\"]\n",
    "    negative_attention_mask_2 = embeddings[f\"{n_default_key}.mask2\"]\n",
    "\n",
    "    return {\n",
    "        \"positive_embeds\": positive_embeds,\n",
    "        \"positive_attention_mask\": positive_attention_mask,\n",
    "        \"positive_embeds_2\": positive_embeds_2,\n",
    "        \"positive_attention_mask_2\": positive_attention_mask_2,\n",
    "\n",
    "        \"negative_embeds\": negative_embeds,\n",
    "        \"negative_attention_mask\": negative_attention_mask,\n",
    "        \"negative_embeds_2\": negative_embeds_2,\n",
    "        \"negative_attention_mask_2\": negative_attention_mask_2,\n",
    "    }\n",
    "    \n",
    "def load_video_pipeline(model_path, weight_dtype, config):\n",
    "    try:\n",
    "        # Get Vae\n",
    "        vae = AutoencoderKLMagvit.from_pretrained(\n",
    "            model_path, \n",
    "            subfolder=\"vae\"\n",
    "        ).to(weight_dtype)\n",
    "        print(\"Vae loaded ...\")\n",
    "\n",
    "        # Get Transformer\n",
    "        transformer_additional_kwargs = OmegaConf.to_container(config['transformer_additional_kwargs'])\n",
    "        transformer = HunyuanTransformer3DModel.from_pretrained_2d(\n",
    "            model_path, \n",
    "            subfolder=\"transformer\",\n",
    "            transformer_additional_kwargs=transformer_additional_kwargs\n",
    "        ).to(weight_dtype)\n",
    "        print(\"Transformer loaded ...\")\n",
    "\n",
    "        # Load Clip\n",
    "        clip_image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "            model_path, subfolder=\"image_encoder\"\n",
    "        ).to(weight_dtype)\n",
    "        clip_image_processor = CLIPImageProcessor.from_pretrained(\n",
    "            model_path, subfolder=\"image_encoder\"\n",
    "        )\n",
    "\n",
    "        # Load sampler and create pipeline\n",
    "        Choosen_Scheduler = DDIMScheduler\n",
    "        scheduler = Choosen_Scheduler.from_pretrained(\n",
    "            model_path, \n",
    "            subfolder=\"scheduler\"\n",
    "        )\n",
    "        pipeline = RuyiInpaintPipeline.from_pretrained(\n",
    "            model_path,\n",
    "            vae=vae,\n",
    "            transformer=transformer,\n",
    "            scheduler=scheduler,\n",
    "            torch_dtype=weight_dtype,\n",
    "            clip_image_encoder=clip_image_encoder,\n",
    "            clip_image_processor=clip_image_processor,\n",
    "        )\n",
    "\n",
    "        # Load embeddings\n",
    "        embeddings = load_safetensors(os.path.join(model_path, \"embeddings.safetensors\"))\n",
    "        pipeline.embeddings = embeddings\n",
    "        print(\"Pipeline loaded ...\")\n",
    "\n",
    "        # Setup GPU memory mode\n",
    "        if low_gpu_memory_mode:\n",
    "            pipeline.enable_sequential_cpu_offload()\n",
    "        else:\n",
    "            pipeline.enable_model_cpu_offload()\n",
    "\n",
    "        # Set hidden states offload steps\n",
    "        pipeline.transformer.hidden_cache_size = gpu_offload_steps\n",
    "        \n",
    "        # Load Sampler\n",
    "        if scheduler_name == \"DPM++\":\n",
    "            noise_scheduler = DPMSolverMultistepScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "        elif scheduler_name == \"Euler\":\n",
    "            noise_scheduler = EulerDiscreteScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "        elif scheduler_name == \"Euler A\":\n",
    "            noise_scheduler = EulerAncestralDiscreteScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "        elif scheduler_name == \"PNDM\":\n",
    "            noise_scheduler = PNDMScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "        elif scheduler_name == \"DDIM\":\n",
    "            noise_scheduler = DDIMScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "        pipeline.scheduler = noise_scheduler\n",
    "        return pipeline\n",
    "    except Exception as e:\n",
    "        print(\"[Ruyi] Setup pipeline failed:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_video(video_pipe, embeddings, start_img, end_img, config, seed: int = -1, video_filename: str = \"test.mp4\", video_length: int = 120):\n",
    "    # Set random seed\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "    generator= torch.Generator(device).manual_seed(seed)\n",
    "\n",
    "    # Count most suitable height and width\n",
    "    if video_size is None:\n",
    "        aspect_ratio_sample_size = {key : [x / 512 * base_resolution for x in ASPECT_RATIO_512[key]] for key in ASPECT_RATIO_512.keys()}\n",
    "        original_width, original_height = start_img[0].size if type(start_img) is list else Image.open(start_img).size\n",
    "        closest_size, closest_ratio = get_closest_ratio(original_height, original_width, ratios=aspect_ratio_sample_size)\n",
    "        height, width = [int(x / 16) * 16 for x in closest_size]\n",
    "    else:\n",
    "        height, width = video_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        video_length = int(video_length // video_pipe.vae.mini_batch_encoder * video_pipe.vae.mini_batch_encoder) if video_length != 1 else 1\n",
    "        input_video, input_video_mask, clip_image = get_image_to_video_latent(start_img, end_img, video_length=video_length, sample_size=(height, width))\n",
    "    \n",
    "        for _lora_path, _lora_weight in zip(loras.get(\"models\", []), loras.get(\"weights\", [])):\n",
    "            video_pipe = merge_lora(video_pipe, _lora_path, _lora_weight)\n",
    "        \n",
    "        sample = video_pipe(\n",
    "            prompt_embeds = embeddings[\"positive_embeds\"],\n",
    "            prompt_attention_mask = embeddings[\"positive_attention_mask\"],\n",
    "            prompt_embeds_2 = embeddings[\"positive_embeds_2\"],\n",
    "            prompt_attention_mask_2 = embeddings[\"positive_attention_mask_2\"],\n",
    "    \n",
    "            negative_prompt_embeds = embeddings[\"negative_embeds\"],\n",
    "            negative_prompt_attention_mask = embeddings[\"negative_attention_mask\"],\n",
    "            negative_prompt_embeds_2 = embeddings[\"negative_embeds_2\"],\n",
    "            negative_prompt_attention_mask_2 = embeddings[\"negative_attention_mask_2\"],\n",
    "    \n",
    "            video_length = video_length,\n",
    "            height      = height,\n",
    "            width       = width,\n",
    "            generator   = generator,\n",
    "            guidance_scale = cfg,\n",
    "            num_inference_steps = steps,\n",
    "    \n",
    "            video        = input_video,\n",
    "            mask_video   = input_video_mask,\n",
    "            clip_image   = clip_image, \n",
    "        ).videos\n",
    "    \n",
    "        for _lora_path, _lora_weight in zip(loras.get(\"models\", []), loras.get(\"weights\", [])):\n",
    "            video_pipe = unmerge_lora(video_pipe, _lora_path, _lora_weight)\n",
    "\n",
    "    save_videos_grid(sample, video_filename, fps=24)\n",
    "\n",
    "def save_video(frames, fps: int, filename: str):\n",
    "    \"\"\"\n",
    "    Saves a list of frames as a video file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        writer = imageio.get_writer(temp_video_path, fps=fps)\n",
    "        for frame in frames:\n",
    "            writer.append_data(np.array(frame))\n",
    "        writer.close()\n",
    "\n",
    "    os.rename(temp_video_path, filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def convert_to_gif(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a video file to a GIF.\n",
    "    \"\"\"\n",
    "    clip = mp.VideoFileClip(video_path)\n",
    "    clip = clip.set_fps(8)\n",
    "    clip = clip.resize(height=240)\n",
    "    gif_path = video_path.replace(\".mp4\", \".gif\")\n",
    "    clip.write_gif(gif_path, fps=8)\n",
    "    return gif_path\n",
    "\n",
    "\n",
    "def resize_if_unfit(input_video: str) -> str:\n",
    "    \"\"\"\n",
    "    Resizes the video to the target dimensions if it does not match.\n",
    "    \"\"\"\n",
    "    width, height = get_video_dimensions(input_video)\n",
    "\n",
    "    if width == 720 and height == 480:\n",
    "        return input_video\n",
    "    else:\n",
    "        return center_crop_resize(input_video)\n",
    "\n",
    "\n",
    "def get_video_dimensions(input_video_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Retrieves the dimensions of the video.\n",
    "    \"\"\"\n",
    "    reader = imageio_ffmpeg.read_frames(input_video_path)\n",
    "    metadata = next(reader)\n",
    "    return metadata[\"size\"]\n",
    "\n",
    "\n",
    "def center_crop_resize(input_video_path: str, target_width: int = 720, target_height: int = 480) -> str:\n",
    "    \"\"\"\n",
    "    Resizes and center-crops the video to the target dimensions.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    width_factor = target_width / orig_width\n",
    "    height_factor = target_height / orig_height\n",
    "    resize_factor = max(width_factor, height_factor)\n",
    "\n",
    "    inter_width = int(orig_width * resize_factor)\n",
    "    inter_height = int(orig_height * resize_factor)\n",
    "\n",
    "    target_fps = 8\n",
    "    ideal_skip = max(0, math.ceil(orig_fps / target_fps) - 1)\n",
    "    skip = min(5, ideal_skip)  # Cap at 5\n",
    "\n",
    "    while (total_frames / (skip + 1)) < 49 and skip > 0:\n",
    "        skip -= 1\n",
    "\n",
    "    processed_frames = []\n",
    "    frame_count = 0\n",
    "    total_read = 0\n",
    "\n",
    "    while frame_count < 49 and total_read < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if total_read % (skip + 1) == 0:\n",
    "            resized = cv2.resize(frame, (inter_width, inter_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            start_x = (inter_width - target_width) // 2\n",
    "            start_y = (inter_height - target_height) // 2\n",
    "            cropped = resized[start_y:start_y + target_height, start_x:start_x + target_width]\n",
    "\n",
    "            processed_frames.append(cropped)\n",
    "            frame_count += 1\n",
    "\n",
    "        total_read += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n",
    "        temp_video_path = temp_file.name\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(temp_video_path, fourcc, target_fps, (target_width, target_height))\n",
    "\n",
    "        for frame in processed_frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    return temp_video_path\n",
    "\n",
    "\n",
    "def extract_last_frame(video_filename: str, output_image_filename: str):\n",
    "    \"\"\"\n",
    "    Extracts the last frame from a video file and saves it as an image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = imageio.get_reader(video_filename, 'ffmpeg')\n",
    "        last_frame = None\n",
    "        for frame in reader:\n",
    "            last_frame = frame\n",
    "        reader.close()\n",
    "\n",
    "        if last_frame is not None:\n",
    "            imageio.imwrite(output_image_filename, last_frame)\n",
    "            print(f\"Last frame saved successfully as '{output_image_filename}'.\")\n",
    "        else:\n",
    "            print(\"The video contains no frames.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{video_filename}' was not found.\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "    except RuntimeError as re:\n",
    "        print(f\"RuntimeError: {re}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def create_scenes(text: str, video_summary: str, config: dict):\n",
    "    \"\"\"\n",
    "    Creates scenes based on the extracted lyrics using OpenAI's API.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Create a json list of diverse, unique scenes (groupings of text), scene_description (200 words or less), and action_sequence (30 words or less) from the following text.  Scenes should be groups of lyrics with new scenes when the lyric context changes.  Text: {text}   \n",
    "The json list should have the start value for the first item in the scene and the text that is combined for all items in the same scene.  \n",
    "The scene_description should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes.  Use theme descriptions, such as graphic novel, photograph, oil painting, etc.\n",
    "The action_sequence should describe the action in the scene.  Scenes should be unique, creative, imaginative, and awe-inspiring to create an amazing video.  Create beautiful and mesmerizing scene descriptions that are creative, unique, artistic, and imaginative. Each scene must be unique, imaginative, and visually captivating, blending creativity with artistic flair. Use powerful, descriptive language to craft scenes that are awe-inspiring and leave the audience in wonder. These scenes should evoke a sense of beauty, grandeur, mystery, or anything emotional, drawing from both realistic and fantastical elements. Ensure the descriptions are immersive, emotionally resonant, and filled with unexpected twists that engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  Action can't have sudden movements or fast zooms, camera movement.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.66)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "def revise_scenes(scenes, config: dict):\n",
    "    \"\"\"\n",
    "    Revise scenes based on the extracted scenes.\n",
    "    \"\"\"\n",
    "    # Generate scenes JSON\n",
    "    prompt = f'''Revise the JSON scenes to update the scene_description and action_sequence to engage the senses and imagination, suitable for creating a stunning, cinematic video experience.  Use descriptions of special effects in the scenes.  JSON scenes: {scenes}   \n",
    "The scene_description (200 words or less) should include details such as attire, setting, mood, lighting, and any significant movements or expressions, painting a clear visual scene consistent with the video theme and different from other scenes. Use theme descriptions, such as graphic novel, photograph, oil painting, etc.\n",
    "The action_sequence (30 words or less) should describe the action in the scene.  The goal is to create input to create a stunning, cinematic video experience.   Action can't have sudden movements or fast zooms, camera movement.\n",
    "Only update the scene_description and action_sequence.  Do not delete any items as having scenes with the given start times are important.  We do not want to have the same scene_description and action_sequence for the items with repeatitive input text.  Please change these to be creative and consistent with dynamic video sequences.\n",
    "Return only the json list, less jargon. The json list fields should be: start, text, scene_description, action_sequence'''\n",
    "\n",
    "    result = get_openai_prompt_response(prompt, config, openai_model=config[\"openai_model\"], temperature=0.33)\n",
    "    result = result.replace(\"```\", \"\").replace(\"```json\\n\", \"\").replace(\"json\\n\", \"\").replace(\"\\n\", \"\")\n",
    "    scenes = json.loads(result)\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def process_audio_scenes(audio_file: str, config: dict):\n",
    "    # set maximum duration for an image basis, should be in intervals of video generation length\n",
    "    video_gen_length = 5\n",
    "    max_duration_seconds  = video_gen_length * 2\n",
    "    \"\"\"\n",
    "    Processes a single audio file through the entire workflow.\n",
    "    \"\"\"\n",
    "    # Create unique identifier based on audio file name\n",
    "    audio_basename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = f\"{audio_basename}_{timestamp}\"\n",
    "\n",
    "    # Create unique directories for images and videos\n",
    "    print(f\"Create unique directories for images and videos\")\n",
    "    audio_images_dir = os.path.join(config[\"base_working_dir\"], unique_id)\n",
    "    audio_videos_dir = os.path.join(config[\"base_video_dir\"], unique_id)\n",
    "    os.makedirs(audio_images_dir, exist_ok=True)\n",
    "    os.makedirs(audio_videos_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Transcribe audio using Whisper\n",
    "    print(f\"Transcribe audio using Whisper\")\n",
    "    model = whisper.load_model(\"turbo\")\n",
    "    result = model.transcribe(audio_file)\n",
    "\n",
    "    # Cleanup Whisper model memory\n",
    "    del model\n",
    "    reset_memory(device)\n",
    "\n",
    "    segments = result['segments']\n",
    "\n",
    "    # Extract list of start times and texts\n",
    "    segment_texts_and_start_times = [(segment['text'].strip(), segment['start']) for segment in segments]\n",
    "\n",
    "    # Combine texts\n",
    "    text = \"\"\n",
    "    for segment_text, start in segment_texts_and_start_times:\n",
    "        text += f\"Start: {start}, Text: {segment_text}\\n\"\n",
    "\n",
    "    last_end_value = segments[-1]['end']\n",
    "\n",
    "    # Path to scenes.json file\n",
    "    scenes_file_path = os.path.join(audio_images_dir, \"scenes.json\")\n",
    "\n",
    "    # Check if scenes.json exists\n",
    "    if os.path.exists(scenes_file_path):\n",
    "        print(f\"Scenes file already exists at {scenes_file_path}. Skipping scene generation.\")\n",
    "        with open(scenes_file_path, \"r\") as scenes_file:\n",
    "            scenes = json.load(scenes_file)\n",
    "        return scenes, audio_images_dir, audio_videos_dir, last_end_value\n",
    "\n",
    "    # Step 2: Generate video summary using OpenAI\n",
    "    print(f\"Generate video summary using OpenAI\")\n",
    "    video_summary_prompt = f'Create a short summary that describes a music video based on these lyrics: {text}'\n",
    "    video_summary = get_openai_prompt_response(video_summary_prompt, config, openai_model=config[\"openai_model\"])\n",
    "\n",
    "    # Step 3: Create scenes based on lyrics\n",
    "    print(f\"Create scenes based on lyrics\")\n",
    "    try:\n",
    "        scenes = create_scenes(text, video_summary, config)\n",
    "    except:\n",
    "        try:\n",
    "            scenes = create_scenes(text, video_summary, config)\n",
    "        except:\n",
    "            try:\n",
    "                scenes = create_scenes(text, video_summary, config)\n",
    "            except: \n",
    "                return \"\", audio_images_dir, audio_videos_dir, last_end_value\n",
    "            \n",
    "    # we don't want scenes longer than 18 seconds\n",
    "    new_scenes = []\n",
    "    for i in range(len(scenes)):\n",
    "        scene = scenes[i]\n",
    "        if i == 0:\n",
    "            start_time = 0\n",
    "        else:\n",
    "            start_time = scene['start']\n",
    "        # Determine the end time\n",
    "        if i < len(scenes) - 1:\n",
    "            end_time = scenes[i + 1]['start']\n",
    "        else:\n",
    "            end_time = last_end_value\n",
    "        duration = end_time - start_time\n",
    "        # Split the scene if duration exceeds 18 seconds\n",
    "        while duration > 18:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "            start_time += max_duration_seconds\n",
    "            duration = end_time - start_time\n",
    "        # Append the remaining part of the scene\n",
    "        if duration > 0:\n",
    "            new_scene = scene.copy()\n",
    "            new_scene['start'] = start_time\n",
    "            new_scenes.append(new_scene)\n",
    "    # Replace the original scenes with the new list\n",
    "    scenes = new_scenes\n",
    "    # improve the scenes with a revision\n",
    "    try:\n",
    "        scenes_revised = revise_scenes(scenes, config)\n",
    "        scenes = scenes_revised\n",
    "        print(f'revised scenes')\n",
    "    except:\n",
    "        try:\n",
    "            scenes_revised = revise_scenes(scenes, config)\n",
    "            scenes = scenes_revised\n",
    "            print(f'revised scenes')\n",
    "        except:\n",
    "            print('cannot revise scenes')\n",
    "            \n",
    "    \n",
    "    # Save the scenes to scenes.json\n",
    "    with open(scenes_file_path, \"w\") as scenes_file:\n",
    "        json.dump(scenes, scenes_file)\n",
    "        \n",
    "    return scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def process_audio_images(config: dict, scenes, audio_images_dir):\n",
    "    # Step 4: Load Flux pipeline and generate images\n",
    "    print(f\"Load Flux pipeline and generate images\")\n",
    "    flux_pipe = load_flux_pipe()\n",
    "    height = HEIGHT\n",
    "    width = WIDTH\n",
    "    guidance_scale = 3.9\n",
    "    num_inference_steps = 8\n",
    "    max_sequence_length = 512\n",
    "    seed = -1\n",
    "\n",
    "    # Generate images for each scene\n",
    "    image_num = 1\n",
    "    for scene in scenes:\n",
    "        image_prompt = scene['scene_description']\n",
    "        image = gen_flux_image(flux_pipe, image_prompt, config, height, width, guidance_scale, num_inference_steps, max_sequence_length, seed)\n",
    "        filename = f\"image_{str(image_num).zfill(2)}.jpg\"\n",
    "        image_path = os.path.join(audio_images_dir, filename)\n",
    "        image.save(image_path, dpi=(300, 300))\n",
    "        image_num += 1\n",
    "\n",
    "    # Move the pipeline back to CPU and delete it\n",
    "    flux_pipe.to('cpu')\n",
    "    del flux_pipe\n",
    "    reset_memory(device)\n",
    "    return\n",
    "\n",
    "def process_audio_video(config: dict, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp, video_length):\n",
    "    # Step 6: Load Video Pipeline\n",
    "    print(f\"Load Video Pipeline\")\n",
    "    video_pipe = load_video_pipeline(model_path, dtype, config_royi)\n",
    "\n",
    "    # Load control embeddings\n",
    "    embeddings = get_control_embeddings(video_pipe, aspect_ratio, motion, camera_direction)\n",
    "\n",
    "    # Temporary image path\n",
    "    temp_image = os.path.join(audio_images_dir, \"temp_image.jpg\")\n",
    "    video_num = 1\n",
    "\n",
    "    # Step 7: Generate video sequences\n",
    "    for i, scene in enumerate(scenes):\n",
    "        prompt = scene[\"action_sequence\"]\n",
    "\n",
    "        # Use the initial image for each scene\n",
    "        image_input = os.path.join(audio_images_dir, f\"image_{str(i+1).zfill(2)}.jpg\")\n",
    "\n",
    "        # Calculate duration to keep the video in 6-second increments\n",
    "        if i + 1 < len(scenes):\n",
    "            next_start_time = scenes[i + 1][\"start\"]\n",
    "        else:\n",
    "            next_start_time = last_end_value  # Use the final ending time for the last scene\n",
    "\n",
    "        if i == 0:\n",
    "            duration = next_start_time\n",
    "        else:\n",
    "            duration = next_start_time - scene[\"start\"]\n",
    "        num_video_segments = int((duration + 3) // 6)\n",
    "\n",
    "        print(f\"Scene {i+1} has {num_video_segments} segments\")\n",
    "        for j in range(num_video_segments):\n",
    "            video_name = f\"video_{str(video_num).zfill(2)}_{str(i+1)}_{str(j+1).zfill(2)}_{timestamp}.mp4\"\n",
    "            video_output_path = os.path.join(audio_videos_dir, video_name)\n",
    "             # Load images\n",
    "            start_img = [Image.open(image_input).convert(\"RGB\")]\n",
    "            end_img   = None\n",
    "\n",
    "            generate_video(video_pipe, embeddings, start_img, end_img, config, seed=-1, video_filename=video_output_path, video_length=video_length)\n",
    "            time.sleep(1)  # Pause for 1 second\n",
    "\n",
    "            # After generating the video, extract the last frame to use as input for the next segment\n",
    "            extract_last_frame(video_output_path, temp_image)\n",
    "\n",
    "            # Use the last frame as input for the next video segment in the same scene\n",
    "            image_input = temp_image\n",
    "\n",
    "            video_num += 1  # Increment video number for the next segment\n",
    "\n",
    "    # Move the pipeline back to CPU before deleting\n",
    "    video_pipe.to('cpu')\n",
    "    del video_pipe\n",
    "    reset_memory(device)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def process_all_audios(audio_file, config: dict):\n",
    "    \"\"\"\n",
    "    Processes a list of audio files through the workflow.\n",
    "    \"\"\"\n",
    "    print(f\"Processing audio file: {audio_file}\")\n",
    "    scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_audio_scenes(audio_file, config)\n",
    "    print(f'{len(scenes)} scenes:\\n{json.dumps(scenes, indent=4)}')\n",
    "    print(f'last_end_value: {last_end_value} timestamp: {timestamp}')\n",
    "    # Create starting images for scenes\n",
    "    process_audio_images(config, scenes, audio_images_dir)\n",
    "    return config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp\n",
    "\n",
    "def create_video():\n",
    "    config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp = process_all_audios(audio_file, CONFIG)\n",
    "    process_audio_video(config, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp, video_length)\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efd2eb-8304-4027-a8c3-eaafe04ade14",
   "metadata": {},
   "source": [
    "### Run new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0cc96-e609-4cd2-9c67-dcd63562988a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file: /mnt/d/Share/Audio/ChristmasOrnaments.mp3\n",
      "Create unique directories for images and videos\n",
      "Transcribe audio using Whisper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silwa/anaconda3/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate video summary using OpenAI\n",
      "Create scenes based on lyrics\n",
      "revised scenes\n",
      "14 scenes:\n",
      "[\n",
      "    {\n",
      "        \"start\": 0,\n",
      "        \"text\": \"Each Christmas past, the story hangs like ornaments on our old pine tree. Your laughter bright like shiny rings, memories of you and me.\",\n",
      "        \"scene_description\": \"A warm, inviting room glows with the soft light of an old pine tree, its branches heavy with colorful ornaments. Children in cozy, festive sweaters play nearby, their laughter ringing like joyful chimes. The air is rich with the scent of pine needles and freshly baked cookies, evoking a sense of nostalgia. Outside, snowflakes drift lazily, creating a serene winter wonderland. The tree, adorned with twinkling lights, stands as a symbol of love and cherished memories.\",\n",
      "        \"action_sequence\": \"Children dance playfully around the tree, their laughter echoing softly, harmonizing with the gentle glow of the lights.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 10,\n",
      "        \"text\": \"Each Christmas past, the story hangs like ornaments on our old pine tree. Your laughter bright like shiny rings, memories of you and me.\",\n",
      "        \"scene_description\": \"A cozy room filled with the soft glow of an old pine tree, its branches adorned with shimmering ornaments. Children in festive attire giggle and play, their laughter a sweet melody in the air. The scent of pine and cinnamon wafts through the room, wrapping everyone in warmth. Outside, snowflakes create a magical scene, framing the window with winter's beauty. The tree stands proudly, a beacon of love and memories.\",\n",
      "        \"action_sequence\": \"Children weave joyfully through the room, their laughter blending with the soft twinkle of the tree lights.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 20,\n",
      "        \"text\": \"Each Christmas past, the story hangs like ornaments on our old pine tree. Your laughter bright like shiny rings, memories of you and me.\",\n",
      "        \"scene_description\": \"A charming room aglow with the soft light of an old pine tree, its branches heavy with colorful ornaments. Children in festive sweaters frolic nearby, their laughter ringing like joyful bells. The air is infused with the comforting scents of pine and cinnamon, evoking cherished memories. Outside, snowflakes drift gently, creating a picturesque winter scene. The tree stands tall, a symbol of love and warmth.\",\n",
      "        \"action_sequence\": \"Children playfully navigate the room, their laughter harmonizing with the gentle glow of the tree lights.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 31.2,\n",
      "        \"text\": \"But now the tree feels a little less bright, no place for you beside the lights. Yet in my heart you're always near, guiding us through each silent year.\",\n",
      "        \"scene_description\": \"A dimly lit room where the tree stands as a silent guardian, its lights flickering softly. Shadows dance across the walls, creating an atmosphere heavy with longing. A single candle flickers, illuminating a solitary ornament that glistens with memories of lost love. The mood is contemplative, wrapped in a blanket of nostalgia, as whispers of the past linger in the air.\",\n",
      "        \"action_sequence\": \"A figure stands still, fingers gently tracing the contours of the ornaments, lost in bittersweet reflection.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 41.2,\n",
      "        \"text\": \"But now the tree feels a little less bright, no place for you beside the lights. Yet in my heart you're always near, guiding us through each silent year.\",\n",
      "        \"scene_description\": \"In a softly lit room, the tree stands as a quiet sentinel, its lights casting gentle shadows. The atmosphere is thick with absence, a single candle flickering nearby, revealing a lone ornament that sparkles with memories. The mood is introspective, enveloped in nostalgia, as echoes of the past float through the air.\",\n",
      "        \"action_sequence\": \"A figure stands quietly, fingers gliding over the ornaments, lost in a sea of cherished memories.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 53.74,\n",
      "        \"text\": \"Kids are laughing tangled in the lights, once hiding cookies in the reindeer sight. Stories told by the fireside glow, I see your smile in the snow.\",\n",
      "        \"scene_description\": \"A lively scene by the fireplace, where children giggle and play, entangled in a colorful web of lights. The fire crackles warmly, casting playful shadows on their joyful faces. Outside, snow blankets the ground, reflecting the warm glow from within. The atmosphere is filled with happiness and love, as families gather to share stories, their laughter mingling with the soothing sounds of the crackling fire.\",\n",
      "        \"action_sequence\": \"Children chase each other gleefully, their laughter filling the room with infectious joy.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 63.74,\n",
      "        \"text\": \"Kids are laughing tangled in the lights, once hiding cookies in the reindeer sight. Stories told by the fireside glow, I see your smile in the snow.\",\n",
      "        \"scene_description\": \"A vibrant scene unfolds by the fireplace, where children giggle, tangled in a colorful array of lights. The fire crackles, casting a warm glow that dances across their delighted faces. Outside, snowflakes fall softly, blanketing the world in white. The atmosphere is alive with joy, as families gather around, sharing stories that fill the air with laughter and love.\",\n",
      "        \"action_sequence\": \"Children playfully chase each other, their laughter harmonizing with the gentle crackle of the fire.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 74.53999999999999,\n",
      "        \"text\": \"Though the tree feels different now, with new ornaments to show. Your spirit's there when you feel it somehow, in every twinkle. Every glow.\",\n",
      "        \"scene_description\": \"The tree, now adorned with new ornaments, glimmers under soft, warm lighting. Each ornament sparkles like a star, symbolizing fresh beginnings. The room hums with a gentle energy, as if the spirit of the past embraces the present. The air vibrates with connection, where old memories intertwine with new ones, creating a beautiful tapestry of love.\",\n",
      "        \"action_sequence\": \"A gentle breeze stirs, causing the ornaments to sway softly, twinkling in a delicate dance.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 84.53999999999999,\n",
      "        \"text\": \"Though the tree feels different now, with new ornaments to show. Your spirit's there when you feel it somehow, in every twinkle. Every glow.\",\n",
      "        \"scene_description\": \"The tree, adorned with new ornaments, radiates under soft lighting. Each ornament glimmers like a star, representing new beginnings. The room is filled with a gentle hum, as if the past's spirit embraces the present. The air is thick with connection, where old memories blend with new, creating a rich tapestry of love.\",\n",
      "        \"action_sequence\": \"A gentle breeze flows through, causing the ornaments to sway softly, twinkling in response.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 96.62,\n",
      "        \"text\": \"Like fragile glass that catches light, our memories keep love alive. Through the dark you are our sight, helping our hearts to thrive.\",\n",
      "        \"scene_description\": \"A serene twilight setting, the room glows with the soft light of candles and tree lights. Shadows stretch across the walls, creating a dreamlike ambiance. The air is thick with emotion, as memories flicker like fragile glass, each reflecting moments of love and joy. The atmosphere invites reflection and remembrance, wrapping everyone in a tender embrace.\",\n",
      "        \"action_sequence\": \"A figure stands, eyes closed, absorbing the warmth of cherished memories that fill the air.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 109.16,\n",
      "        \"text\": \"The tree is full with love and grace. New memories blend with hope. And every ornament I trace, the stories that you told.\",\n",
      "        \"scene_description\": \"A close-up of the tree reveals vibrant ornaments, each a splash of color telling tales of joy and love. The room is bathed in soft, golden light, creating a magical glow. The spirit of hope lingers in the air, as new memories weave into the fabric of the season. The atmosphere is both celebratory and reflective, a beautiful blend of past and present.\",\n",
      "        \"action_sequence\": \"Hands trace the ornaments lovingly, each touch igniting a memory like a gentle spark.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 119.16,\n",
      "        \"text\": \"The tree is full with love and grace. New memories blend with hope. And every ornament I trace, the stories that you told.\",\n",
      "        \"scene_description\": \"A close-up of the tree, each ornament a vibrant splash of color, narrating tales of joy and love. The room glows softly in golden light, creating a magical ambiance. The spirit of hope fills the air, as new memories intertwine with the essence of the season. The atmosphere is both celebratory and reflective, capturing the beauty of past and present.\",\n",
      "        \"action_sequence\": \"Hands glide over the ornaments tenderly, each touch sparking a memory like a gentle whisper.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 128.44,\n",
      "        \"text\": \"As snowflakes dance on this Christmas night, We gather round, our hearts take flight.\",\n",
      "        \"scene_description\": \"An enchanting winter night, outside the world is a swirling dance of snowflakes. Inside, families gather, their faces illuminated by the soft, warm glow of candles and the tree. The mood is joyous, filled with laughter and love, as hearts soar with the spirit of Christmas. The scene resembles a beautiful oil painting, capturing the essence of togetherness and warmth.\",\n",
      "        \"action_sequence\": \"Families embrace, their laughter rising like music, harmonizing with the gentle fall of snow outside.\"\n",
      "    },\n",
      "    {\n",
      "        \"start\": 145.98,\n",
      "        \"text\": \"With love's enduring gentle glee, Our family stars forever be.\",\n",
      "        \"scene_description\": \"A radiant finale, the room glows with love and warmth as family members share tender moments. The tree stands as a beacon of joy, its lights twinkling like stars in the night sky. The atmosphere is filled with a sense of belonging and unity, creating a heartwarming tableau that embodies the true spirit of Christmas.\",\n",
      "        \"action_sequence\": \"Family members join hands, forming a circle of love, their smiles illuminating the room.\"\n",
      "    }\n",
      "]\n",
      "last_end_value: 161.56 timestamp: 20241227_103910\n",
      "Load Flux pipeline and generate images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed12eb99c4448ed9eb656a8cf9221f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5980cbcc4cd2497e8af8877e6d7640da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1214e4a815451b8de72d2807217b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (97 > 77). Running this sequence through the model will result in indexing errors\n",
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee64f26d4ff409fb14e22096b136049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e743eb00620e47e3b3207f401d9a2ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0df95f2265e4bf6adfc31180aeff7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b163f0409a45a99468cf3a9b43962b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b153958cfd474731ac40e58c5ef6a7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f205c2b507546e1bcb07d2c2208719f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1885211a8ce4d55b09dcedb38ecd65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8201d3d3524f7d90f0756a044b4074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09a342ba39b4886ac5eae08e685771a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce16fd0580614e7998a4ec69382d87be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5774c8ed30b749f9ad504df1237300ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192c171fa6e94934995c3b992914ca86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec824fb3164415f9e5523e55017e611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`height` and `width` have to be divisible by 16 but are 1080 and 1920. Dimensions will be resized accordingly\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b487a91c0e4e66958cc9f2372e2d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Video Pipeline\n",
      "Vae loaded ...\n",
      "Transformer loaded ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eeb759b550b40178ec849f9c5426f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline loaded ...\n",
      "Scene 1 has 2 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silwa/anaconda3/lib/python3.11/site-packages/diffusers/models/embeddings.py:987: FutureWarning: `get_2d_sincos_pos_embed` uses `torch` and supports `device`. `from_numpy` is no longer required.  Pass `output_type='pt' to use the new version now.\n",
      "  deprecate(\"output_type=='np'\", \"0.33.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2af2d3ad0e4cccbf8a86a90a5b9977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/ChristmasOrnaments_20241227_103910/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aee3dd64c8d437a962bf54ea36afcb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/ChristmasOrnaments_20241227_103910/temp_image.jpg'.\n",
      "Scene 2 has 2 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29590335dd044be595786c114b2e0766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/ChristmasOrnaments_20241227_103910/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e5571e51d84adf9cba747fa85b8742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/ChristmasOrnaments_20241227_103910/temp_image.jpg'.\n",
      "Scene 3 has 2 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055511dd78774b559c993e0b099fb987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/ChristmasOrnaments_20241227_103910/temp_image.jpg'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9155ad3d8f43ad81002d2ff9565573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last frame saved successfully as './images/ChristmasOrnaments_20241227_103910/temp_image.jpg'.\n",
      "Scene 4 has 2 segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcfb17d85d44d07979d28aca099a1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run new systems\n",
    "for audio_file in CONFIG[\"audio_files\"]:\n",
    "    create_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f01978-588d-4f2a-910a-68786df670de",
   "metadata": {},
   "source": [
    "### Run previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba626ac1-4c9d-487c-8939-afb86fde6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run saved config\n",
    "scenes_file_path = './images/AlphabetJoy_20241117_152536/scenes.json'\n",
    "audio_images_dir = './images/AlphabetJoy_20241117_152536'\n",
    "audio_videos_dir = './output/AlphabetJoy_20241117_152536'\n",
    "timestamp = '20241117_152536'\n",
    "last_end_value = 199.4\n",
    "\n",
    "with open(scenes_file_path, \"r\") as scenes_file:\n",
    "    scenes = json.load(scenes_file)\n",
    "\n",
    "process_audio_video(CONFIG, scenes, audio_images_dir, audio_videos_dir, last_end_value, timestamp, video_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a0750-5e16-4713-8ac3-825cb8713762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b3f0a-4844-4c20-9b27-f3f3794de269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
